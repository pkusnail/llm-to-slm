# Qwen3-30B to 8B Knowledge Distillation

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

A practical example of knowledge distillation from **Qwen3-30B-Instruct** to **Qwen3-8B**, demonstrating how to transfer domain-specific knowledge to smaller, deployable models.

## ğŸ¯ What This Project Demonstrates

### The Use Case: Domain-Specific Knowledge Transfer

This project serves as a **practical example** of how to create specialized AI models for professional domains. While general-purpose LLMs work well for common tasks, they may need additional training for specialized professional scenarios.

### ğŸ“š Example Domains Covered

This implementation includes examples from several professional domains (though these are just demonstrations):

**ğŸ”§ AIOps & System Operations:**

- System troubleshooting (network latency, memory leaks, disk usage)
- Log analysis and root cause identification
- Configuration optimization recommendations

**ğŸ¥ Healthcare & Medical:**
- Medical terminology and clinical workflows
- Drug interaction analysis
- Patient case study formatting

**ğŸ’¼ Financial Services:**
- Risk assessment scenarios
- Regulatory compliance explanations
- Financial product analysis

**âš–ï¸ Legal & Compliance:**
- Contract clause interpretation
- Legal document summarization
- Compliance requirement analysis

**ğŸ“ Technical Education:**
- Step-by-step problem explanations
- Code review and debugging guidance
- Concept clarification with examples

**ğŸ­ Industrial & Manufacturing:**
- Equipment maintenance procedures
- Quality control processes
- Safety protocol explanations

### ğŸ¯ Approach: Teacher-Student Knowledge Transfer

This example demonstrates a **three-stage process**:

1. **Domain Data Collection**: Gather or create examples from your target domain
2. **Teacher Generation**: Use a larger model to create high-quality training responses  
3. **Knowledge Distillation**: Train a smaller model to replicate the teacher's expertise

**Note:** The techniques shown here are well-established in the field. This project simply provides a working example with complete code and data.

## ğŸ¯ Overview

This project demonstrates end-to-end knowledge distillation with a **three-stage pipeline**:

1. **Teacher Data Generation**: Use Qwen3-30B to generate high-quality training data
2. **Knowledge Distillation**: Train Qwen3-8B student model using teacher's knowledge
3. **Evaluation & Comparison**: Validate distillation effectiveness through comprehensive testing

### Key Features

- âœ… **Complete Pipeline**: From data generation to model evaluation
- âœ… **GPU Optimized**: Efficient multi-GPU training and inference
- âœ… **LoRA Integration**: Memory-efficient fine-tuning (174.6M parameters)
- âœ… **Temperature Validation**: Proven KD effectiveness through inference testing
- âœ… **Reproducible**: All training data, configs, and models included

## ğŸ“Š Example Results

This implementation shows the following outcomes (your results may vary):

### ğŸ“ˆ Dataset Statistics  
- **Training Data**: 3,164 samples generated by teacher model
- **Domain Coverage**: AIOps, mathematics, coding, and other technical areas
- **Model Size**: Reduced from 30B to 8B parameters (73% compression)
- **Training Method**: LoRA fine-tuning with 174.6M trainable parameters

### ğŸ–¥ï¸ Experimental Environment
**Hardware Configuration:**
- **GPUs**: 8x NVIDIA A100 40GB
- **System RAM**: 128GB+ 
- **GPU Allocation**: 
  - Teacher Model (Qwen3-30B): GPUs 0-5 (6 GPUs)
  - Student Model (Qwen3-8B): GPUs 6-7 (2 GPUs)

**Performance Benchmarks:**
- **Teacher Data Generation**: ~2-3 hours (3,164 samples)
- **Knowledge Distillation Training**: ~1 hour (100 steps)
- **Quick Inference Test**: ~3 minutes (10 samples)
- **Full Evaluation**: ~30 minutes (350 samples)

**Memory Usage:**
- **Teacher Model**: ~120GB VRAM across 6 GPUs
- **Student Model**: ~30GB VRAM across 2 GPUs  
- **Training Peak**: ~180GB total VRAM usage

### ğŸ§ª Validation Approach
- **Temperature Testing**: Demonstrates knowledge transfer effectiveness
- **Multi-domain Evaluation**: Tests performance across different professional areas
- **Comparison Testing**: Provides tools to compare original vs. distilled models

### âš ï¸ Limitations & Considerations
- Results depend heavily on the quality of your training data
- Domain-specific performance requires careful dataset curation
- Model capabilities are limited by the original teacher model's knowledge
- Deployment requires significant GPU resources for optimal performance

## ğŸ¯ When This Example Might Be Useful

**âœ… Could Be Helpful For:**
- **Learning**: Understanding knowledge distillation techniques in practice
- **Research**: Exploring domain-specific model compression methods
- **Prototyping**: Starting point for your own domain-specific models
- **Education**: Teaching knowledge distillation concepts with real code
- **Experimentation**: Testing different approaches to model specialization

**âš ï¸ Probably Not What You Need If:**
- You're looking for a ready-to-use commercial solution
- You don't have experience with ML model training and deployment
- You need guaranteed performance for production use
- You prefer to use established commercial APIs (GPT-4, Claude, etc.)
- You don't have access to appropriate GPU hardware

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- CUDA-capable GPUs (minimum 2x 24GB for inference, 8x 40GB for training)
- 64GB+ system RAM

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd qwen3-knowledge-distillation
   ```

2. **Create virtual environment**
   ```bash
   python -m venv l2s
   source l2s/bin/activate  # Linux/Mac
   # or
   l2s\Scripts\activate     # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install torch transformers accelerate peft
   pip install datasets rich tqdm numpy pandas
   ```

4. **Verify installation**
   ```bash
   python -c "from src.distillation.kd import run_kd_pipeline; print('âœ… Installation successful')"
   ```

### Data Flow and Requirements

This project includes **pre-generated teacher data** for immediate use, or you can create your own custom dataset.

#### Option 1: Use Pre-generated Data (Recommended)

The repository includes teacher-generated training data ready for KD training:
- **Location**: `outputs/experiment/qwen3_30b_to_8b_ultrabatch_512/sft/`
- **Training Data**: `sft_train_data_clean.jsonl` (5.2MB, 3,164 samples)
- **Evaluation Data**: `sft_eval_data_clean.jsonl` (0.6MB, evaluation set)
- **Content**: Each sample contains `{prompt, original_answer, teacher_response}`

**Data Sources:**
- **GSM8K**: Mathematical reasoning problems (2,000 samples)
- **HumanEval**: Python coding challenges (164 samples)  
- **AIOps Synthetic**: System troubleshooting scenarios (1,000 samples)

#### Option 2: Generate Custom Data

If you want to create your own custom dataset, follow this detailed process:

**Step 1: Download and Prepare Initial Data**
```bash
# Download datasets and create initial instruction data
python scripts/prepare_training_data.py \
    --gsm8k_samples 2000 \
    --humaneval_samples 164 \
    --aiops_samples 1000 \
    --output_dir outputs/experiment/my_custom_data

# This creates initial instruction data with format: {prompt, original_answer}
# Output files:
# - outputs/experiment/my_custom_data/sft_train_data_clean.jsonl
# - outputs/experiment/my_custom_data/sft_eval_data_clean.jsonl
```

**Key Parameters for prepare_training_data.py:**
- `--gsm8k_samples`: Number of GSM8K math problems (default: 2000)
- `--humaneval_samples`: Number of HumanEval coding tasks (default: 164) 
- `--aiops_samples`: Number of synthetic AIOps scenarios (default: 1000)
- `--output_dir`: Where to save the prepared data (default: outputs/experiment/prepared_data)

**Step 2: Use Your Custom Data for KD Training**
```bash
# Train with your custom data
python scripts/run_improved_kd.py \
    --train_data outputs/experiment/my_custom_data/sft_train_data_clean.jsonl \
    --eval_data outputs/experiment/my_custom_data/sft_eval_data_clean.jsonl \
    --learning_rate 2e-5 \
    --temperature 2.5 \
    --alpha 0.8 \
    --max_steps 100 \
    --output_dir outputs/experiment/my_kd_experiment
```

**Important Notes:**
- The `prepare_training_data.py` script generates **initial instruction data** only
- For full KD training, you need data with **teacher responses** (like the pre-generated data)
- The prepare script is mainly for creating custom instruction datasets
- **Recommended**: Use Option 1 (pre-generated data) for immediate KD training

### Quick Inference Test

Test the distilled model immediately with our pre-trained checkpoint:

```bash
# Quick test (10 samples, ~3 minutes)
./run_kd_inference_test.sh quick

# Medium test (50 samples, ~15 minutes)
./run_kd_inference_test.sh medium

# Full evaluation (350 samples, ~30 minutes)
./run_kd_inference_test.sh full
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/                           # Core implementation
â”‚   â”œâ”€â”€ distillation/
â”‚   â”‚   â”œâ”€â”€ kd.py                 # Knowledge distillation engine
â”‚   â”‚   â”œâ”€â”€ sft.py                # Teacher data generation
â”‚   â”‚   â””â”€â”€ base.py               # Base classes and utilities
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ common.py             # Common utilities
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ evaluator.py          # Model evaluation tools
â”œâ”€â”€ scripts/                       # Training scripts
â”‚   â”œâ”€â”€ run_improved_kd.py        # Main KD training script
â”‚   â”œâ”€â”€ evaluate_distillation.py  # Model evaluation
â”‚   â””â”€â”€ fix_sft_data.py           # Data preprocessing
â”œâ”€â”€ outputs/experiment/            # Experiment results
â”‚   â”œâ”€â”€ qwen3_30b_to_8b_ultrabatch_512/sft/
â”‚   â”‚   â”œâ”€â”€ sft_train_data_clean.jsonl    # Training data (5.2MB)
â”‚   â”‚   â””â”€â”€ sft_eval_data_clean.jsonl     # Evaluation data (0.6MB)
â”‚   â””â”€â”€ gpu_optimized_kd_20250910_170252/
â”‚       â”œâ”€â”€ final_model/                   # Distilled model checkpoint
â”‚       â”œâ”€â”€ kd_config.json                # Training configuration
â”‚       â””â”€â”€ kd_results.json               # Training metrics
â”œâ”€â”€ test_kd_inference_v2.py        # Inference comparison tool
â”œâ”€â”€ run_kd_inference_test.sh       # KD inference test runner script
â””â”€â”€ learn.md                       # Detailed technical documentation
```

## ğŸ”¬ Detailed Usage

### Stage 1: Teacher Data Generation (Pre-completed)

The teacher model (Qwen3-30B-Instruct) has already generated high-quality training data:

```python
# This stage is already completed, but for reference:
from src.distillation.sft import run_sft_pipeline

results = run_sft_pipeline(
    teacher_model_path="Qwen/Qwen3-30B-A3B-Instruct-2507",
    student_model_path="Qwen/Qwen3-8B",
    train_data_path="data/processed/train_dataset.jsonl",
    output_dir="outputs/experiment/teacher_data_generation"
)
```

**Output**: 
- `sft_train_data_clean.jsonl` (3,164 samples)
- `sft_eval_data_clean.jsonl` (evaluation set)

### Stage 2: Knowledge Distillation Training

Train the student model using the pre-generated teacher data:

```bash
# Using default pre-generated data (recommended)
python scripts/run_improved_kd.py \
    --learning_rate 2e-5 \
    --temperature 2.5 \
    --alpha 0.8 \
    --max_steps 100 \
    --output_dir outputs/experiment/my_kd_experiment

# Or specify custom data paths
python scripts/run_improved_kd.py \
    --train_data path/to/your/sft_train_data_clean.jsonl \
    --eval_data path/to/your/sft_eval_data_clean.jsonl \
    --learning_rate 2e-5 \
    --temperature 2.5 \
    --alpha 0.8 \
    --max_steps 100 \
    --output_dir outputs/experiment/my_kd_experiment
```

**Key Parameters**:
- `--train_data`: Path to training data (optional, uses pre-generated data by default)
- `--eval_data`: Path to evaluation data (optional, uses pre-generated data by default)
- `--learning_rate`: Conservative learning rate (2e-5)
- `--temperature`: Distillation temperature (2.5)
- `--alpha`: KL divergence weight (0.8)
- `--max_steps`: Training steps (100)

### Stage 3: Evaluation and Comparison

Compare the performance of original vs distilled models:

```bash
# Quick comparison test
python test_kd_inference_v2.py --mode quick --temperature 0.7

# Detailed evaluation
python scripts/evaluate_distillation.py \
    --model_path outputs/experiment/gpu_optimized_kd_20250910_170252/final_model \
    --output_dir outputs/evaluation
```

## âš™ï¸ Configuration

### Hardware Requirements

**Minimum (Inference Only)**:
- 2x GPU with 24GB VRAM
- 32GB system RAM
- Expected time: ~10 minutes for quick test

**Recommended (Full Training)**:
- 8x GPU with 40GB VRAM (A100)
- 128GB system RAM
- Expected time: ~4-5 hours for complete pipeline

**Alternative Configurations:**
- **4x A100**: Reduced batch sizes, ~6-8 hours total
- **2x A100**: Limited to inference and evaluation only
- **V100/RTX 4090**: Possible but with significant time overhead

### Model Configuration

The system automatically configures GPU allocation:

```python
# 8 GPUs setup
teacher_gpus = [0, 1, 2, 3, 4, 5]  # Qwen3-30B (48 layers)
student_gpus = [6, 7]              # Qwen3-8B (36 layers)

# 4 GPUs setup  
teacher_gpus = [0, 1, 2]
student_gpus = [3]

# 2 GPUs setup
teacher_gpus = [0]
student_gpus = [1]
```

### Training Parameters

Key hyperparameters used in our successful experiment:

```json
{
  "learning_rate": 2e-5,
  "temperature": 2.5,
  "alpha": 0.8,
  "batch_size": 1,
  "gradient_accumulation_steps": 32,
  "max_length": 1024,
  "epochs": 1,
  "warmup_steps": 20,
  "weight_decay": 0.01
}
```

## ğŸ“ˆ Monitoring Training

### Training Logs

Monitor training progress through detailed logging:

```bash
tail -f outputs/experiment/your_experiment/kd_training.log
```

Key metrics to watch:
- **KL Loss**: Should gradually decrease
- **CE Loss**: Cross-entropy loss on student predictions  
- **Total Loss**: Combined weighted loss
- **GPU Utilization**: Monitor memory usage

### Loss Analysis

Successful training shows:
```
Step 5:  KL_loss: 3.03, CE_loss: 2.15, Total: 2.42
Step 10: KL_loss: 2.87, CE_loss: 2.08, Total: 2.33
Step 15: KL_loss: 2.73, CE_loss: 2.01, Total: 2.27
```

## ğŸ§ª Validation Methods

### Temperature-Based Testing

Our key validation method uses temperature variation to demonstrate knowledge transfer:

```python
# Conservative temperature - should show minimal difference
results_01 = test_model(temperature=0.1)

# Balanced temperature - should show clear differences  
results_07 = test_model(temperature=0.7)

# Creative temperature - should show significant differences
results_12 = test_model(temperature=1.2)
```

### Performance Metrics

- **Perplexity Comparison**: Original vs Distilled model
- **Generation Quality**: Response coherence and relevance
- **Knowledge Retention**: Ability to maintain teacher's capabilities
- **Efficiency Gains**: Inference speed and memory usage

## ğŸ”§ Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```bash
# Reduce batch size
python scripts/run_improved_kd.py --batch_size 1 --gradient_accumulation_steps 64

# Use gradient checkpointing
# (Already enabled in default config)
```

**2. Import Errors**
```bash
# Ensure you're in the project root
export PYTHONPATH=$PYTHONPATH:$(pwd)

# Activate virtual environment
source l2s/bin/activate
```

**3. Model Loading Issues**
```bash
# Check GPU availability
python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}')"

# Verify model access
huggingface-cli login  # If using gated models
```

**4. Data Path Issues**
```bash
# Verify data files exist
ls -la outputs/experiment/qwen3_30b_to_8b_ultrabatch_512/sft/
```

### Performance Tuning

**For limited GPU memory**:
```python
config = {
    "per_device_batch_size": 1,
    "gradient_accumulation_steps": 64,  # Increase to maintain effective batch size
    "gradient_checkpointing": True,
    "max_length": 512  # Reduce sequence length
}
```

**For faster training**:
```python
config = {
    "bf16": True,  # Use bfloat16 precision
    "dataloader_num_workers": 4,
    "max_steps": 50  # Reduce for quick experiments
}
```

## ğŸ“š Advanced Usage

### Custom Data Generation

Generate your own teacher data:

```python
from src.distillation.sft import run_sft_pipeline

# Prepare your instruction dataset
custom_data = [
    {"instruction": "Explain quantum computing", "input": "", "expected_answer": "..."},
    # ... more samples
]

# Run teacher data generation
results = run_sft_pipeline(
    teacher_model_path="Qwen/Qwen3-30B-A3B-Instruct-2507",
    custom_data=custom_data,
    output_dir="outputs/custom_experiment"
)
```

### Hyperparameter Tuning

Experiment with different configurations:

```bash
# Higher KL weight (focus more on teacher distribution)
python scripts/run_improved_kd.py --alpha 0.9 --temperature 2.0

# Lower temperature (sharper distributions)  
python scripts/run_improved_kd.py --temperature 1.5 --alpha 0.7

# Longer training
python scripts/run_improved_kd.py --max_steps 200 --learning_rate 1e-5
```

### Evaluation Customization

Create custom evaluation metrics:

```python
from src.evaluation.evaluator import run_evaluation

# Custom evaluation with specific metrics
results = run_evaluation(
    model_path="path/to/your/model",
    eval_data_path="path/to/eval/data", 
    custom_metrics=["bleu", "rouge", "perplexity"],
    batch_size=4
)
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Install development dependencies
pip install -e .
pip install pytest black isort flake8

# Run tests
pytest tests/

# Format code
black . && isort . && flake8 .
```

## ğŸ“„ Citation

If you use this work in your research, please cite:

```bibtex
@misc{qwen3-knowledge-distillation,
  title={Qwen3-30B to 8B Knowledge Distillation: A Complete Implementation},
  author={Your Name},
  year={2024},
  publisher={GitHub},
  url={https://github.com/your-username/qwen3-knowledge-distillation}
}
```

## ğŸ“œ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Qwen Team** for the excellent base models
- **Hugging Face** for transformers and PEFT libraries
- **PyTorch Team** for the deep learning framework
- **Community contributors** for testing and feedback

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/your-username/qwen3-knowledge-distillation/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-username/qwen3-knowledge-distillation/discussions)
- **Documentation**: See `learn.md` for detailed technical documentation

---

â­ If you find this project helpful, please consider giving it a star!