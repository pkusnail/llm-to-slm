# Qwen3-30B to 8B Knowledge Distillation

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

A practical example of knowledge distillation from **Qwen3-30B-Instruct** to **Qwen3-8B**, demonstrating how to transfer domain-specific knowledge to smaller, deployable models.

## 🎯 What This Project Demonstrates

### The Use Case: Domain-Specific Knowledge Transfer

This project serves as a **practical example** of how to create specialized AI models for professional domains. While general-purpose LLMs work well for common tasks, they may need additional training for specialized professional scenarios.

### 📚 Example Domains Covered

This implementation includes examples from several professional domains (though these are just demonstrations):

**🔧 AIOps & System Operations:**

- System troubleshooting (network latency, memory leaks, disk usage)
- Log analysis and root cause identification
- Configuration optimization recommendations

**🏥 Healthcare & Medical:**
- Medical terminology and clinical workflows
- Drug interaction analysis
- Patient case study formatting

**💼 Financial Services:**
- Risk assessment scenarios
- Regulatory compliance explanations
- Financial product analysis

**⚖️ Legal & Compliance:**
- Contract clause interpretation
- Legal document summarization
- Compliance requirement analysis

**🎓 Technical Education:**
- Step-by-step problem explanations
- Code review and debugging guidance
- Concept clarification with examples

**🏭 Industrial & Manufacturing:**
- Equipment maintenance procedures
- Quality control processes
- Safety protocol explanations

### 🎯 Approach: Teacher-Student Knowledge Transfer

This example demonstrates a **three-stage process**:

1. **Domain Data Collection**: Gather or create examples from your target domain
2. **Teacher Generation**: Use a larger model to create high-quality training responses  
3. **Knowledge Distillation**: Train a smaller model to replicate the teacher's expertise

**Note:** The techniques shown here are well-established in the field. This project simply provides a working example with complete code and data.

## 🎯 Overview

This project demonstrates end-to-end knowledge distillation with a **three-stage pipeline**:

1. **Teacher Data Generation**: Use Qwen3-30B to generate high-quality training data
2. **Knowledge Distillation**: Train Qwen3-8B student model using teacher's knowledge
3. **Evaluation & Comparison**: Validate distillation effectiveness through comprehensive testing

### Key Features

- ✅ **Complete Pipeline**: From data generation to model evaluation
- ✅ **GPU Optimized**: Efficient multi-GPU training and inference
- ✅ **LoRA Integration**: Memory-efficient fine-tuning (174.6M parameters)
- ✅ **Temperature Validation**: Proven KD effectiveness through inference testing
- ✅ **Reproducible**: All training data, configs, and models included

## 📊 Example Results

This implementation shows the following outcomes (your results may vary):

### 📈 Dataset Statistics  
- **Training Data**: 3,164 samples generated by teacher model
- **Domain Coverage**: AIOps, mathematics, coding, and other technical areas
- **Model Size**: Reduced from 30B to 8B parameters (73% compression)
- **Training Method**: LoRA fine-tuning with 174.6M trainable parameters

### 🖥️ Experimental Environment
**Hardware Configuration:**
- **GPUs**: 8x NVIDIA A100 40GB
- **System RAM**: 128GB+ 
- **GPU Allocation**: 
  - Teacher Model (Qwen3-30B): GPUs 0-5 (6 GPUs)
  - Student Model (Qwen3-8B): GPUs 6-7 (2 GPUs)

**Performance Benchmarks:**
- **Teacher Data Generation**: ~2-3 hours (3,164 samples)
- **Knowledge Distillation Training**: ~1 hour (100 steps)
- **Quick Inference Test**: ~3 minutes (10 samples)
- **Full Evaluation**: ~30 minutes (350 samples)

**Memory Usage:**
- **Teacher Model**: ~120GB VRAM across 6 GPUs
- **Student Model**: ~30GB VRAM across 2 GPUs  
- **Training Peak**: ~180GB total VRAM usage

### 🧪 Validation Approach
- **Temperature Testing**: Demonstrates knowledge transfer effectiveness
- **Multi-domain Evaluation**: Tests performance across different professional areas
- **Comparison Testing**: Provides tools to compare original vs. distilled models

### ⚠️ Limitations & Considerations
- Results depend heavily on the quality of your training data
- Domain-specific performance requires careful dataset curation
- Model capabilities are limited by the original teacher model's knowledge
- Deployment requires significant GPU resources for optimal performance

## 🎯 When This Example Might Be Useful

**✅ Could Be Helpful For:**
- **Learning**: Understanding knowledge distillation techniques in practice
- **Research**: Exploring domain-specific model compression methods
- **Prototyping**: Starting point for your own domain-specific models
- **Education**: Teaching knowledge distillation concepts with real code
- **Experimentation**: Testing different approaches to model specialization

**⚠️ Probably Not What You Need If:**
- You're looking for a ready-to-use commercial solution
- You don't have experience with ML model training and deployment
- You need guaranteed performance for production use
- You prefer to use established commercial APIs (GPT-4, Claude, etc.)
- You don't have access to appropriate GPU hardware

## 🚀 Quick Start

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- CUDA-capable GPUs (minimum 2x 24GB for inference, 8x 40GB for training)
- 64GB+ system RAM

### Installation

**⚡ Automated Setup (Recommended)**

1. **Clone and setup automatically**
   ```bash
   git clone <repository-url>
   cd qwen3-knowledge-distillation
   ./setup.sh
   ```

The setup script will:
- ✅ Check Python version compatibility  
- 📦 Create virtual environment (`venv`)
- ⬆️ Upgrade pip to latest version
- 📚 Install all required dependencies
- 🔍 Verify installation success

**🔧 Manual Setup**

If you prefer manual installation:

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd qwen3-knowledge-distillation
   ```

2. **⚠️ IMPORTANT: Create virtual environment** 
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   # or
   venv\Scripts\activate     # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

**💡 Always Use Virtual Environment**: This project requires specific versions of ML libraries. Using a virtual environment prevents conflicts with your system Python packages.

### Data Flow and Requirements

This project includes **pre-generated teacher data** for immediate use, or you can create your own custom dataset.

#### Option 1: Use Pre-generated Data (Recommended)

The repository includes teacher-generated training data ready for KD training:
- **Location**: `outputs/experiment/qwen3_30b_to_8b_ultrabatch_512/sft/`
- **Training Data**: `sft_train_data_clean.jsonl` (5.2MB, 3,164 samples)
- **Evaluation Data**: `sft_eval_data_clean.jsonl` (0.6MB, evaluation set)
- **Content**: Each sample contains `{prompt, original_answer, teacher_response}`

**Data Sources:**
- **GSM8K**: Mathematical reasoning problems (2,000 samples)
- **HumanEval**: Python coding challenges (164 samples)  
- **AIOps Synthetic**: System troubleshooting scenarios (1,000 samples)

#### Option 2: Generate Custom Data

If you want to create your own custom dataset, follow this detailed process:

**Step 1: Download and Prepare Initial Data**
```bash
# Download datasets and create initial instruction data
python scripts/prepare_training_data.py \
    --gsm8k_samples 2000 \
    --humaneval_samples 164 \
    --aiops_samples 1000 \
    --output_dir outputs/experiment/my_custom_data

# This creates initial instruction data with format: {prompt, original_answer}
# Output files:
# - outputs/experiment/my_custom_data/sft_train_data_clean.jsonl
# - outputs/experiment/my_custom_data/sft_eval_data_clean.jsonl
```

**Key Parameters for prepare_training_data.py:**
- `--gsm8k_samples`: Number of GSM8K math problems (default: 2000)
- `--humaneval_samples`: Number of HumanEval coding tasks (default: 164) 
- `--aiops_samples`: Number of synthetic AIOps scenarios (default: 1000)
- `--output_dir`: Where to save the prepared data (default: outputs/experiment/prepared_data)

**Step 2: Use Your Custom Data for KD Training**
```bash
# Train with your custom data
python scripts/run_improved_kd.py \
    --train_data outputs/experiment/my_custom_data/sft_train_data_clean.jsonl \
    --eval_data outputs/experiment/my_custom_data/sft_eval_data_clean.jsonl \
    --learning_rate 2e-5 \
    --temperature 2.5 \
    --alpha 0.8 \
    --max_steps 100 \
    --output_dir outputs/experiment/my_kd_experiment
```

**Important Notes:**
- The `prepare_training_data.py` script generates **initial instruction data** only
- For full KD training, you need data with **teacher responses** (like the pre-generated data)
- The prepare script is mainly for creating custom instruction datasets
- **Recommended**: Use Option 1 (pre-generated data) for immediate KD training

### Quick Inference Test

Test the distilled model immediately with our pre-trained checkpoint:

```bash
# Quick test (10 samples, ~3 minutes)
./run_kd_inference_test.sh quick

# Medium test (50 samples, ~15 minutes)
./run_kd_inference_test.sh medium

# Full evaluation (350 samples, ~30 minutes)
./run_kd_inference_test.sh full
```

## 📁 Project Structure

```
├── src/                           # Core implementation
│   ├── distillation/
│   │   ├── kd.py                 # Knowledge distillation engine
│   │   ├── sft.py                # Teacher data generation
│   │   └── base.py               # Base classes and utilities
│   ├── utils/
│   │   └── common.py             # Common utilities
│   └── evaluation/
│       └── evaluator.py          # Model evaluation tools
├── scripts/                       # Training scripts
│   ├── run_improved_kd.py        # Main KD training script
│   ├── evaluate_distillation.py  # Model evaluation
│   └── fix_sft_data.py           # Data preprocessing
├── outputs/experiment/            # Experiment results
│   ├── qwen3_30b_to_8b_ultrabatch_512/sft/
│   │   ├── sft_train_data_clean.jsonl    # Training data (5.2MB)
│   │   └── sft_eval_data_clean.jsonl     # Evaluation data (0.6MB)
│   └── gpu_optimized_kd_20250910_170252/
│       ├── final_model/                   # Distilled model checkpoint
│       ├── kd_config.json                # Training configuration
│       └── kd_results.json               # Training metrics
├── test_kd_inference_v2.py        # Inference comparison tool
├── run_kd_inference_test.sh       # KD inference test runner script
└── learn.md                       # Detailed technical documentation
```

## 🔬 Detailed Usage

### Stage 1: Teacher Data Generation (Pre-completed)

The teacher model (Qwen3-30B-Instruct) has already generated high-quality training data:

```python
# This stage is already completed, but for reference:
from src.distillation.sft import run_sft_pipeline

results = run_sft_pipeline(
    teacher_model_path="Qwen/Qwen3-30B-A3B-Instruct-2507",
    student_model_path="Qwen/Qwen3-8B",
    train_data_path="data/processed/train_dataset.jsonl",
    output_dir="outputs/experiment/teacher_data_generation"
)
```

**Output**: 
- `sft_train_data_clean.jsonl` (3,164 samples)
- `sft_eval_data_clean.jsonl` (evaluation set)

### Stage 2: Knowledge Distillation Training

Train the student model using the pre-generated teacher data:

```bash
# Using default pre-generated data (recommended)
python scripts/run_improved_kd.py \
    --learning_rate 2e-5 \
    --temperature 2.5 \
    --alpha 0.8 \
    --max_steps 100 \
    --output_dir outputs/experiment/my_kd_experiment

# Or specify custom data paths
python scripts/run_improved_kd.py \
    --train_data path/to/your/sft_train_data_clean.jsonl \
    --eval_data path/to/your/sft_eval_data_clean.jsonl \
    --learning_rate 2e-5 \
    --temperature 2.5 \
    --alpha 0.8 \
    --max_steps 100 \
    --output_dir outputs/experiment/my_kd_experiment
```

**Key Parameters**:
- `--train_data`: Path to training data (optional, uses pre-generated data by default)
- `--eval_data`: Path to evaluation data (optional, uses pre-generated data by default)
- `--learning_rate`: Conservative learning rate (2e-5)
- `--temperature`: Distillation temperature (2.5)
- `--alpha`: KL divergence weight (0.8)
- `--max_steps`: Training steps (100)

### Stage 3: Evaluation and Comparison

Compare the performance of original vs distilled models using our comprehensive evaluation system:

## 📊 Model Evaluation Guide

### Overview

Our evaluation system provides multiple methods to assess KD training effectiveness:
1. **GPU-Optimized Inference Testing** - Direct comparison between original and KD models
2. **Traditional Perplexity Evaluation** - Statistical analysis of model performance  
3. **Multi-Temperature Analysis** - Understanding knowledge transfer effectiveness

### Method 1: Quick Evaluation (Recommended)

Use the automated evaluation script for immediate results:

```bash
# Quick start - runs all evaluation modes
./scripts/run_evaluation.sh

# Manual execution with full parameter control
python test_kd_inference_v2.py \
  --mode quick \
  --temperature 1.2 \
  --student_model "Qwen/Qwen3-8B" \
  --kd_model_path "outputs/experiment/optimized_kd_fixed/final_model" \
  --eval_data "data/cloud_aiops/cloud_aiops_eval_data.jsonl" \
  --max_length 1536 \
  --original_gpu_ids 0 1 2 3 \
  --kd_gpu_ids 4 5 6 7 \
  --quick_batch_size 8
```

### Method 2: Traditional Perplexity Evaluation

For statistical analysis and research purposes:

```bash
python scripts/evaluate_distillation.py \
  --model_path "outputs/experiment/optimized_kd_fixed/final_model" \
  --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \
  --student_base "Qwen/Qwen3-8B" \
  --eval_data "data/cloud_aiops/cloud_aiops_eval_data.jsonl" \
  --sample_size 50 \
  --max_length 1536 \
  --max_new_tokens 512 \
  --temperature 1.2
```

### Evaluation Parameters Explained

#### Critical Parameters

| Parameter | Purpose | Default | Impact |
|-----------|---------|---------|--------|
| `--temperature` | **Inference randomness** | 1.2 | **1.2 best showcases KD effects** |
| `--max_length` | **Sequence length** | 1536 | **Must match training config** |
| `--kd_model_path` | **KD model location** | Required | **Path to LoRA adapters** |
| `--eval_data` | **Test dataset** | Required | **Use domain-specific data** |

#### GPU Allocation Parameters

| Parameter | Purpose | Default | Notes |
|-----------|---------|---------|-------|
| `--original_gpu_ids` | Original model GPUs | [0,1,2,3] | Adjust based on hardware |
| `--kd_gpu_ids` | KD model GPUs | [4,5,6,7] | Prevents memory conflicts |
| `--quick_batch_size` | Quick mode batch size | 8 | Scale with GPU count |
| `--medium_batch_size` | Medium mode batch size | 16 | For 50-sample tests |
| `--full_batch_size` | Full mode batch size | 24 | For complete datasets |

#### Temperature Parameter Deep Dive

**🌡️ Understanding Temperature Settings**

Our evaluation uses **inference temperature** (different from training temperature):

- **Training Temperature (8.0)**: Used during KD for logits softening - **DO NOT CHANGE**
- **Inference Temperature**: Controls generation randomness - **CONFIGURABLE**

| Temperature | Effect | Use Case |
|-------------|--------|----------|
| **0.1** | Conservative, deterministic | May **mask KD differences** |
| **0.7** | Balanced responses | Standard evaluation |
| **1.2** | Creative, diverse | **BEST for showing KD effects** ⭐ |

**Why Temperature=1.2 is Optimal:**

Based on our experimental validation (`learn.md`):
- **Temperature=0.7**: KD model 738 chars vs Original 954 chars (-216, efficiency gain)
- **Temperature=1.2**: KD model 1905 chars, shows Teacher-style explanations (+1111, style transfer)

### Evaluation Modes

#### Quick Mode (Recommended for Development)
```bash
python test_kd_inference_v2.py --mode quick --temperature 1.2 \
  --kd_model_path "your/model/path" \
  --eval_data "your/eval/data.jsonl"
```
- **Samples**: 10 representative examples
- **Time**: ~3 minutes  
- **Purpose**: Rapid validation of KD effects

#### Medium Mode (Balanced Analysis)
```bash
python test_kd_inference_v2.py --mode medium --temperature 1.2 \
  --kd_model_path "your/model/path" \
  --eval_data "your/eval/data.jsonl"
```
- **Samples**: 50 random samples
- **Time**: ~15 minutes
- **Purpose**: Statistical significance

#### Full Mode (Complete Analysis) 
```bash
python test_kd_inference_v2.py --mode full --temperature 1.2 \
  --kd_model_path "your/model/path" \
  --eval_data "your/eval/data.jsonl"
```
- **Samples**: All available samples
- **Time**: ~30 minutes
- **Purpose**: Comprehensive evaluation

### Expected Results Structure

#### Output Files

1. **Comparison Results**: `kd_comparison_[mode]_[timestamp].json`
   ```json
   {
     "config": {
       "mode": "quick",
       "sample_count": 10,
       "batch_size": 8,
       "temperature": 1.2
     },
     "timing": {
       "original_time": 45.2,
       "kd_time": 43.8
     },
     "results": [
       {
         "id": "sample_001",
         "domain": "aiops",
         "prompt": "How to troubleshoot high memory usage?",
         "original_response": "Check processes with top command...",
         "kd_response": "To troubleshoot high memory usage systematically...",
         "original_time": 4.5,
         "kd_time": 4.3
       }
     ]
   }
   ```

2. **Perplexity Results**: `evaluation_results_[timestamp].json`
   ```json
   {
     "results": {
       "student": {
         "perplexity": 12.45,
         "loss": 2.52
       },
       "baseline": {
         "perplexity": 15.23,
         "loss": 2.73
       },
       "teacher": {
         "perplexity": 8.92,
         "loss": 2.19
       }
     },
     "improvements": {
       "perplexity_improvement_pct": 18.25,
       "loss_improvement_pct": 7.69
     }
   }
   ```

### Reference Performance Benchmarks

#### Expected Score Ranges (AIOps Domain)

| Metric | Original 8B | KD-Trained 8B | Teacher 30B | Target |
|--------|-------------|---------------|-------------|--------|
| **Perplexity** | 15-18 | 12-15 | 8-12 | **Lower is better** |
| **Response Length** | 200-500 chars | 400-800 chars | 600-1200 chars | **Longer indicates detail** |
| **Technical Terms** | 2-4 per response | 4-6 per response | 6-8 per response | **More indicates domain knowledge** |
| **Structured Format** | 30% responses | 60% responses | 80% responses | **Higher indicates teaching style** |

#### Quality Indicators

**🟢 Good KD Training (Success Indicators)**
- Perplexity improvement: >15%
- Response length increase: >50% 
- More structured explanations
- Consistent technical terminology
- Teacher-like step-by-step reasoning

**🟡 Moderate KD Training (Needs Tuning)**
- Perplexity improvement: 5-15%
- Response length increase: 20-50%
- Some structure improvement
- Inconsistent terminology

**🔴 Poor KD Training (Needs Retraining)**
- Perplexity improvement: <5%
- No significant response changes
- Same as baseline behavior
- No knowledge transfer evidence

### Multi-Temperature Comparison Analysis

For comprehensive evaluation, run temperature comparison:

```bash
# Compare effectiveness at different temperatures
for temp in 0.1 0.7 1.2; do
  python test_kd_inference_v2.py \
    --mode quick \
    --temperature $temp \
    --kd_model_path "your/model/path" \
    --eval_data "your/eval/data.jsonl"
done
```

**Expected Temperature Analysis:**
- **T=0.1**: Minimal differences (models behave similarly)
- **T=0.7**: Clear efficiency improvements  
- **T=1.2**: Maximum knowledge transfer demonstration

### Troubleshooting Evaluation Issues

#### Common Problems

1. **"Model path not found"**
   ```bash
   # Verify model exists
   ls -la outputs/experiment/your_experiment/final_model/
   # Should contain: adapter_config.json, adapter_model.bin
   ```

2. **"CUDA out of memory during evaluation"**
   ```bash
   # Reduce batch size
   python test_kd_inference_v2.py --mode quick --quick_batch_size 4
   ```

3. **"Evaluation data not found"**
   ```bash
   # Check data path
   ls -la data/cloud_aiops/cloud_aiops_eval_data.jsonl
   # Or use pre-generated data
   --eval_data "outputs/experiment/qwen3_30b_to_8b_ultrabatch_512/sft/sft_eval_data_clean.jsonl"
   ```

4. **"Temperature confusion"**
   - **Training temperature (8.0)**: Fixed in KD training config
   - **Inference temperature (1.2)**: Variable in evaluation scripts

### Best Practices

1. **Always use temperature=1.2** for showcasing KD effects
2. **Match max_length** to your training configuration  
3. **Use domain-specific evaluation data** that matches training data
4. **Run multiple temperature comparisons** for comprehensive analysis
5. **Save all evaluation results** with timestamps for comparison
6. **Monitor GPU memory usage** during evaluation
7. **Use representative sample sizes** (quick=10, medium=50, full=all)

## ⚙️ Configuration

### Hardware Requirements

**Minimum (Inference Only)**:
- 2x GPU with 24GB VRAM
- 32GB system RAM
- Expected time: ~10 minutes for quick test

**Recommended (Full Training)**:
- 8x GPU with 40GB VRAM (A100)
- 128GB system RAM
- Expected time: ~4-5 hours for complete pipeline

**Alternative Configurations:**
- **4x A100**: Reduced batch sizes, ~6-8 hours total
- **2x A100**: Limited to inference and evaluation only
- **V100/RTX 4090**: Possible but with significant time overhead

### Model Configuration

The system automatically configures GPU allocation:

```python
# 8 GPUs setup
teacher_gpus = [0, 1, 2, 3, 4, 5]  # Qwen3-30B (48 layers)
student_gpus = [6, 7]              # Qwen3-8B (36 layers)

# 4 GPUs setup  
teacher_gpus = [0, 1, 2]
student_gpus = [3]

# 2 GPUs setup
teacher_gpus = [0]
student_gpus = [1]
```

### Training Parameters

Key hyperparameters used in our successful experiment:

```json
{
  "learning_rate": 2e-5,
  "temperature": 2.5,
  "alpha": 0.8,
  "batch_size": 1,
  "gradient_accumulation_steps": 32,
  "max_length": 1024,
  "epochs": 1,
  "warmup_steps": 20,
  "weight_decay": 0.01
}
```

## 📈 Monitoring Training

### Training Logs

Monitor training progress through detailed logging:

```bash
tail -f outputs/experiment/your_experiment/kd_training.log
```

Key metrics to watch:
- **KL Loss**: Should gradually decrease
- **CE Loss**: Cross-entropy loss on student predictions  
- **Total Loss**: Combined weighted loss
- **GPU Utilization**: Monitor memory usage

### Loss Analysis

Successful training shows:
```
Step 5:  KL_loss: 3.03, CE_loss: 2.15, Total: 2.42
Step 10: KL_loss: 2.87, CE_loss: 2.08, Total: 2.33
Step 15: KL_loss: 2.73, CE_loss: 2.01, Total: 2.27
```

## 🧪 Validation Methods

### Temperature-Based Testing

Our key validation method uses temperature variation to demonstrate knowledge transfer:

```python
# Conservative temperature - should show minimal difference
results_01 = test_model(temperature=0.1)

# Balanced temperature - should show clear differences  
results_07 = test_model(temperature=0.7)

# Creative temperature - should show significant differences
results_12 = test_model(temperature=1.2)
```

### Performance Metrics

- **Perplexity Comparison**: Original vs Distilled model
- **Generation Quality**: Response coherence and relevance
- **Knowledge Retention**: Ability to maintain teacher's capabilities
- **Efficiency Gains**: Inference speed and memory usage

## 🔧 Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```bash
# Reduce batch size
python scripts/run_improved_kd.py --batch_size 1 --gradient_accumulation_steps 64

# Use gradient checkpointing
# (Already enabled in default config)
```

**2. Import Errors**
```bash
# Ensure you're in the project root
export PYTHONPATH=$PYTHONPATH:$(pwd)

# Activate virtual environment
source venv/bin/activate
```

**3. Model Loading Issues**
```bash
# Check GPU availability
python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}')"

# Verify model access
huggingface-cli login  # If using gated models
```

**4. Data Path Issues**
```bash
# Verify data files exist
ls -la outputs/experiment/qwen3_30b_to_8b_ultrabatch_512/sft/
```

### Performance Tuning

**For limited GPU memory**:
```python
config = {
    "per_device_batch_size": 1,
    "gradient_accumulation_steps": 64,  # Increase to maintain effective batch size
    "gradient_checkpointing": True,
    "max_length": 512  # Reduce sequence length
}
```

**For faster training**:
```python
config = {
    "bf16": True,  # Use bfloat16 precision
    "dataloader_num_workers": 4,
    "max_steps": 50  # Reduce for quick experiments
}
```

## 📚 Advanced Usage

### Custom Data Generation

Generate your own teacher data:

```python
from src.distillation.sft import run_sft_pipeline

# Prepare your instruction dataset
custom_data = [
    {"instruction": "Explain quantum computing", "input": "", "expected_answer": "..."},
    # ... more samples
]

# Run teacher data generation
results = run_sft_pipeline(
    teacher_model_path="Qwen/Qwen3-30B-A3B-Instruct-2507",
    custom_data=custom_data,
    output_dir="outputs/custom_experiment"
)
```

### Hyperparameter Tuning

Experiment with different configurations:

```bash
# Higher KL weight (focus more on teacher distribution)
python scripts/run_improved_kd.py --alpha 0.9 --temperature 2.0

# Lower temperature (sharper distributions)  
python scripts/run_improved_kd.py --temperature 1.5 --alpha 0.7

# Longer training
python scripts/run_improved_kd.py --max_steps 200 --learning_rate 1e-5
```

### Evaluation Customization

Create custom evaluation metrics:

```python
from src.evaluation.evaluator import run_evaluation

# Custom evaluation with specific metrics
results = run_evaluation(
    model_path="path/to/your/model",
    eval_data_path="path/to/eval/data", 
    custom_metrics=["bleu", "rouge", "perplexity"],
    batch_size=4
)
```

## 📜 License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## 🙏 Acknowledgments

- **Qwen Team** for the excellent base models
- **Hugging Face** for transformers and PEFT libraries
- **PyTorch Team** for the deep learning framework
- **Community contributors** for testing and feedback

---

⭐ If you find this project helpful, please consider giving it a star!
