"""
ä¼˜åŒ–çš„çŸ¥è¯†è’¸é¦å®ç° - å……åˆ†åˆ©ç”¨8ä¸ªA100 GPU
æ”¹è¿›GPUåˆ†é…ç­–ç•¥ï¼Œæé«˜ååé‡å’Œè®­ç»ƒæ•ˆç‡
"""
import os
import json
import logging
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
import math

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import (
    TrainingArguments,
    Trainer,
    AutoModelForCausalLM,
    AutoTokenizer,
    get_linear_schedule_with_warmup,
    get_cosine_schedule_with_warmup
)
from accelerate import Accelerator
import numpy as np

from distillation.base import BaseDistiller, DistillationDataset, collate_fn, KnowledgeDistillationLoss
from utils.common import load_jsonl, setup_logging, set_seed, cleanup_cache

logger = logging.getLogger(__name__)


class OptimizedKDDistiller(BaseDistiller):
    """ä¼˜åŒ–çš„çŸ¥è¯†è’¸é¦å™¨ - å……åˆ†åˆ©ç”¨8ä¸ªA100"""
    
    def __init__(
        self,
        teacher_model_path: str,
        student_model_path: str,
        output_dir: str,
        temperature: float = 8.0,  # é»˜è®¤é«˜æ¸©åº¦
        alpha: float = 0.8,
        use_online_kd: bool = True,
        
        # GPUä¼˜åŒ–å‚æ•°
        enable_8gpu_optimization: bool = True,
        teacher_gpus: Optional[List[int]] = None,  # å¯è‡ªå®šä¹‰Teacher GPU
        student_gpus: Optional[List[int]] = None,  # å¯è‡ªå®šä¹‰Student GPU
        
        **kwargs
    ):
        super().__init__(teacher_model_path, student_model_path, output_dir, **kwargs)
        
        self.temperature = temperature
        self.alpha = alpha
        self.use_online_kd = use_online_kd
        self.enable_8gpu_optimization = enable_8gpu_optimization
        
        # GPUåˆ†é…ç­–ç•¥
        gpu_count = torch.cuda.device_count()
        logger.info(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        
        if enable_8gpu_optimization and gpu_count >= 8:
            # ğŸš€ ä¼˜åŒ–ç­–ç•¥ï¼šå……åˆ†åˆ©ç”¨8ä¸ªGPU
            self.teacher_device_map, self.student_device_map = self._create_optimized_8gpu_mapping()
        elif gpu_count >= 6:
            # å¤‡ç”¨ç­–ç•¥ï¼š6-7ä¸ªGPU
            self.teacher_device_map, self.student_device_map = self._create_balanced_mapping(gpu_count)
        else:
            # æœ€å°é…ç½®ï¼šä½¿ç”¨auto
            self.teacher_device_map = "auto"
            self.student_device_map = "auto"
            
        # åŠ è½½æ¨¡å‹
        self.load_tokenizer()
        self.load_student_model(student_device_map=self.student_device_map)
        
        if use_online_kd:
            self.load_teacher_model(teacher_device_map=self.teacher_device_map)
        
        # æŸå¤±å‡½æ•°
        self.kd_loss_fn = KnowledgeDistillationLoss(temperature, alpha)
    
    def _create_optimized_8gpu_mapping(self) -> Tuple[Dict, Dict]:
        """
        åˆ›å»ºä¼˜åŒ–çš„8GPUåˆ†é…ç­–ç•¥
        
        ç­–ç•¥ï¼š
        - Teacher (30B, 48å±‚): GPU 0-4 (æ¯ä¸ªGPU 9-10å±‚ï¼Œæ›´å‡åŒ€åˆ†å¸ƒ)
        - Student (8B, 36å±‚): GPU 5-7 (æ¯ä¸ªGPU 12å±‚ï¼Œæé«˜åˆ©ç”¨ç‡)
        - GPU 7 ä¸“é—¨å¤„ç†Studentçš„è¾“å‡ºå±‚ï¼Œå‡å°‘é€šä¿¡å¼€é”€
        """
        
        # ğŸ¯ Teacheråˆ†å¸ƒï¼š48å±‚åˆ†å¸ƒåˆ°5ä¸ªGPU (0-4)ï¼Œæ›´å‡åŒ€
        teacher_device_map = {"model.embed_tokens": 0}
        
        # æ¯ä¸ªGPUå¤§çº¦9-10å±‚ï¼Œæ›´ç²¾ç¡®åˆ†é…
        teacher_layers_per_gpu = [10, 10, 10, 9, 9]  # æ€»è®¡48å±‚
        current_layer = 0
        
        for gpu_id, layer_count in enumerate(teacher_layers_per_gpu):
            for _ in range(layer_count):
                teacher_device_map[f"model.layers.{current_layer}"] = gpu_id
                current_layer += 1
        
        teacher_device_map.update({"model.norm": 4, "lm_head": 4})
        
        # ğŸ¯ Studentåˆ†å¸ƒï¼š36å±‚åˆ†å¸ƒåˆ°3ä¸ªGPU (5-7)
        student_device_map = {"model.embed_tokens": 5}
        
        # æ¯ä¸ªGPU 12å±‚ï¼Œå……åˆ†åˆ©ç”¨
        student_layers_per_gpu = [12, 12, 12]  # æ€»è®¡36å±‚
        current_layer = 0
        
        for i, layer_count in enumerate(student_layers_per_gpu):
            gpu_id = 5 + i  # GPU 5, 6, 7
            for _ in range(layer_count):
                student_device_map[f"model.layers.{current_layer}"] = gpu_id
                current_layer += 1
        
        student_device_map.update({"model.norm": 7, "lm_head": 7})
        
        logger.info("ğŸš€ å¯ç”¨8GPUä¼˜åŒ–åˆ†é…:")
        logger.info(f"   Teacher (30B, 48å±‚) â†’ GPU 0-4: {teacher_layers_per_gpu}")
        logger.info(f"   Student (8B, 36å±‚) â†’ GPU 5-7: {student_layers_per_gpu}")
        logger.info("   é¢„æœŸGPUåˆ©ç”¨ç‡æ˜¾è‘—æå‡!")
        
        return teacher_device_map, student_device_map
    
    def _create_balanced_mapping(self, gpu_count: int) -> Tuple[Dict, Dict]:
        """ä¸º6-7ä¸ªGPUåˆ›å»ºå¹³è¡¡åˆ†é…"""
        
        # Teacherç”¨å‰4ä¸ªGPU
        teacher_device_map = {"model.embed_tokens": 0}
        layers_per_gpu = 48 // 4
        
        for i in range(48):
            gpu_id = i // layers_per_gpu
            gpu_id = min(gpu_id, 3)  # é™åˆ¶åœ¨0-3
            teacher_device_map[f"model.layers.{i}"] = gpu_id
        teacher_device_map.update({"model.norm": 3, "lm_head": 3})
        
        # Studentç”¨åé¢çš„GPU
        student_device_map = {"model.embed_tokens": 4}
        remaining_gpus = gpu_count - 4
        student_layers_per_gpu = 36 // remaining_gpus
        
        for i in range(36):
            gpu_id = 4 + (i // student_layers_per_gpu)
            gpu_id = min(gpu_id, gpu_count - 1)
            student_device_map[f"model.layers.{i}"] = gpu_id
        student_device_map.update({"model.norm": gpu_count - 1, "lm_head": gpu_count - 1})
        
        logger.info(f"âš–ï¸ å¹³è¡¡åˆ†é… ({gpu_count}GPU): Teacher(0-3), Student(4-{gpu_count-1})")
        
        return teacher_device_map, student_device_map


class OptimizedKDTrainer(Trainer):
    """ä¼˜åŒ–çš„KDè®­ç»ƒå™¨ - æ”¯æŒåŠ¨æ€å­¦ä¹ ç‡å’Œæ—©åœ"""
    
    def __init__(
        self,
        teacher_model=None,
        kd_loss_fn=None,
        temperature: float = 8.0,
        alpha: float = 0.8,
        eval_steps: int = 250,
        early_stopping_patience: int = 3,
        warmup_ratio: float = 0.1,
        lr_scheduler_type: str = "cosine",
        **kwargs
    ):
        self.teacher_model = teacher_model
        self.kd_loss_fn = kd_loss_fn
        self.temperature = temperature
        self.alpha = alpha
        self.eval_steps = eval_steps
        self.early_stopping_patience = early_stopping_patience
        self.warmup_ratio = warmup_ratio
        self.lr_scheduler_type = lr_scheduler_type
        
        # æ—©åœç›¸å…³
        self.best_eval_loss = float('inf')
        self.no_improve_count = 0
        self.should_stop = False
        
        super().__init__(**kwargs)
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """è®¡ç®—KDæŸå¤±"""
        labels = inputs.get("labels")
        
        # Studentå‰å‘ä¼ æ’­
        student_outputs = model(**inputs)
        student_logits = student_outputs.logits
        
        if self.teacher_model is not None:
            # Teacherå‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_outputs = self.teacher_model(**inputs)
                teacher_logits = teacher_outputs.logits
            
            # è®¡ç®—KDæŸå¤±
            kd_loss, loss_dict = self.kd_loss_fn(
                student_logits, teacher_logits, labels, inputs.get("attention_mask")
            )
            
            # è®°å½•è¯¦ç»†æŸå¤±
            if self.state.global_step % self.args.logging_steps == 0:
                self.log({
                    "train/total_loss": loss_dict["total_loss"],
                    "train/ce_loss": loss_dict["ce_loss"], 
                    "train/kl_loss": loss_dict["kl_loss"],
                    "train/temperature": self.temperature,
                    "train/alpha": self.alpha
                })
            
            return (kd_loss, student_outputs) if return_outputs else kd_loss
        else:
            # ä»…Studentè®­ç»ƒï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
            return student_outputs.loss if hasattr(student_outputs, 'loss') else None
    
    def evaluate(
        self,
        eval_dataset=None,
        ignore_keys=None,
        metric_key_prefix: str = "eval"
    ):
        """å¢å¼ºè¯„ä¼° - åŒ…å«æ—©åœæ£€æŸ¥"""
        eval_result = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)
        
        # æ—©åœæ£€æŸ¥
        current_eval_loss = eval_result.get(f"{metric_key_prefix}_loss", float('inf'))
        
        if current_eval_loss < self.best_eval_loss:
            self.best_eval_loss = current_eval_loss
            self.no_improve_count = 0
            logger.info(f"âœ… è¯„ä¼°æ”¹å–„: {current_eval_loss:.4f} (æœ€ä½³è®°å½•)")
        else:
            self.no_improve_count += 1
            logger.info(f"âš ï¸ è¯„ä¼°æœªæ”¹å–„: {current_eval_loss:.4f} (å·²{self.no_improve_count}æ¬¡)")
            
            if self.no_improve_count >= self.early_stopping_patience:
                logger.info(f"ğŸ›‘ æ—©åœè§¦å‘: {self.early_stopping_patience}æ¬¡æœªæ”¹å–„")
                self.should_stop = True
        
        return eval_result
    
    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval):
        """é‡å†™ä»¥æ”¯æŒæ—©åœ"""
        if self.should_stop:
            self.control.should_training_stop = True
            
        return super()._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)


def run_optimized_kd_pipeline(
    teacher_model_path: str,
    student_model_path: str,
    train_data: str,
    eval_data: Optional[str] = None,
    output_dir: str = "outputs/optimized_kd",
    experiment_name: str = "optimized_kd",
    
    # è®­ç»ƒå‚æ•°
    kd_epochs: int = 4,
    kd_batch_size: int = 4,
    kd_lr: float = 1.5e-4,
    kd_grad_accum: int = 32,
    
    # è’¸é¦å‚æ•°
    temperature: float = 8.0,
    alpha: float = 0.8,
    
    # åºåˆ—å‚æ•°
    max_length: int = 1536,
    
    # è¯„ä¼°å‚æ•°
    eval_steps: int = 250,
    logging_steps: int = 50,
    save_steps: int = 500,
    
    # å­¦ä¹ ç‡è°ƒåº¦
    warmup_ratio: float = 0.1,
    lr_scheduler_type: str = "cosine",
    
    # æ—©åœ
    early_stopping_patience: int = 3,
    
    # ç¡¬ä»¶ä¼˜åŒ–
    use_bf16: bool = True,
    gradient_checkpointing: bool = True,
    dataloader_num_workers: int = 8,
    
    # GPUä¼˜åŒ–
    enable_8gpu_optimization: bool = True,
    
    # wandb
    use_wandb: bool = False,
    wandb_project: str = None,
    wandb_tags: List[str] = None,
    
    **kwargs
) -> Dict[str, Any]:
    """è¿è¡Œä¼˜åŒ–çš„KDç®¡é“"""
    
    logger.info("ğŸš€ å¯åŠ¨ä¼˜åŒ–KDç®¡é“")
    logger.info(f"Teacher: {teacher_model_path}")
    logger.info(f"Student: {student_model_path}")
    logger.info(f"æ•°æ®: {train_data}")
    logger.info(f"æ¸©åº¦: {temperature} (ä¼˜åŒ–)")
    logger.info(f"è½®æ•°: {kd_epochs} epochs")
    logger.info(f"æ‰¹æ¬¡: {kd_batch_size}x{kd_grad_accum}={kd_batch_size*kd_grad_accum}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir) / experiment_name
    output_path.mkdir(parents=True, exist_ok=True)
    
    start_time = time.time()
    
    try:
        # ğŸ¯ æ­¥éª¤1: åˆå§‹åŒ–ä¼˜åŒ–è’¸é¦å™¨
        logger.info("ğŸ“¡ åˆå§‹åŒ–ä¼˜åŒ–KDè’¸é¦å™¨...")
        distiller = OptimizedKDDistiller(
            teacher_model_path=teacher_model_path,
            student_model_path=student_model_path,
            output_dir=str(output_path),
            temperature=temperature,
            alpha=alpha,
            enable_8gpu_optimization=enable_8gpu_optimization
        )
        
        # ğŸ¯ æ­¥éª¤2: å‡†å¤‡æ•°æ®é›†
        logger.info("ğŸ“š åŠ è½½è®­ç»ƒæ•°æ®...")
        train_dataset = DistillationDataset(
            data=load_jsonl(train_data),
            tokenizer=distiller.tokenizer,
            max_length=max_length
        )
        
        eval_dataset = None
        if eval_data:
            eval_dataset = DistillationDataset(
                data=load_jsonl(eval_data),
                tokenizer=distiller.tokenizer,
                max_length=max_length
            )
        
        logger.info(f"âœ… è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        if eval_dataset:
            logger.info(f"âœ… è¯„ä¼°æ ·æœ¬: {len(eval_dataset)}")
        
        # ğŸ¯ æ­¥éª¤3: é…ç½®è®­ç»ƒå‚æ•°
        total_steps = (len(train_dataset) // (kd_batch_size * kd_grad_accum)) * kd_epochs
        warmup_steps = int(total_steps * warmup_ratio)
        
        training_args = TrainingArguments(
            output_dir=str(output_path),
            
            # è®­ç»ƒè½®æ•°
            num_train_epochs=kd_epochs,
            per_device_train_batch_size=kd_batch_size,
            per_device_eval_batch_size=kd_batch_size,
            gradient_accumulation_steps=kd_grad_accum,
            
            # å­¦ä¹ ç‡
            learning_rate=kd_lr,
            warmup_steps=warmup_steps,
            lr_scheduler_type=lr_scheduler_type,
            
            # è¯„ä¼°å’Œä¿å­˜
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=eval_steps if eval_dataset else None,
            save_steps=save_steps,
            save_total_limit=3,
            logging_steps=logging_steps,
            
            # ç¡¬ä»¶ä¼˜åŒ–
            bf16=use_bf16,
            gradient_checkpointing=gradient_checkpointing,
            dataloader_num_workers=dataloader_num_workers,
            
            # å…¶ä»–
            remove_unused_columns=False,
            load_best_model_at_end=True if eval_dataset else False,
            metric_for_best_model="eval_loss" if eval_dataset else None,
            
            # wandb
            report_to="wandb" if use_wandb else None,
            run_name=f"{experiment_name}_{time.strftime('%m%d_%H%M')}" if use_wandb else None,
        )
        
        # ğŸ¯ æ­¥éª¤4: åˆ›å»ºä¼˜åŒ–è®­ç»ƒå™¨
        logger.info("ğŸƒ åˆ›å»ºä¼˜åŒ–è®­ç»ƒå™¨...")
        trainer = OptimizedKDTrainer(
            model=distiller.student_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=distiller.tokenizer,
            data_collator=collate_fn,
            
            # KDç‰¹å®šå‚æ•°
            teacher_model=distiller.teacher_model,
            kd_loss_fn=distiller.kd_loss_fn,
            temperature=temperature,
            alpha=alpha,
            
            # ä¼˜åŒ–å‚æ•°
            eval_steps=eval_steps,
            early_stopping_patience=early_stopping_patience,
            warmup_ratio=warmup_ratio,
            lr_scheduler_type=lr_scheduler_type
        )
        
        # ğŸ¯ æ­¥éª¤5: å¼€å§‹ä¼˜åŒ–è®­ç»ƒ
        logger.info("ğŸš€ å¼€å§‹ä¼˜åŒ–KDè®­ç»ƒ...")
        logger.info(f"ğŸ“Š æ€»æ­¥æ•°: {total_steps}, é¢„çƒ­æ­¥æ•°: {warmup_steps}")
        
        train_result = trainer.train()
        
        # ğŸ¯ æ­¥éª¤6: ä¿å­˜æœ€ç»ˆæ¨¡å‹
        logger.info("ğŸ’¾ ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹...")
        final_model_path = output_path / "final_model"
        trainer.save_model(str(final_model_path))
        
        # ğŸ¯ æ­¥éª¤7: ç”Ÿæˆç»“æœæŠ¥å‘Š
        end_time = time.time()
        execution_time = end_time - start_time
        
        result = {
            "config": {
                "experiment_name": experiment_name,
                "timestamp": time.strftime("%Y%m%d_%H%M%S"),
                "models": {
                    "teacher": teacher_model_path,
                    "student": student_model_path
                },
                "data": {
                    "train": train_data,
                    "eval": eval_data
                },
                "training": {
                    "epochs": kd_epochs,
                    "batch_size": kd_batch_size,
                    "learning_rate": kd_lr,
                    "gradient_accumulation_steps": kd_grad_accum,
                    "effective_batch_size": kd_batch_size * kd_grad_accum,
                    "max_length": max_length,
                    "warmup_ratio": warmup_ratio,
                    "lr_scheduler_type": lr_scheduler_type
                },
                "distillation": {
                    "temperature": temperature,
                    "alpha": alpha,
                    "use_online_kd": True
                },
                "optimization": {
                    "8gpu_optimization": enable_8gpu_optimization,
                    "early_stopping_patience": early_stopping_patience,
                    "eval_steps": eval_steps
                },
                "hardware": {
                    "use_bf16": use_bf16,
                    "gradient_checkpointing": gradient_checkpointing,
                    "dataloader_num_workers": dataloader_num_workers
                }
            },
            "training_history": train_result.log_history if hasattr(train_result, 'log_history') else [],
            "execution_time": execution_time,
            "final_loss": train_result.training_loss if hasattr(train_result, 'training_loss') else None,
            "output_files": {
                "model": str(final_model_path),
                "train_data": train_data,
                "eval_data": eval_data
            }
        }
        
        # ä¿å­˜ç»“æœ
        result_file = output_path / "optimized_kd_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info("ğŸ‰ ä¼˜åŒ–KDç®¡é“å®Œæˆï¼")
        logger.info(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.1f}ç§’ ({execution_time/3600:.1f}å°æ—¶)")
        logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜: {final_model_path}")
        logger.info(f"ğŸ“Š ç»“æœä¿å­˜: {result_file}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–KDè®­ç»ƒå¤±è´¥: {e}")
        raise e
    finally:
        cleanup_cache()