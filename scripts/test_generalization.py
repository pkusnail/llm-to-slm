#!/usr/bin/env python3
"""
Student Model Generalization Test
æµ‹è¯•Studentæ¨¡å‹åœ¨éAIOpsé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json
import time
from pathlib import Path

def load_model(model_path, base_model="Qwen/Qwen3-8B"):
    """åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒLoRAé€‚é…å™¨ï¼‰"""
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # å¦‚æœæ˜¯checkpointè·¯å¾„ï¼ŒåŠ è½½LoRAé€‚é…å™¨
    if Path(model_path).exists() and "checkpoint" in model_path:
        print(f"åŠ è½½LoRAé€‚é…å™¨: {model_path}")
        model = PeftModel.from_pretrained(model, model_path)
        
    model.eval()
    return model, tokenizer

def test_general_capabilities(model, tokenizer):
    """æµ‹è¯•é€šç”¨èƒ½åŠ›"""
    test_cases = [
        # æ•°å­¦æ¨ç†
        {
            "category": "æ•°å­¦",
            "prompt": "è®¡ç®—ï¼š25 Ã— 4 + 18 Ã· 3 = ?",
            "expected_keywords": ["100", "6", "106"]
        },
        # é€»è¾‘æ¨ç†
        {
            "category": "é€»è¾‘",
            "prompt": "å¦‚æœæ‰€æœ‰çš„çŒ«éƒ½æ˜¯åŠ¨ç‰©ï¼Œè€Œå°é»‘æ˜¯ä¸€åªçŒ«ï¼Œé‚£ä¹ˆå°é»‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "expected_keywords": ["åŠ¨ç‰©", "animal"]
        },
        # è¯­è¨€ç†è§£
        {
            "category": "è¯­è¨€",
            "prompt": "è¯·è§£é‡Š'ç”»è›‡æ·»è¶³'è¿™ä¸ªæˆè¯­çš„å«ä¹‰ã€‚",
            "expected_keywords": ["å¤šä½™", "é€‚å¾—å…¶å", "ç”»è›‡", "è¶³"]
        },
        # å¸¸è¯†æ¨ç†
        {
            "category": "å¸¸è¯†", 
            "prompt": "ä¸ºä»€ä¹ˆå†¬å¤©ä¼šä¸‹é›ªè€Œä¸æ˜¯ä¸‹é›¨ï¼Ÿ",
            "expected_keywords": ["æ¸©åº¦", "å†°ç‚¹", "æ°´è’¸æ°”", "å‡å›º"]
        },
        # åˆ›æ„å†™ä½œ
        {
            "category": "åˆ›æ„",
            "prompt": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„å°è¯—ï¼ˆ4è¡Œï¼‰ã€‚",
            "expected_keywords": ["æ˜¥", "èŠ±", "ç»¿", "æš–"]
        }
    ]
    
    results = []
    
    for test_case in test_cases:
        print(f"\næµ‹è¯•ç±»åˆ«: {test_case['category']}")
        print(f"é—®é¢˜: {test_case['prompt']}")
        
        # ç”Ÿæˆå›ç­”
        inputs = tokenizer(test_case['prompt'], return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(test_case['prompt'], '').strip()
        
        print(f"å›ç­”: {response}")
        
        # ç®€å•å…³é”®è¯åŒ¹é…è¯„ä¼°
        keyword_matches = 0
        for keyword in test_case['expected_keywords']:
            if keyword.lower() in response.lower():
                keyword_matches += 1
        
        relevance_score = keyword_matches / len(test_case['expected_keywords'])
        
        results.append({
            "category": test_case['category'],
            "prompt": test_case['prompt'],
            "response": response,
            "keyword_matches": keyword_matches,
            "total_keywords": len(test_case['expected_keywords']),
            "relevance_score": relevance_score
        })
        
        print(f"ç›¸å…³åº¦å¾—åˆ†: {relevance_score:.2f}")
    
    return results

def test_multiple_models():
    """æµ‹è¯•å¤šä¸ªæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›"""
    models_to_test = [
        {
            "name": "Teacher_30B",
            "model_path": "Qwen/Qwen3-30B-A3B-Instruct-2507",
            "base_model": "Qwen/Qwen3-30B-A3B-Instruct-2507",
            "output_file": "outputs/generalization_teacher_30b.json"
        },
        {
            "name": "Student_8B_Baseline", 
            "model_path": "Qwen/Qwen3-8B",
            "base_model": "Qwen/Qwen3-8B",
            "output_file": "outputs/generalization_student_8b_baseline.json"
        },
        {
            "name": "KD_Student_Checkpoint500",
            "model_path": "outputs/experiment/ultra_optimized_kd/checkpoint-500",
            "base_model": "Qwen/Qwen3-8B", 
            "output_file": "outputs/generalization_kd_student.json"
        }
    ]
    
    all_results = {}
    
    for model_config in models_to_test:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_config['name']}")
        print(f"ğŸ“ è·¯å¾„: {model_config['model_path']}")
        print(f"{'='*60}")
        
        try:
            model, tokenizer = load_model(model_config['model_path'], model_config['base_model'])
            results = test_general_capabilities(model, tokenizer)
            
            # è®¡ç®—æ€»ä½“å¾—åˆ†
            avg_relevance = sum(r['relevance_score'] for r in results) / len(results)
            
            print(f"\nğŸ“ˆ {model_config['name']} æ³›åŒ–èƒ½åŠ›æµ‹è¯•ç»“æœ:")
            print(f"å¹³å‡ç›¸å…³åº¦å¾—åˆ†: {avg_relevance:.3f}")
            
            # æŒ‰ç±»åˆ«æ˜¾ç¤ºç»“æœ
            for result in results:
                print(f"{result['category']:>6}: {result['relevance_score']:.3f} ({result['keyword_matches']}/{result['total_keywords']})")
            
            # ä¿å­˜ç»“æœ
            Path(model_config['output_file']).parent.mkdir(parents=True, exist_ok=True)
            
            summary = {
                "model_name": model_config['name'],
                "model_path": model_config['model_path'], 
                "base_model": model_config['base_model'],
                "test_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "overall_score": avg_relevance,
                "detailed_results": results
            }
            
            with open(model_config['output_file'], 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            all_results[model_config['name']] = {
                'overall_score': avg_relevance,
                'detailed_results': results
            }
            
            # è¯„ä¼°ç»“è®º
            if avg_relevance > 0.6:
                print("âœ… æ¨¡å‹ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›")
            elif avg_relevance > 0.4:
                print("âš ï¸ æ¨¡å‹æ³›åŒ–èƒ½åŠ›æœ‰æ‰€ä¸‹é™ï¼Œä½†ä»åœ¨å¯æ¥å—èŒƒå›´")
            else:
                print("âŒ æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ˜æ˜¾ä¸‹é™ï¼Œéœ€è¦è°ƒæ•´è®­ç»ƒç­–ç•¥")
                
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {model_config['output_file']}")
            
            # æ¸…ç†GPUå†…å­˜
            del model, tokenizer
            import torch
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ æµ‹è¯• {model_config['name']} æ—¶å‡ºé”™: {str(e)}")
            all_results[model_config['name']] = {'overall_score': 0.0, 'error': str(e)}
    
    # æ‰“å°å¯¹æ¯”æ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ“Š æ‰€æœ‰æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¯¹æ¯”æ€»ç»“:")
    print(f"{'='*60}")
    for name, result in all_results.items():
        if 'error' not in result:
            print(f"{name:>25}: {result['overall_score']:.3f}")
        else:
            print(f"{name:>25}: æµ‹è¯•å¤±è´¥")
    
    return all_results

def main():
    print("ğŸ§ª Multi-Model Generalization Test")
    print("æµ‹è¯•Teacher(30B)ã€Student Baseline(8B)ã€KD Studentæ³›åŒ–èƒ½åŠ›å¯¹æ¯”")
    
    results = test_multiple_models()
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_file = "outputs/generalization_comparison.json" 
    with open(comparison_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "comparison_results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {comparison_file}")

if __name__ == "__main__":
    main()