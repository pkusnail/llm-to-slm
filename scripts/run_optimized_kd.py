#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„KDçŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬
å……åˆ†åˆ©ç”¨8ä¸ªA100 GPUï¼Œå®æ–½æ‰€æœ‰ä¼˜åŒ–å»ºè®®
"""
import os
import sys
import json
import argparse
from pathlib import Path
import time
from typing import Dict, Any

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from distillation.kd import run_kd_pipeline
from utils.common import setup_logging, set_seed, print_gpu_utilization, cleanup_cache

import logging
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

# wandbé›†æˆ
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸ wandbæœªå®‰è£…ï¼Œå°†è·³è¿‡å®éªŒè·Ÿè¸ªã€‚å®‰è£…å‘½ä»¤: pip install wandb")

console = Console()
logger = logging.getLogger(__name__)


def run_optimized_kd_training(
    # åŸºç¡€å‚æ•°
    teacher_model: str,
    student_model: str,
    output_dir: str,
    train_data: str,
    eval_data: str = None,
    experiment_name: str = "optimized_kd_training",
    
    # ğŸš€ ä¼˜åŒ–çš„KDè®­ç»ƒå‚æ•°
    kd_epochs: int = 4,  # å¢åŠ åˆ°4ä¸ªepoch
    kd_batch_size: int = 4,  # å¢å¤§batch sizeå……åˆ†åˆ©ç”¨GPU
    kd_lr: float = 1.5e-4,  # åŸºç¡€å­¦ä¹ ç‡
    kd_grad_accum: int = 32,  # ä¿æŒgradient accumulation
    
    # ğŸŒ¡ï¸ ä¼˜åŒ–çš„è’¸é¦å‚æ•°
    temperature: float = 8.0,  # æé«˜æ¸©åº¦åˆ°8.0
    alpha: float = 0.8,  # KLæŸå¤±æƒé‡
    
    # ğŸ“ æ‰©å±•åºåˆ—é•¿åº¦
    max_length: int = 1536,  # ä»1024å¢åŠ åˆ°1536
    
    # ğŸ“Š è¯„ä¼°å’Œç›‘æ§å‚æ•°
    eval_steps: int = 250,  # æ¯250æ­¥è¯„ä¼°ä¸€æ¬¡
    logging_steps: int = 50,  # æ›´é¢‘ç¹çš„æ—¥å¿—
    save_steps: int = 500,  # å®šæœŸä¿å­˜checkpoint
    
    # ğŸ”§ å­¦ä¹ ç‡è°ƒåº¦å‚æ•°
    warmup_ratio: float = 0.1,  # 10% warmup
    lr_scheduler_type: str = "cosine",  # cosineè°ƒåº¦
    
    # ğŸ¯ æ—©åœå‚æ•°
    early_stopping_patience: int = 3,
    
    # wandbå‚æ•°
    wandb_project: str = "Optimized_KD",
    wandb_tags: list = None
) -> Dict[str, Any]:
    """è¿è¡Œä¼˜åŒ–çš„KDè®­ç»ƒ"""
    
    if wandb_tags is None:
        wandb_tags = ["optimized-hyperparams", "8gpu-utilization", "aiops-data", "temperature-8.0"]
    
    console.print(f"ğŸš€ [bold green]å¯åŠ¨ä¼˜åŒ–KDè®­ç»ƒ[/bold green]: {experiment_name}")
    console.print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    console.print(f"ğŸ‘¨â€ğŸ« Teacher: {teacher_model}")
    console.print(f"ğŸ“ Student: {student_model}")
    
    if WANDB_AVAILABLE and wandb_project:
        wandb.init(
            project=wandb_project,
            name=f"{experiment_name}_{time.strftime('%m%d_%H%M')}",
            tags=wandb_tags,
            config={
                "teacher_model": teacher_model,
                "student_model": student_model,
                "kd_epochs": kd_epochs,
                "kd_batch_size": kd_batch_size,
                "kd_lr": kd_lr,
                "kd_grad_accum": kd_grad_accum,
                "effective_batch_size": kd_batch_size * kd_grad_accum,
                "temperature": temperature,
                "alpha": alpha,
                "max_length": max_length,
                "warmup_ratio": warmup_ratio,
                "lr_scheduler_type": lr_scheduler_type,
                "dataset": "real_aiops_cloud_native",
                "gpu_optimization": "8xA100_full_utilization"
            }
        )
        console.print(f"ğŸ“Š wandbå·²åˆå§‹åŒ–: https://wandb.ai/{wandb.run.entity}/{wandb_project}/runs/{wandb.run.id}")
    
    console.print("\nğŸ“‹ [bold yellow]ä¼˜åŒ–è®­ç»ƒé…ç½®[/bold yellow]:")
    console.print(f"   ğŸŒ¡ï¸  è’¸é¦æ¸©åº¦: [red]{temperature}[/red] (ä»2.5ä¼˜åŒ–åˆ°8.0)")
    console.print(f"   ğŸ”„ è®­ç»ƒè½®æ•°: [red]{kd_epochs}[/red] epochs (ä»1å¢åŠ åˆ°4)")
    console.print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: [red]{kd_batch_size}[/red] (ä»2å¢åŠ åˆ°4)")
    console.print(f"   ğŸ“ˆ æœ‰æ•ˆæ‰¹æ¬¡: [red]{kd_batch_size * kd_grad_accum}[/red] (ä»64å¢åŠ åˆ°128)")
    console.print(f"   ğŸ¯ å­¦ä¹ ç‡: [red]{kd_lr}[/red] + {lr_scheduler_type}è°ƒåº¦")
    console.print(f"   ğŸ“ åºåˆ—é•¿åº¦: [red]{max_length}[/red] (ä»1024å¢åŠ åˆ°1536)")
    console.print(f"   â±ï¸  è¯„ä¼°é¢‘ç‡: æ¯[red]{eval_steps}[/red]æ­¥")
    console.print(f"   ğŸ›¡ï¸  æ—©åœæœºåˆ¶: [red]{early_stopping_patience}[/red]æ¬¡æ— æ”¹å–„åˆ™åœæ­¢")
    
    # æ‰“å°å½“å‰GPUçŠ¶æ€
    print_gpu_utilization()
    
    console.print("\nğŸš€ [bold green]å¯åŠ¨ä¼˜åŒ–KDè’¸é¦æµç¨‹[/bold green] (å……åˆ†åˆ©ç”¨8ä¸ªA100)...")
    console.print("ğŸ’¡ [yellow]ä¼˜åŒ–é‡ç‚¹[/yellow]:")
    console.print("   â€¢ 8ä¸ªGPUå…¨éƒ¨åˆ©ç”¨ï¼šTeacheråˆ†å¸ƒæ›´å‡åŒ€")
    console.print("   â€¢ æ›´å¤§æ‰¹æ¬¡ï¼šæé«˜GPUååé‡")
    console.print("   â€¢ æ›´é«˜æ¸©åº¦ï¼šæ›´å¥½çš„çŸ¥è¯†ä¼ é€’")
    console.print("   â€¢ å¤šepochï¼šå……åˆ†å­¦ä¹ ")
    console.print("   â€¢ å®šæœŸè¯„ä¼°ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ")
    
    try:
        result = run_kd_pipeline(
            teacher_model_path=teacher_model,
            student_model_path=student_model,
            train_data_path=train_data,
            eval_data_path=eval_data,
            output_dir=output_dir,
            experiment_name=experiment_name,
            
            # KDè®­ç»ƒå‚æ•°
            kd_epochs=kd_epochs,
            kd_batch_size=kd_batch_size,
            kd_lr=kd_lr,
            kd_grad_accum=kd_grad_accum,
            
            # è’¸é¦å‚æ•°
            temperature=temperature,
            alpha=alpha,
            
            # åºåˆ—é•¿åº¦
            max_length=max_length,
            
            # è¯„ä¼°å‚æ•°
            eval_steps=eval_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            
            # å­¦ä¹ ç‡è°ƒåº¦
            warmup_ratio=warmup_ratio,
            lr_scheduler_type=lr_scheduler_type,
            
            # æ—©åœ
            early_stopping_patience=early_stopping_patience,
            
            # ç¡¬ä»¶ä¼˜åŒ–
            use_bf16=True,
            gradient_checkpointing=True,
            dataloader_num_workers=8,  # æ›´å¤šæ•°æ®åŠ è½½çº¿ç¨‹
            
            # GPUåˆ†é…ä¼˜åŒ–ï¼šæ›´å‡åŒ€åˆ†å¸ƒTeacheræ¨¡å‹
            use_manual_device_map=True,
            teacher_device_map="auto",  # è®©ç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–Teacheråˆ†å¸ƒ
            student_device_map="auto",  # Studentä¹Ÿè‡ªåŠ¨åˆ†å¸ƒ
            
            # wandb
            use_wandb=WANDB_AVAILABLE and wandb_project is not None,
            wandb_project=wandb_project,
            wandb_tags=wandb_tags
        )
        
        console.print(f"\nâœ… [bold green]ä¼˜åŒ–KDè®­ç»ƒå®Œæˆï¼[/bold green]")
        
        # æ˜¾ç¤ºä¼˜åŒ–æ•ˆæœæ€»ç»“
        if "execution_time" in result:
            total_time = result["execution_time"]
            console.print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: [green]{total_time:.1f}ç§’[/green] ({total_time/3600:.1f}å°æ—¶)")
            
        if "final_loss" in result:
            final_loss = result["final_loss"]
            console.print(f"ğŸ“‰ æœ€ç»ˆLoss: [green]{final_loss:.4f}[/green]")
            
        if "output_files" in result and "model" in result["output_files"]:
            model_path = result["output_files"]["model"]
            console.print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜: [blue]{model_path}[/blue]")
        
        # GPUåˆ©ç”¨ç‡æœ€ç»ˆç»Ÿè®¡
        print_gpu_utilization()
        
        return result
        
    except KeyboardInterrupt:
        console.print("\nâš ï¸ [yellow]è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­[/yellow]")
        cleanup_cache()
        return {"status": "interrupted"}
    except Exception as e:
        console.print(f"\nâŒ [red]è®­ç»ƒå¤±è´¥[/red]: {str(e)}")
        cleanup_cache()
        raise e
    finally:
        if WANDB_AVAILABLE:
            wandb.finish()


def main():
    parser = argparse.ArgumentParser(
        description="ä¼˜åŒ–çš„KDçŸ¥è¯†è’¸é¦è®­ç»ƒ - å……åˆ†åˆ©ç”¨8ä¸ªA100 GPU",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--teacher_model", required=True, help="æ•™å¸ˆæ¨¡å‹è·¯å¾„æˆ–åç§°")
    parser.add_argument("--student_model", required=True, help="å­¦ç”Ÿæ¨¡å‹è·¯å¾„æˆ–åç§°")  
    parser.add_argument("--train_data", required=True, help="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºç›®å½•")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--eval_data", help="è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--experiment_name", default="optimized_kd", help="å®éªŒåç§°")
    
    # ğŸš€ ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°
    parser.add_argument("--kd_epochs", type=int, default=4, help="KDè®­ç»ƒè½®æ•°")
    parser.add_argument("--kd_batch_size", type=int, default=4, help="KDæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--kd_lr", type=float, default=1.5e-4, help="KDå­¦ä¹ ç‡")
    parser.add_argument("--kd_grad_accum", type=int, default=32, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    
    # ğŸŒ¡ï¸ è’¸é¦å‚æ•°
    parser.add_argument("--temperature", type=float, default=8.0, help="è’¸é¦æ¸©åº¦")
    parser.add_argument("--alpha", type=float, default=0.8, help="KLæŸå¤±æƒé‡")
    
    # ğŸ“ åºåˆ—å‚æ•°
    parser.add_argument("--max_length", type=int, default=1536, help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # ğŸ“Š è¯„ä¼°å‚æ•°
    parser.add_argument("--eval_steps", type=int, default=250, help="è¯„ä¼°æ­¥æ•°é—´éš”")
    parser.add_argument("--logging_steps", type=int, default=50, help="æ—¥å¿—æ­¥æ•°é—´éš”")
    parser.add_argument("--save_steps", type=int, default=500, help="ä¿å­˜æ­¥æ•°é—´éš”")
    
    # ğŸ”§ å­¦ä¹ ç‡è°ƒåº¦
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="é¢„çƒ­æ¯”ä¾‹")
    parser.add_argument("--lr_scheduler_type", default="cosine", help="å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹")
    
    # ğŸ¯ æ—©åœ
    parser.add_argument("--early_stopping_patience", type=int, default=3, help="æ—©åœè€å¿ƒå€¼")
    
    # wandbå‚æ•°
    parser.add_argument("--wandb_project", default="Optimized_KD", help="wandbé¡¹ç›®å")
    parser.add_argument("--wandb_tags", nargs="+", 
                       default=["optimized-hyperparams", "8gpu-utilization", "aiops-data"],
                       help="wandbæ ‡ç­¾")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--log_level", default="INFO", help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—å’Œéšæœºç§å­
    setup_logging(level=getattr(logging, args.log_level.upper()))
    set_seed(args.seed)
    
    console.print(f"ğŸ¯ [bold blue]ä¼˜åŒ–KDè®­ç»ƒå¯åŠ¨[/bold blue]")
    console.print(f"ğŸ”§ éšæœºç§å­: {args.seed}")
    console.print(f"ğŸ“‹ æ—¥å¿—çº§åˆ«: {args.log_level}")
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not Path(args.train_data).exists():
        console.print(f"âŒ [red]è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨[/red]: {args.train_data}")
        return 1
    
    if args.eval_data and not Path(args.eval_data).exists():
        console.print(f"âŒ [red]è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨[/red]: {args.eval_data}")
        return 1
    
    try:
        result = run_optimized_kd_training(
            teacher_model=args.teacher_model,
            student_model=args.student_model,
            output_dir=args.output_dir,
            train_data=args.train_data,
            eval_data=args.eval_data,
            experiment_name=args.experiment_name,
            
            kd_epochs=args.kd_epochs,
            kd_batch_size=args.kd_batch_size,
            kd_lr=args.kd_lr,
            kd_grad_accum=args.kd_grad_accum,
            
            temperature=args.temperature,
            alpha=args.alpha,
            
            max_length=args.max_length,
            
            eval_steps=args.eval_steps,
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
            
            warmup_ratio=args.warmup_ratio,
            lr_scheduler_type=args.lr_scheduler_type,
            
            early_stopping_patience=args.early_stopping_patience,
            
            wandb_project=args.wandb_project,
            wandb_tags=args.wandb_tags
        )
        
        console.print(f"\nğŸ‰ [bold green]ä¼˜åŒ–KDè®­ç»ƒæˆåŠŸå®Œæˆï¼[/bold green]")
        return 0
        
    except KeyboardInterrupt:
        console.print(f"\nâš ï¸ [yellow]è®­ç»ƒè¢«ä¸­æ–­[/yellow]")
        return 130
    except Exception as e:
        console.print(f"\nâŒ [red]è®­ç»ƒå¤±è´¥[/red]: {str(e)}")
        logger.exception("è®­ç»ƒå¼‚å¸¸")
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)