#!/usr/bin/env python3
"""
KDçŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬
ç‹¬ç«‹è¿è¡ŒKDé˜¶æ®µï¼Œæ”¯æŒåœ¨çº¿å’Œç¦»çº¿è’¸é¦
"""
import os
import sys
import json
import argparse
from pathlib import Path
import time
from typing import Dict, Any

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from distillation.kd import run_kd_pipeline
from utils.common import setup_logging, set_seed, print_gpu_utilization, cleanup_cache

import logging
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

console = Console()
logger = logging.getLogger(__name__)


def run_kd_training(
    # åŸºç¡€å‚æ•°
    teacher_model: str,
    student_model: str,
    output_dir: str,
    train_data: str,
    eval_data: str = None,
    experiment_name: str = "kd_training",
    
    # KDè®­ç»ƒå‚æ•°
    kd_epochs: int = 1,
    kd_batch_size: int = 1,
    kd_lr: float = 1.5e-4,
    kd_grad_accum: int = 16,
    
    # KDè’¸é¦å‚æ•°
    temperature: float = 2.0,
    alpha: float = 0.5,
    use_online_kd: bool = True,
    
    # ç³»ç»Ÿå‚æ•°
    max_length: int = 2048,
    use_bf16: bool = True,
    gradient_checkpointing: bool = True,
    seed: int = 42,
    
    # é«˜çº§é€‰é¡¹
    save_steps: int = 1000,
    eval_steps: int = 1000,
    logging_steps: int = 50,
    warmup_ratio: float = 0.1,
    generate_teacher_logits: bool = False,
    logits_batch_size: int = 2
):
    """è¿è¡ŒKDè®­ç»ƒ"""
    
    output_path = Path(output_dir) / experiment_name
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(str(output_path / "kd_training.log"))
    set_seed(seed)
    
    console.print(f"ğŸ§  å¼€å§‹KDè’¸é¦è®­ç»ƒ: {experiment_name}")
    console.print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    console.print(f"ğŸ‘¨â€ğŸ« Teacher: {teacher_model}")
    console.print(f"ğŸ“ Student: {student_model}")
    
    # æ˜¾ç¤ºé…ç½®
    console.print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    console.print(f"   è’¸é¦æ¨¡å¼: {'åœ¨çº¿KD' if use_online_kd else 'ç¦»çº¿KD'}")
    console.print(f"   æ‰¹æ¬¡å¤§å°: {kd_batch_size}")
    console.print(f"   è®­ç»ƒè½®æ•°: {kd_epochs}")
    console.print(f"   å­¦ä¹ ç‡: {kd_lr}")
    console.print(f"   æ¢¯åº¦ç´¯ç§¯: {kd_grad_accum}")
    console.print(f"   è’¸é¦æ¸©åº¦: {temperature}")
    console.print(f"   æŸå¤±æƒé‡Î±: {alpha}")
    console.print(f"   åºåˆ—é•¿åº¦: {max_length}")
    
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if not Path(train_data).exists():
        console.print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_data}")
        return False
    
    if eval_data and not Path(eval_data).exists():
        console.print(f"âŒ è¯„ä¼°æ•°æ®ä¸å­˜åœ¨: {eval_data}")
        eval_data = None
    
    # æ£€æŸ¥å­¦ç”Ÿæ¨¡å‹ (å…è®¸HuggingFaceæ¨¡å‹æ ‡è¯†ç¬¦)
    def is_huggingface_model_id(model_path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯HuggingFaceæ¨¡å‹æ ‡è¯†ç¬¦"""
        return "/" in model_path and not Path(model_path).exists()
    
    if not Path(student_model).exists() and not is_huggingface_model_id(student_model):
        console.print(f"âŒ å­¦ç”Ÿæ¨¡å‹ä¸å­˜åœ¨: {student_model}")
        console.print("ğŸ’¡ è¯·å…ˆè¿è¡ŒSFTè®­ç»ƒæˆ–ç¡®ä¿å­¦ç”Ÿæ¨¡å‹è·¯å¾„æ­£ç¡®")
        console.print("ğŸ’¡ æˆ–è€…ä½¿ç”¨HuggingFaceæ¨¡å‹æ ‡è¯†ç¬¦ï¼Œå¦‚ 'Qwen/Qwen3-8B'")
        return False
    
    # æ£€æŸ¥GPUçŠ¶æ€
    print_gpu_utilization()
    
    # ä¿å­˜é…ç½®
    config = {
        "experiment_name": experiment_name,
        "timestamp": time.strftime("%Y%m%d_%H%M%S"),
        "models": {
            "teacher": teacher_model,
            "student": student_model
        },
        "data": {
            "train": train_data,
            "eval": eval_data
        },
        "training": {
            "epochs": kd_epochs,
            "batch_size": kd_batch_size,
            "learning_rate": kd_lr,
            "gradient_accumulation_steps": kd_grad_accum,
            "max_length": max_length
        },
        "distillation": {
            "temperature": temperature,
            "alpha": alpha,
            "use_online_kd": use_online_kd,
            "generate_teacher_logits": generate_teacher_logits
        },
        "hardware": {
            "use_bf16": use_bf16,
            "gradient_checkpointing": gradient_checkpointing
        },
        "random_seed": seed
    }
    
    with open(output_path / "kd_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    start_time = time.time()
    
    try:
        console.print(f"\nğŸš€ å¯åŠ¨KDè’¸é¦æµç¨‹ ({'åœ¨çº¿æ¨¡å¼' if use_online_kd else 'ç¦»çº¿æ¨¡å¼'})...")
        
        # è®­ç»ƒå‚æ•°
        training_args = {
            "per_device_train_batch_size": kd_batch_size,
            "num_train_epochs": kd_epochs,
            "learning_rate": kd_lr,
            "gradient_accumulation_steps": kd_grad_accum,
            "save_steps": save_steps,
            "logging_steps": logging_steps,
            "eval_steps": eval_steps,
            "eval_strategy": "steps" if eval_data else "no",
            "warmup_ratio": warmup_ratio,
            "bf16": use_bf16,
            "gradient_checkpointing": gradient_checkpointing,
            "save_strategy": "steps",
            "load_best_model_at_end": True if eval_data else False,
            "metric_for_best_model": "eval_loss" if eval_data else None,
            "report_to": "none"
        }
        
        # KDå‚æ•°
        kd_args = {
            "temperature": temperature,
            "alpha": alpha,
            "use_online_kd": use_online_kd
        }
        
        # æ˜¾å­˜ä½¿ç”¨æç¤º
        if use_online_kd:
            console.print("âš ï¸ åœ¨çº¿KDéœ€è¦åŒæ—¶åŠ è½½Teacherå’ŒStudentæ¨¡å‹ï¼Œæ˜¾å­˜éœ€æ±‚è¾ƒå¤§")
            console.print("ğŸ’¡ å¦‚é‡OOMï¼Œå¯å°è¯•: --kd_batch_size 1 --kd_grad_accum 32")
        else:
            console.print("ğŸ’¾ ç¦»çº¿KDæ¨¡å¼ï¼Œæ˜¾å­˜éœ€æ±‚è¾ƒå°ä½†éœ€è¦é¢„ç”Ÿæˆlogits")
        
        # è¿è¡ŒKDæµç¨‹
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]KDè’¸é¦è®­ç»ƒä¸­..."),
            console=console
        ) as progress:
            task = progress.add_task("KD", total=None)
            
            training_history = run_kd_pipeline(
                teacher_model_path=teacher_model,
                student_model_path=student_model,
                train_data_path=train_data,
                output_dir=str(output_path),
                eval_data_path=eval_data,
                use_online_kd=use_online_kd,
                generate_teacher_logits=generate_teacher_logits,
                training_args=training_args,
                kd_args=kd_args,
                max_length=max_length,
                seed=seed,
                logits_batch_size=logits_batch_size
            )
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        result = {
            "config": config,
            "training_history": training_history[-10:] if training_history else [],
            "execution_time": time.time() - start_time,
            "output_files": {
                "model": str(output_path / "final_model"),
                "train_data": train_data,
                "eval_data": eval_data
            }
        }
        
        with open(output_path / "kd_results.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        cleanup_cache()
        
        console.print(f"\nğŸ‰ KDè’¸é¦è®­ç»ƒå®Œæˆ!")
        console.print(f"â±ï¸ è®­ç»ƒç”¨æ—¶: {result['execution_time']:.1f}ç§’")
        console.print(f"ğŸ“Š æ¨¡å‹ä¿å­˜åˆ°: {output_path / 'final_model'}")
        
        # ä¸‹ä¸€æ­¥å»ºè®®
        console.print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        console.print(f"   1. è¿è¡Œè¯„ä¼°: python scripts/run_eval.py --model {output_path / 'final_model'}")
        console.print(f"   2. å¯¹æ¯”æ¨¡å‹: æ¯”è¾ƒSFTæ¨¡å‹å’ŒKDæ¨¡å‹çš„æ€§èƒ½å·®å¼‚")
        
        return True
        
    except Exception as e:
        logger.error(f"KDè®­ç»ƒå¤±è´¥: {e}")
        console.print(f"âŒ KDè®­ç»ƒå¤±è´¥: {e}")
        
        # é”™è¯¯è¯Šæ–­æç¤º
        console.print(f"\nğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        if "CUDA out of memory" in str(e):
            console.print("   - å‡å°‘batch_size: --kd_batch_size 1")
            console.print("   - å¢åŠ æ¢¯åº¦ç´¯ç§¯: --kd_grad_accum 32")
            console.print("   - å‡å°‘åºåˆ—é•¿åº¦: --max_length 1024")
            console.print("   - è€ƒè™‘ç¦»çº¿KD: --use_online_kd False")
        elif "model" in str(e).lower():
            console.print("   - æ£€æŸ¥Teacheræ¨¡å‹è·¯å¾„")
            console.print("   - ç¡®è®¤Studentæ¨¡å‹å­˜åœ¨(éœ€å…ˆè¿è¡ŒSFT)")
            console.print("   - éªŒè¯æ¨¡å‹æ ¼å¼å’Œå…¼å®¹æ€§")
        elif "data" in str(e).lower():
            console.print("   - æ£€æŸ¥è®­ç»ƒæ•°æ®æ ¼å¼")
            console.print("   - ç¡®è®¤æ•°æ®è·¯å¾„æ­£ç¡®")
            console.print("   - éªŒè¯æ•°æ®åŒ…å«teacher_responseå­—æ®µ")
        
        return False


def main():
    parser = argparse.ArgumentParser(
        description="KDçŸ¥è¯†è’¸é¦è®­ç»ƒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ§  ä½¿ç”¨ç¤ºä¾‹:

1. åŸºç¡€KDè®­ç»ƒ (ä½¿ç”¨SFTæ¨¡å‹):
   python scripts/run_kd.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "outputs/my_sft/final_model" \\
     --train_data "outputs/my_sft/sft_train_data_clean.jsonl" \\
     --output_dir outputs --experiment_name my_kd

2. å¿«é€ŸéªŒè¯é…ç½®:
   python scripts/run_kd.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "outputs/my_sft/final_model" \\
     --train_data "outputs/my_sft/sft_train_data_clean.jsonl" \\
     --preset quick --output_dir outputs --experiment_name quick_kd

3. ç¦»çº¿KD (èŠ‚çœæ˜¾å­˜):
   python scripts/run_kd.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "outputs/my_sft/final_model" \\
     --train_data "outputs/my_sft/sft_train_data_clean.jsonl" \\
     --use_online_kd False --output_dir outputs --experiment_name offline_kd

4. è‡ªå®šä¹‰è’¸é¦å‚æ•°:
   python scripts/run_kd.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "outputs/my_sft/final_model" \\
     --train_data "outputs/my_sft/sft_train_data_clean.jsonl" \\
     --temperature 3.0 --alpha 0.3 --kd_batch_size 1 \\
     --output_dir outputs --experiment_name custom_kd

ğŸ“š è¯¦ç»†å‚æ•°æŒ‡å—è¯·æŸ¥çœ‹: docs/PARAMETER_GUIDE.md

âš ï¸  æ³¨æ„äº‹é¡¹:
   - KDéœ€è¦å…ˆå®ŒæˆSFTè®­ç»ƒ
   - åœ¨çº¿KDæ˜¾å­˜éœ€æ±‚å¤§ï¼Œå»ºè®®batch_size=1
   - ç¦»çº¿KDéœ€è¦é¢å¤–æ—¶é—´ç”Ÿæˆlogitsä½†èŠ‚çœæ˜¾å­˜
        """
    )
    
    # === åŸºç¡€å‚æ•° ===
    basic_group = parser.add_argument_group('åŸºç¡€é…ç½®')
    basic_group.add_argument("--teacher_model", type=str, required=True,
                           help="æ•™å¸ˆæ¨¡å‹è·¯å¾„")
    basic_group.add_argument("--student_model", type=str, required=True,
                           help="å­¦ç”Ÿæ¨¡å‹è·¯å¾„ (é€šå¸¸æ˜¯SFTè®­ç»ƒåçš„æ¨¡å‹)")
    basic_group.add_argument("--train_data", type=str, required=True,
                           help="è®­ç»ƒæ•°æ®è·¯å¾„ (æ¨èä½¿ç”¨SFTç”Ÿæˆçš„æ¸…ç†æ•°æ®)")
    basic_group.add_argument("--output_dir", type=str, required=True,
                           help="è¾“å‡ºæ ¹ç›®å½•")
    basic_group.add_argument("--experiment_name", type=str, default="kd_training",
                           help="å®éªŒåç§°")
    basic_group.add_argument("--eval_data", type=str,
                           help="è¯„ä¼°æ•°æ®è·¯å¾„")
    
    # === é¢„è®¾é…ç½® ===
    preset_group = parser.add_argument_group('é¢„è®¾é…ç½®')
    preset_group.add_argument("--preset", type=str,
                            choices=['quick', 'standard', 'high_quality'],
                            help="é¢„è®¾é…ç½®: quick(å¿«é€Ÿ), standard(æ ‡å‡†), high_quality(é«˜è´¨é‡)")
    
    # === KDè®­ç»ƒå‚æ•° ===
    kd_group = parser.add_argument_group('KDè®­ç»ƒå‚æ•°')
    kd_group.add_argument("--kd_epochs", type=int, default=1,
                        help="è®­ç»ƒè½®æ•° (æ¨è1-2)")
    kd_group.add_argument("--kd_batch_size", type=int, default=1,
                        help="æ‰¹æ¬¡å¤§å° (åœ¨çº¿KDæ¨è1-2)")
    kd_group.add_argument("--kd_lr", type=float, default=1.5e-4,
                        help="å­¦ä¹ ç‡ (ç•¥ä½äºSFT)")
    kd_group.add_argument("--kd_grad_accum", type=int, default=16,
                        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (è¡¥å¿å°batch_size)")
    
    # === KDè’¸é¦å‚æ•° ===
    distill_group = parser.add_argument_group('è’¸é¦å‚æ•°')
    distill_group.add_argument("--temperature", type=float, default=2.0,
                             help="è’¸é¦æ¸©åº¦ (1.5-4.0ï¼Œè¶Šå¤§è¶Šå¹³æ»‘)")
    distill_group.add_argument("--alpha", type=float, default=0.5,
                             help="æŸå¤±æƒé‡ (0.3åé‡è’¸é¦ï¼Œ0.7åé‡ä»»åŠ¡)")
    distill_group.add_argument("--use_online_kd", type=bool, default=True,
                             help="ä½¿ç”¨åœ¨çº¿KD (Falseä¸ºç¦»çº¿KD)")
    distill_group.add_argument("--generate_teacher_logits", type=bool, default=False,
                             help="ç”ŸæˆTeacher logits (ç¦»çº¿KDæ—¶ä½¿ç”¨)")
    distill_group.add_argument("--logits_batch_size", type=int, default=2,
                             help="ç”Ÿæˆlogitsçš„æ‰¹æ¬¡å¤§å°")
    
    # === ç³»ç»Ÿå‚æ•° ===
    sys_group = parser.add_argument_group('ç³»ç»Ÿå‚æ•°')
    sys_group.add_argument("--max_length", type=int, default=2048,
                         help="æœ€å¤§åºåˆ—é•¿åº¦")
    sys_group.add_argument("--use_bf16", type=bool, default=True,
                         help="ä½¿ç”¨bf16æ··åˆç²¾åº¦")
    sys_group.add_argument("--gradient_checkpointing", type=bool, default=True,
                         help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    sys_group.add_argument("--seed", type=int, default=42,
                         help="éšæœºç§å­")
    
    # === é«˜çº§é€‰é¡¹ ===
    advanced_group = parser.add_argument_group('é«˜çº§é€‰é¡¹')
    advanced_group.add_argument("--save_steps", type=int, default=1000,
                              help="ä¿å­˜é—´éš”")
    advanced_group.add_argument("--eval_steps", type=int, default=1000,
                              help="è¯„ä¼°é—´éš”")
    advanced_group.add_argument("--logging_steps", type=int, default=50,
                              help="æ—¥å¿—é—´éš”")
    advanced_group.add_argument("--warmup_ratio", type=float, default=0.1,
                              help="å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    # åº”ç”¨é¢„è®¾é…ç½®
    if args.preset:
        if args.preset == 'quick':
            # å¿«é€ŸéªŒè¯é…ç½®
            args.kd_batch_size = 2
            args.kd_grad_accum = 8
            args.max_length = 1024
            console.print("ğŸš€ ä½¿ç”¨å¿«é€ŸéªŒè¯é…ç½®")
        elif args.preset == 'standard':
            # æ ‡å‡†é…ç½®
            args.kd_batch_size = 1
            args.kd_grad_accum = 16
            args.max_length = 2048
            console.print("â­ ä½¿ç”¨æ ‡å‡†æ¨èé…ç½®")
        elif args.preset == 'high_quality':
            # é«˜è´¨é‡é…ç½®
            args.kd_epochs = 2
            args.kd_batch_size = 1
            args.kd_grad_accum = 32
            args.max_length = 4096
            args.temperature = 3.0
            args.alpha = 0.3
            args.kd_lr = 1e-4
            console.print("ğŸ’ ä½¿ç”¨é«˜è´¨é‡é…ç½®")
    
    # è¿è¡ŒKDè®­ç»ƒ (ç§»é™¤presetå‚æ•°)
    args_dict = vars(args)
    args_dict.pop('preset', None)  # ç§»é™¤presetå‚æ•°
    success = run_kd_training(**args_dict)
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())