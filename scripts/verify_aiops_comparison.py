#!/usr/bin/env python3
"""
Direct AIOps comparison between 30B Teacher and 8B Student
éªŒè¯8Båœ¨AIOpsæ–¹é¢æ˜¯å¦çœŸçš„æ¯”30Bå¼º
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import time
from pathlib import Path

def load_model(model_path, base_model=None):
    """åŠ è½½æ¨¡å‹"""
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    if base_model is None:
        base_model = model_path
    
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    model.eval()
    return model, tokenizer

def test_aiops_knowledge(model, tokenizer, model_name):
    """AIOpsä¸“ä¸šçŸ¥è¯†æµ‹è¯•"""
    test_cases = [
        {
            "category": "Kubernetes",
            "question": "å¦‚ä½•æŸ¥çœ‹Kubernetesé›†ç¾¤ä¸­æ‰€æœ‰Podçš„çŠ¶æ€ï¼Ÿ",
            "keywords": ["kubectl", "get", "pods", "çŠ¶æ€", "all-namespaces"]
        },
        {
            "category": "ç›‘æ§å‘Šè­¦",
            "question": "Prometheusä¸­å¦‚ä½•é…ç½®CPUä½¿ç”¨ç‡è¶…è¿‡80%çš„å‘Šè­¦è§„åˆ™ï¼Ÿ",
            "keywords": ["alert", "cpu", "usage", "80", "prometheus", "rules"]
        },
        {
            "category": "æ—¥å¿—åˆ†æ",
            "question": "åœ¨å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œå¦‚ä½•å®ç°åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ªï¼Ÿ",
            "keywords": ["tracing", "jaeger", "zipkin", "span", "distributed", "å¾®æœåŠ¡"]
        },
        {
            "category": "æ•…éšœå¤„ç†",
            "question": "å½“åº”ç”¨å“åº”æ—¶é—´è¿‡æ…¢æ—¶ï¼Œåº”è¯¥ä»å“ªäº›æ–¹é¢æ’æŸ¥ï¼Ÿ",
            "keywords": ["cpu", "å†…å­˜", "æ•°æ®åº“", "ç½‘ç»œ", "ç¼“å­˜", "ç´¢å¼•"]
        },
        {
            "category": "å®¹å™¨ä¼˜åŒ–",
            "question": "Dockerå®¹å™¨å†…å­˜ä½¿ç”¨è¿‡é«˜çš„å¸¸è§åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼Ÿ",
            "keywords": ["å†…å­˜æ³„æ¼", "limit", "jvm", "èµ„æºé™åˆ¶", "gc", "heap"]
        },
        {
            "category": "äº‘åŸç”Ÿ",
            "question": "Service Meshçš„ä¸»è¦ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
            "keywords": ["istio", "envoy", "traffic", "security", "observability", "æœåŠ¡ç½‘æ ¼"]
        }
    ]
    
    results = []
    
    for case in test_cases:
        print(f"\næµ‹è¯•ç±»åˆ«: {case['category']}")
        print(f"é—®é¢˜: {case['question']}")
        
        # ç”Ÿæˆå›ç­”
        inputs = tokenizer(f"é—®é¢˜: {case['question']}\nå›ç­”: ", return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.3,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(f"é—®é¢˜: {case['question']}\nå›ç­”: ", "").strip()
        
        print(f"å›ç­”: {response}")
        
        # å…³é”®è¯åŒ¹é…è¯„åˆ†
        matched_keywords = []
        for keyword in case['keywords']:
            if keyword.lower() in response.lower():
                matched_keywords.append(keyword)
        
        score = len(matched_keywords) / len(case['keywords'])
        
        results.append({
            "category": case['category'],
            "question": case['question'],
            "response": response,
            "expected_keywords": case['keywords'],
            "matched_keywords": matched_keywords,
            "score": score
        })
        
        print(f"åŒ¹é…å…³é”®è¯: {matched_keywords}")
        print(f"å¾—åˆ†: {score:.3f} ({len(matched_keywords)}/{len(case['keywords'])})")
    
    overall_score = sum(r['score'] for r in results) / len(results)
    print(f"\nğŸ“Š {model_name} AIOpsæ€»ä½“å¾—åˆ†: {overall_score:.3f}")
    
    return overall_score, results

def main():
    """ä¸»å‡½æ•°ï¼šç›´æ¥å¯¹æ¯”30B Teacherå’Œ8B Studentçš„AIOpsèƒ½åŠ›"""
    print("ğŸ” Direct AIOps Comparison: 30B Teacher vs 8B Student")
    print("="*60)
    
    models_to_test = [
        {
            "name": "Teacher_30B",
            "model_path": "Qwen/Qwen3-30B-A3B-Instruct-2507",
            "base_model": "Qwen/Qwen3-30B-A3B-Instruct-2507"
        },
        {
            "name": "Student_8B_Baseline", 
            "model_path": "Qwen/Qwen3-8B",
            "base_model": "Qwen/Qwen3-8B"
        }
    ]
    
    all_results = {}
    
    for model_config in models_to_test:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_config['name']}")
        print(f"ğŸ“ è·¯å¾„: {model_config['model_path']}")
        print(f"{'='*60}")
        
        try:
            model, tokenizer = load_model(model_config['model_path'], model_config['base_model'])
            overall_score, detailed_results = test_aiops_knowledge(model, tokenizer, model_config['name'])
            
            all_results[model_config['name']] = {
                'overall_score': overall_score,
                'detailed_results': detailed_results
            }
            
            # æ¸…ç†GPUå†…å­˜
            del model, tokenizer
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ æµ‹è¯• {model_config['name']} æ—¶å‡ºé”™: {str(e)}")
            all_results[model_config['name']] = {'overall_score': 0.0, 'error': str(e)}
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“Š AIOpsèƒ½åŠ›ç›´æ¥å¯¹æ¯”")
    print(f"{'='*60}")
    
    teacher_score = all_results.get('Teacher_30B', {}).get('overall_score', 0.0)
    student_score = all_results.get('Student_8B_Baseline', {}).get('overall_score', 0.0)
    
    print(f"Teacher_30B:          {teacher_score:.3f}")
    print(f"Student_8B_Baseline:  {student_score:.3f}")
    
    if student_score > teacher_score:
        print(f"âœ… ç¡®è®¤: 8B Studentåœ¨AIOpsæ–¹é¢ç¡®å®æ¯”30B Teacherå¼º ({student_score:.3f} > {teacher_score:.3f})")
        print("   å¯èƒ½åŸå› : 30Bæ¨¡å‹è¿‡äºé€šç”¨åŒ–ï¼Œ8Bæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°æ›´é›†ä¸­")
    elif teacher_score > student_score:
        print(f"âŒ ç»“æœ: 30B Teacheråœ¨AIOpsæ–¹é¢æ¯”8B Studentå¼º ({teacher_score:.3f} > {student_score:.3f})")
        print("   è¿™ä¸ä¹‹å‰çš„è¯„æµ‹ç»“æœä¸ä¸€è‡´ï¼Œéœ€è¦æ£€æŸ¥è¯„æµ‹æ–¹æ³•")
    else:
        print("ğŸ“Š ä¸¤ä¸ªæ¨¡å‹åœ¨AIOpsæ–¹é¢è¡¨ç°ç›¸å½“")
    
    # ä¿å­˜ç»“æœ
    output_file = "outputs/aiops_direct_comparison.json"
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "comparison_summary": {
                "teacher_30b_score": teacher_score,
                "student_8b_score": student_score,
                "winner": "Student_8B" if student_score > teacher_score else "Teacher_30B" if teacher_score > student_score else "Tie"
            },
            "detailed_results": all_results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    return all_results

if __name__ == "__main__":
    main()