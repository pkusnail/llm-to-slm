#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„è®­ç»ƒæ•°æ®å‡†å¤‡è„šæœ¬
æ•´åˆæ•°æ®ä¸‹è½½ã€å¤„ç†å’Œæ ¼å¼åŒ–ä¸ºä¸€ä¸ªå®Œæ•´çš„æµç¨‹
"""

import json
import os
import random
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datasets import load_dataset
from rich.console import Console
from rich.progress import track

console = Console()

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    dirs = [
        "data/raw", 
        "data/processed",
        "outputs/experiment/prepared_data"
    ]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    console.print("[green]âœ“[/green] ç›®å½•ç»“æ„å·²åˆ›å»º")

def download_gsm8k(sample_size: int = 2000) -> List[Dict]:
    """ä¸‹è½½å¹¶å¤„ç†GSM8Kæ•°æ®é›†"""
    console.print(f"[blue]æ­£åœ¨ä¸‹è½½GSM8Kæ•°æ®é›† (æ ·æœ¬æ•°: {sample_size})...[/blue]")
    
    try:
        dataset = load_dataset("gsm8k", "main", cache_dir="data/raw/.cache")
        train_data = list(dataset['train'])
        
        # ä¿å­˜åŸå§‹æ•°æ®å¤‡ä»½
        with open("data/raw/gsm8k_train.json", "w", encoding="utf-8") as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        # é‡‡æ ·è®­ç»ƒæ•°æ®
        if sample_size < len(train_data):
            sampled_train = random.sample(train_data, sample_size)
        else:
            sampled_train = train_data
        
        processed_data = []
        for i, item in enumerate(track(sampled_train, description="å¤„ç†GSM8K")):
            processed_data.append({
                "id": f"gsm8k_{i:04d}",
                "domain": "math",
                "difficulty": "medium", 
                "prompt": f"è¯·è§£å†³è¿™ä¸ªæ•°å­¦é—®é¢˜ï¼Œè¦æ±‚ç»™å‡ºè¯¦ç»†çš„è§£é¢˜æ­¥éª¤ï¼š\n\n{item['question']}",
                "original_answer": item['answer'],  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                "metadata": {
                    "source": "gsm8k",
                    "original_index": i
                }
            })
        
        console.print(f"[green]âœ“[/green] GSM8Kå¤„ç†å®Œæˆ: {len(processed_data)} æ¡æ ·æœ¬")
        return processed_data
        
    except Exception as e:
        console.print(f"[red]âœ—[/red] GSM8Kä¸‹è½½å¤±è´¥: {e}")
        return []

def download_humaneval(sample_size: int = 164) -> List[Dict]:
    """ä¸‹è½½å¹¶å¤„ç†HumanEvalæ•°æ®é›†"""
    console.print(f"[blue]æ­£åœ¨ä¸‹è½½HumanEvalæ•°æ®é›† (æ ·æœ¬æ•°: {sample_size})...[/blue]")
    
    try:
        dataset = load_dataset("openai_humaneval", cache_dir="data/raw/.cache")
        test_data = list(dataset['test'])
        
        # ä¿å­˜åŸå§‹æ•°æ®å¤‡ä»½
        with open("data/raw/humaneval.json", "w", encoding="utf-8") as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        processed_data = []
        for i, item in enumerate(track(test_data[:sample_size], description="å¤„ç†HumanEval")):
            processed_data.append({
                "id": f"humaneval_{item['task_id']}",
                "domain": "code",
                "difficulty": "hard",
                "prompt": f"è¯·å®Œæˆä»¥ä¸‹Pythonå‡½æ•°ï¼Œè¦æ±‚ä»£ç ç®€æ´é«˜æ•ˆï¼š\n\n```python\n{item['prompt']}\n```\n\nè¯·ç›´æ¥ç»™å‡ºå®Œæ•´çš„å‡½æ•°å®ç°ã€‚",
                "original_answer": item['canonical_solution'],  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                "metadata": {
                    "source": "humaneval", 
                    "task_id": item['task_id'],
                    "test_cases": item['test'],
                    "entry_point": item['entry_point']
                }
            })
        
        console.print(f"[green]âœ“[/green] HumanEvalå¤„ç†å®Œæˆ: {len(processed_data)} æ¡æ ·æœ¬")
        return processed_data
        
    except Exception as e:
        console.print(f"[red]âœ—[/red] HumanEvalä¸‹è½½å¤±è´¥: {e}")
        return []

def generate_aiops_synthetic(sample_size: int = 1000) -> List[Dict]:
    """ç”ŸæˆAIOpsåˆæˆæ•°æ®é›†"""
    console.print(f"[blue]æ­£åœ¨ç”ŸæˆAIOpsåˆæˆæ•°æ®é›† (æ ·æœ¬æ•°: {sample_size})...[/blue]")
    
    # æ•…éšœæ¨¡æ¿
    templates = [
        {
            "scenario": "æ•°æ®åº“è¿æ¥æ± è€—å°½",
            "logs": [
                "2024-09-09 10:23:11 ERROR [ConnectionPool-1] HikariPool timeout after 30s, total=20, active=20, idle=0",
                "2024-09-09 10:23:12 WARN [PaymentService] Database connection failed: pool exhausted", 
                "2024-09-09 10:23:13 ERROR [CircuitBreaker] Circuit opened for mysql-primary:3306"
            ],
            "config": {
                "hikari_pool_size": 20,
                "connection_timeout": 30000,
                "max_lifetime": 300000
            },
            "metrics": {
                "cpu_usage": "45%",
                "memory_usage": "78%",
                "active_connections": 20,
                "qps": 850
            },
            "root_cause": "æ•°æ®åº“è¿æ¥æ± é…ç½®è¿‡å°ï¼Œåœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹æ— æ³•æ»¡è¶³éœ€æ±‚",
            "fix_plan": [
                "å¢åŠ è¿æ¥æ± å¤§å°åˆ°50-100",
                "ä¼˜åŒ–æ…¢æŸ¥è¯¢å‡å°‘è¿æ¥å ç”¨æ—¶é—´", 
                "æ·»åŠ è¿æ¥æ± ç›‘æ§å‘Šè­¦"
            ]
        },
        {
            "scenario": "å†…å­˜æ³„æ¼å¯¼è‡´OOM", 
            "logs": [
                "2024-09-09 11:15:23 WARN [GC] Full GC took 2.3s, heap: 7.8GB/8GB used",
                "2024-09-09 11:15:45 ERROR [JVM] OutOfMemoryError: Java heap space",
                "2024-09-09 11:15:46 ERROR [K8s] Pod order-service-7d9f restarts: 3/5"
            ],
            "config": {
                "jvm_heap": "8GB", 
                "gc_algorithm": "G1GC",
                "pod_memory_limit": "10GB"
            },
            "metrics": {
                "heap_usage": "95%",
                "gc_frequency": "æ¯30sä¸€æ¬¡FullGC",
                "restart_count": 3
            },
            "root_cause": "åº”ç”¨å­˜åœ¨å†…å­˜æ³„æ¼ï¼Œå¯¹è±¡æ— æ³•è¢«GCå›æ”¶å¯¼è‡´å †å†…å­˜è€—å°½",
            "fix_plan": [
                "ä½¿ç”¨Memory Profilerå®šä½å†…å­˜æ³„æ¼ç‚¹",
                "ä¸´æ—¶å¢åŠ å †å†…å­˜åˆ°12GB", 
                "ä¼˜åŒ–å¯¹è±¡ç”Ÿå‘½å‘¨æœŸç®¡ç†"
            ]
        },
        {
            "scenario": "ç£ç›˜ç©ºé—´ä¸è¶³",
            "logs": [
                "2024-09-09 14:30:12 ERROR [FileSystem] Disk usage: 98% on /var/log",
                "2024-09-09 14:30:15 WARN [LogRotate] Cannot create new log file: No space left on device",
                "2024-09-09 14:30:18 ERROR [ES] Elasticsearch cluster RED: disk threshold exceeded"
            ],
            "config": {
                "log_retention": "30å¤©",
                "disk_size": "500GB",
                "es_shard_size": "50GB" 
            },
            "metrics": {
                "disk_usage": "98%",
                "log_growth_rate": "10GB/day",
                "es_index_size": "450GB"
            },
            "root_cause": "æ—¥å¿—æ–‡ä»¶å’ŒESç´¢å¼•å ç”¨è¿‡å¤šç£ç›˜ç©ºé—´ï¼Œä¸”ç¼ºä¹è‡ªåŠ¨æ¸…ç†æœºåˆ¶",
            "fix_plan": [
                "ç«‹å³æ¸…ç†7å¤©å‰çš„æ—¥å¿—æ–‡ä»¶",
                "é…ç½®æ—¥å¿—è‡ªåŠ¨è½®è½¬å’Œå‹ç¼©",
                "è®¾ç½®ESç´¢å¼•è‡ªåŠ¨åˆ é™¤ç­–ç•¥"
            ]
        },
        {
            "scenario": "ç½‘ç»œå»¶è¿Ÿå¼‚å¸¸", 
            "logs": [
                "2024-09-09 16:45:30 WARN [HTTP] Upstream response time: 5.2s (normal: 200ms)",
                "2024-09-09 16:45:33 ERROR [LoadBalancer] Backend server timeout: api-server-3",
                "2024-09-09 16:45:35 INFO [K8s] Node network plugin showing errors"
            ],
            "config": {
                "upstream_timeout": "3s",
                "keepalive_requests": 100,
                "worker_connections": 1024
            },
            "metrics": {
                "avg_response_time": "5.2s",
                "p99_latency": "8.5s", 
                "network_errors": "12/min"
            },
            "root_cause": "ç½‘ç»œæ’ä»¶å¼‚å¸¸å¯¼è‡´è·¨èŠ‚ç‚¹é€šä¿¡å»¶è¿Ÿå¢åŠ ",
            "fix_plan": [
                "é‡å¯å¼‚å¸¸èŠ‚ç‚¹çš„ç½‘ç»œæ’ä»¶",
                "æ£€æŸ¥ç½‘ç»œè®¾å¤‡é…ç½®",
                "ä¸´æ—¶å°†æµé‡åˆ‡æ¢åˆ°æ­£å¸¸èŠ‚ç‚¹"
            ]
        }
    ]
    
    processed_data = []
    for i in track(range(sample_size), description="ç”ŸæˆAIOpsæ•°æ®"):
        template = random.choice(templates)
        
        # æ„å»ºæç¤ºè¯
        prompt_parts = [
            f"## æ•…éšœåœºæ™¯åˆ†æ\n\n**ç³»ç»Ÿæ—¥å¿—:**",
        ]
        
        for log in template["logs"]:
            prompt_parts.append(f"```\n{log}\n```")
        
        prompt_parts.extend([
            f"\n**é…ç½®ä¿¡æ¯:**",
            f"```json\n{json.dumps(template['config'], ensure_ascii=False, indent=2)}\n```",
            f"\n**ç›‘æ§æŒ‡æ ‡:**",
            f"```json\n{json.dumps(template['metrics'], ensure_ascii=False, indent=2)}\n```", 
            f"\nè¯·åˆ†æä¸Šè¿°æ—¥å¿—å’Œé…ç½®ï¼Œæ‰¾å‡ºæœ€å¯èƒ½çš„æ ¹æœ¬åŸå› ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„ä¿®å¤å»ºè®®ã€‚"
        ])
        
        processed_data.append({
            "id": f"aiops_{i:04d}",
            "domain": "aiops",
            "difficulty": "hard",
            "prompt": "\n".join(prompt_parts),
            "original_answer": template["root_cause"],  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
            "metadata": {
                "source": "synthetic",
                "scenario": template["scenario"],
                "fix_plan": template["fix_plan"]
            }
        })
    
    console.print(f"[green]âœ“[/green] AIOpsæ•°æ®ç”Ÿæˆå®Œæˆ: {len(processed_data)} æ¡æ ·æœ¬")
    return processed_data

def save_training_data(data_list: List[List[Dict]], output_dir: str):
    """ä¿å­˜è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨KDè®­ç»ƒæœŸæœ›çš„è·¯å¾„å’Œæ ¼å¼"""
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_data = []
    for data in data_list:
        all_data.extend(data)
    
    # æ‰“ä¹±æ•°æ®
    random.shuffle(all_data)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # åˆ†ç¦»è®­ç»ƒé›†å’Œè¯„ä¼°é›†
    total_size = len(all_data)
    eval_size = min(350, total_size // 10)  # 10%ä½œä¸ºè¯„ä¼°é›†ï¼Œæœ€å¤š350ä¸ª
    
    eval_data = all_data[:eval_size]
    train_data = all_data[eval_size:]
    
    # ä¿å­˜è®­ç»ƒé›† - ä½¿ç”¨KDè„šæœ¬æœŸæœ›çš„è·¯å¾„
    train_path = f"{output_dir}/sft_train_data_clean.jsonl"
    with open(train_path, "w", encoding="utf-8") as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    # ä¿å­˜è¯„ä¼°é›† - ä½¿ç”¨KDè„šæœ¬æœŸæœ›çš„è·¯å¾„  
    eval_path = f"{output_dir}/sft_eval_data_clean.jsonl"
    with open(eval_path, "w", encoding="utf-8") as f:
        for item in eval_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    console.print(f"[green]âœ“[/green] è®­ç»ƒæ•°æ®å·²ä¿å­˜:")
    console.print(f"  ğŸ“ è®­ç»ƒé›†: {train_path} ({len(train_data)} æ¡)")
    console.print(f"  ğŸ“ è¯„ä¼°é›†: {eval_path} ({len(eval_data)} æ¡)")
    
    # ç»Ÿè®¡ä¿¡æ¯
    domain_counts = {}
    for item in all_data:
        domain = item['domain'] 
        domain_counts[domain] = domain_counts.get(domain, 0) + 1
    
    console.print("[cyan]é¢†åŸŸåˆ†å¸ƒ:[/cyan]")
    for domain, count in domain_counts.items():
        console.print(f"  {domain}: {count} æ¡")
    
    return train_path, eval_path

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å‡†å¤‡KDè®­ç»ƒæ•°æ®")
    parser.add_argument("--output_dir", type=str, 
                       default="outputs/experiment/prepared_data",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--gsm8k_samples", type=int, default=2000,
                       help="GSM8Kæ ·æœ¬æ•°")
    parser.add_argument("--humaneval_samples", type=int, default=164, 
                       help="HumanEvalæ ·æœ¬æ•°")
    parser.add_argument("--aiops_samples", type=int, default=1000,
                       help="AIOpsåˆæˆæ ·æœ¬æ•°")
    
    args = parser.parse_args()
    
    console.print("[bold blue]ğŸš€ å¼€å§‹å‡†å¤‡KDè®­ç»ƒæ•°æ®é›†[/bold blue]\\n")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # åˆ›å»ºç›®å½•
    setup_directories()
    
    # ä¸‹è½½å’Œå¤„ç†æ•°æ®é›†
    gsm8k_data = download_gsm8k(sample_size=args.gsm8k_samples)
    humaneval_data = download_humaneval(sample_size=args.humaneval_samples)
    aiops_data = generate_aiops_synthetic(sample_size=args.aiops_samples)
    
    # ä¿å­˜è®­ç»ƒæ•°æ® - ç›´æ¥ä¿å­˜ä¸ºKDè®­ç»ƒå¯ç”¨çš„æ ¼å¼å’Œè·¯å¾„
    if gsm8k_data or humaneval_data or aiops_data:
        train_path, eval_path = save_training_data(
            [gsm8k_data, humaneval_data, aiops_data],
            args.output_dir
        )
        
        console.print("\\n[bold green]ğŸ‰ æ•°æ®é›†å‡†å¤‡å®Œæˆï¼[/bold green]")
        console.print("[cyan]å¯ä»¥ç›´æ¥ç”¨äºKDè®­ç»ƒ:[/cyan]")
        console.print(f"  python scripts/run_improved_kd.py --train_data {train_path} --eval_data {eval_path}")
    else:
        console.print("\\n[red]âŒ æ²¡æœ‰æˆåŠŸå‡†å¤‡ä»»ä½•æ•°æ®é›†[/red]")

if __name__ == "__main__":
    main()