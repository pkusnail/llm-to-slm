#!/usr/bin/env python3
"""
SFTç›‘ç£å¾®è°ƒè®­ç»ƒè„šæœ¬
ç‹¬ç«‹è¿è¡ŒSFTé˜¶æ®µï¼Œæ”¯æŒæ•°æ®ç”Ÿæˆå’Œæ¨¡å‹è®­ç»ƒ
"""
import os
import sys
import json
import argparse
from pathlib import Path
import time
from typing import Dict, Any

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from distillation.sft import run_sft_pipeline
from utils.common import setup_logging, set_seed, print_gpu_utilization, cleanup_cache

import logging
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

console = Console()
logger = logging.getLogger(__name__)


def run_sft_training(
    # åŸºç¡€å‚æ•°
    teacher_model: str,
    student_model: str,
    output_dir: str,
    train_data: str = "data/processed/train_dataset.jsonl",
    eval_data: str = "data/processed/eval_dataset.jsonl",
    experiment_name: str = "sft_training",
    
    # SFTè®­ç»ƒå‚æ•°
    sft_epochs: int = 1,
    sft_batch_size: int = 2,
    sft_lr: float = 2e-4,
    sft_grad_accum: int = 8,
    
    # æ•°æ®ç”Ÿæˆå‚æ•°
    generate_data: bool = True,
    max_new_tokens: int = 256,
    gen_temperature: float = 0.3,
    gen_top_p: float = 0.9,
    
    # ç³»ç»Ÿå‚æ•°
    max_length: int = 2048,
    use_bf16: bool = True,
    gradient_checkpointing: bool = True,
    seed: int = 42,
    
    # é«˜çº§é€‰é¡¹
    use_lora: bool = True,
    save_steps: int = 1000,
    eval_steps: int = 1000,
    logging_steps: int = 50,
    warmup_ratio: float = 0.1
):
    """è¿è¡ŒSFTè®­ç»ƒ"""
    
    output_path = Path(output_dir) / experiment_name
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(str(output_path / "sft_training.log"))
    set_seed(seed)
    
    console.print(f"ğŸ¯ å¼€å§‹SFTè®­ç»ƒ: {experiment_name}")
    console.print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    console.print(f"ğŸ“ Teacher: {teacher_model}")
    console.print(f"ğŸ“ Student: {student_model}")
    
    # æ˜¾ç¤ºé…ç½®
    console.print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    console.print(f"   æ‰¹æ¬¡å¤§å°: {sft_batch_size}")
    console.print(f"   è®­ç»ƒè½®æ•°: {sft_epochs}")
    console.print(f"   å­¦ä¹ ç‡: {sft_lr}")
    console.print(f"   æ¢¯åº¦ç´¯ç§¯: {sft_grad_accum}")
    console.print(f"   åºåˆ—é•¿åº¦: {max_length}")
    console.print(f"   ç”Ÿæˆé•¿åº¦: {max_new_tokens}")
    
    # æ£€æŸ¥GPUçŠ¶æ€
    print_gpu_utilization()
    
    # ä¿å­˜é…ç½®
    config = {
        "experiment_name": experiment_name,
        "timestamp": time.strftime("%Y%m%d_%H%M%S"),
        "models": {
            "teacher": teacher_model,
            "student": student_model
        },
        "data": {
            "train": train_data,
            "eval": eval_data
        },
        "training": {
            "epochs": sft_epochs,
            "batch_size": sft_batch_size,
            "learning_rate": sft_lr,
            "gradient_accumulation_steps": sft_grad_accum,
            "max_length": max_length,
            "use_lora": use_lora
        },
        "generation": {
            "max_new_tokens": max_new_tokens,
            "temperature": gen_temperature,
            "top_p": gen_top_p
        },
        "hardware": {
            "use_bf16": use_bf16,
            "gradient_checkpointing": gradient_checkpointing
        },
        "random_seed": seed
    }
    
    with open(output_path / "sft_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    start_time = time.time()
    
    try:
        console.print("\nğŸš€ å¯åŠ¨SFTè®­ç»ƒæµç¨‹...")
        
        # è®­ç»ƒå‚æ•°
        training_args = {
            "per_device_train_batch_size": sft_batch_size,
            "num_train_epochs": sft_epochs,
            "learning_rate": sft_lr,
            "gradient_accumulation_steps": sft_grad_accum,
            "save_steps": save_steps,
            "logging_steps": logging_steps,
            "eval_steps": eval_steps,
            "eval_strategy": "steps" if eval_data else "no",
            "warmup_ratio": warmup_ratio,
            "bf16": use_bf16,
            "gradient_checkpointing": gradient_checkpointing,
            "save_strategy": "steps",
            "load_best_model_at_end": True if eval_data else False,
            "metric_for_best_model": "eval_loss" if eval_data else None,
            "report_to": "none"
        }
        
        # ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            "max_new_tokens": max_new_tokens,
            "temperature": gen_temperature,
            "top_p": gen_top_p,
            "do_sample": True
        }
        
        # è¿è¡ŒSFTæµç¨‹
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]SFTè®­ç»ƒä¸­..."),
            console=console
        ) as progress:
            task = progress.add_task("SFT", total=None)
            
            training_history = run_sft_pipeline(
                teacher_model_path=teacher_model,
                student_model_path=student_model,
                train_data_path=train_data,
                output_dir=str(output_path),
                generate_teacher_data=generate_data,
                eval_data_path=eval_data,
                training_args=training_args,
                generation_kwargs=generation_kwargs,
                max_length=max_length,
                seed=seed
            )
        
        # å¦‚æœç”Ÿæˆäº†æ–°æ•°æ®ï¼Œæ¸…ç†æ•°æ®æ ¼å¼
        if generate_data:
            console.print("ğŸ”§ æ¸…ç†æ•°æ®æ ¼å¼...")
            try:
                from scripts.optimize_sft_data import optimize_sft_data_structure
                
                # æ¸…ç†è®­ç»ƒæ•°æ®
                if (output_path / "sft_train_data.jsonl").exists():
                    optimize_sft_data_structure(
                        str(output_path / "sft_train_data.jsonl"),
                        str(output_path / "sft_train_data_clean.jsonl")
                    )
                
                # æ¸…ç†è¯„ä¼°æ•°æ®
                if (output_path / "sft_eval_data.jsonl").exists():
                    optimize_sft_data_structure(
                        str(output_path / "sft_eval_data.jsonl"),
                        str(output_path / "sft_eval_data_clean.jsonl")
                    )
                console.print("âœ… æ•°æ®æ ¼å¼æ¸…ç†å®Œæˆ")
            except Exception as e:
                console.print(f"âš ï¸ æ•°æ®æ¸…ç†å¤±è´¥: {e}")
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        result = {
            "config": config,
            "training_history": training_history[-10:] if training_history else [],
            "execution_time": time.time() - start_time,
            "output_files": {
                "model": str(output_path / "final_model"),
                "train_data": str(output_path / "sft_train_data_clean.jsonl") if generate_data else train_data,
                "eval_data": str(output_path / "sft_eval_data_clean.jsonl") if generate_data and eval_data else eval_data
            }
        }
        
        with open(output_path / "sft_results.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        cleanup_cache()
        
        console.print(f"\nğŸ‰ SFTè®­ç»ƒå®Œæˆ!")
        console.print(f"â±ï¸ è®­ç»ƒç”¨æ—¶: {result['execution_time']:.1f}ç§’")
        console.print(f"ğŸ“Š æ¨¡å‹ä¿å­˜åˆ°: {output_path / 'final_model'}")
        
        if generate_data:
            console.print(f"ğŸ“‹ æ¸…ç†åçš„è®­ç»ƒæ•°æ®: {output_path / 'sft_train_data_clean.jsonl'}")
            if eval_data:
                console.print(f"ğŸ“‹ æ¸…ç†åçš„è¯„ä¼°æ•°æ®: {output_path / 'sft_eval_data_clean.jsonl'}")
        
        return True
        
    except Exception as e:
        logger.error(f"SFTè®­ç»ƒå¤±è´¥: {e}")
        console.print(f"âŒ SFTè®­ç»ƒå¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="SFTç›‘ç£å¾®è°ƒè®­ç»ƒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ ä½¿ç”¨ç¤ºä¾‹:

1. åŸºç¡€SFTè®­ç»ƒ:
   python scripts/run_sft.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "Qwen/Qwen3-8B" --output_dir outputs --experiment_name my_sft

2. å¿«é€ŸéªŒè¯é…ç½®:
   python scripts/run_sft.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "Qwen/Qwen3-8B" --output_dir outputs \\
     --preset quick --experiment_name quick_sft

3. é«˜è´¨é‡è®­ç»ƒé…ç½®:
   python scripts/run_sft.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "Qwen/Qwen3-8B" --output_dir outputs \\
     --preset high_quality --experiment_name high_quality_sft

4. ä½¿ç”¨ç°æœ‰æ•°æ®ç›´æ¥SFT (è·³è¿‡Teacherç”Ÿæˆ):
   python scripts/run_sft.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "Qwen/Qwen3-8B" --output_dir outputs \\
     --no_generate_data --experiment_name direct_sft

5. è‡ªå®šä¹‰å‚æ•°:
   python scripts/run_sft.py --teacher_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \\
     --student_model "Qwen/Qwen3-8B" --output_dir outputs \\
     --sft_batch_size 1 --max_length 1024 --experiment_name custom_sft

ğŸ“š è¯¦ç»†å‚æ•°æŒ‡å—è¯·æŸ¥çœ‹: docs/PARAMETER_GUIDE.md
        """
    )
    
    # === åŸºç¡€å‚æ•° ===
    basic_group = parser.add_argument_group('åŸºç¡€é…ç½®')
    basic_group.add_argument("--teacher_model", type=str, required=True,
                           help="æ•™å¸ˆæ¨¡å‹è·¯å¾„")
    basic_group.add_argument("--student_model", type=str, required=True,
                           help="å­¦ç”Ÿæ¨¡å‹è·¯å¾„")
    basic_group.add_argument("--output_dir", type=str, required=True,
                           help="è¾“å‡ºæ ¹ç›®å½•")
    basic_group.add_argument("--experiment_name", type=str, default="sft_training",
                           help="å®éªŒåç§°")
    basic_group.add_argument("--train_data", type=str,
                           default="data/processed/train_dataset.jsonl",
                           help="è®­ç»ƒæ•°æ®è·¯å¾„")
    basic_group.add_argument("--eval_data", type=str,
                           default="data/processed/eval_dataset.jsonl",
                           help="è¯„ä¼°æ•°æ®è·¯å¾„")
    
    # === é¢„è®¾é…ç½® ===
    preset_group = parser.add_argument_group('é¢„è®¾é…ç½®')
    preset_group.add_argument("--preset", type=str, 
                            choices=['quick', 'standard', 'high_quality'],
                            help="é¢„è®¾é…ç½®: quick(å¿«é€Ÿ), standard(æ ‡å‡†), high_quality(é«˜è´¨é‡)")
    
    # === SFTå‚æ•° ===
    sft_group = parser.add_argument_group('SFTè®­ç»ƒå‚æ•°')
    sft_group.add_argument("--sft_epochs", type=int, default=1,
                         help="è®­ç»ƒè½®æ•° (æ¨è1-2)")
    sft_group.add_argument("--sft_batch_size", type=int, default=2,
                         help="æ‰¹æ¬¡å¤§å° (æ¨è1-4ï¼Œæ˜¾å­˜ä¸è¶³æ—¶ç”¨1)")
    sft_group.add_argument("--sft_lr", type=float, default=2e-4,
                         help="å­¦ä¹ ç‡ (æ¨è1e-4åˆ°3e-4)")
    sft_group.add_argument("--sft_grad_accum", type=int, default=8,
                         help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    
    # === æ•°æ®ç”Ÿæˆå‚æ•° ===
    data_group = parser.add_argument_group('æ•°æ®ç”Ÿæˆå‚æ•°')
    data_group.add_argument("--generate_data", action='store_true', default=True,
                          help="ç”ŸæˆTeacheræ•°æ® (é»˜è®¤å¼€å¯)")
    data_group.add_argument("--no_generate_data", action='store_true', 
                          help="è·³è¿‡Teacheræ•°æ®ç”Ÿæˆï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®è®­ç»ƒ")
    data_group.add_argument("--max_new_tokens", type=int, default=256,
                          help="Teacherç”Ÿæˆçš„æœ€å¤§é•¿åº¦")
    data_group.add_argument("--gen_temperature", type=float, default=0.3,
                          help="ç”Ÿæˆæ¸©åº¦ (0.1ç¡®å®šï¼Œ0.7å¤šæ ·)")
    data_group.add_argument("--gen_top_p", type=float, default=0.9,
                          help="nucleusé‡‡æ ·å‚æ•°")
    
    # === ç³»ç»Ÿå‚æ•° ===
    sys_group = parser.add_argument_group('ç³»ç»Ÿå‚æ•°')
    sys_group.add_argument("--max_length", type=int, default=2048,
                         help="æœ€å¤§åºåˆ—é•¿åº¦")
    sys_group.add_argument("--use_bf16", type=bool, default=True,
                         help="ä½¿ç”¨bf16æ··åˆç²¾åº¦")
    sys_group.add_argument("--gradient_checkpointing", type=bool, default=True,
                         help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    sys_group.add_argument("--seed", type=int, default=42,
                         help="éšæœºç§å­")
    
    # === é«˜çº§é€‰é¡¹ ===
    advanced_group = parser.add_argument_group('é«˜çº§é€‰é¡¹')
    advanced_group.add_argument("--use_lora", type=bool, default=True,
                              help="ä½¿ç”¨LoRAå¾®è°ƒ")
    advanced_group.add_argument("--save_steps", type=int, default=1000,
                              help="ä¿å­˜é—´éš”")
    advanced_group.add_argument("--eval_steps", type=int, default=1000,
                              help="è¯„ä¼°é—´éš”")
    advanced_group.add_argument("--logging_steps", type=int, default=50,
                              help="æ—¥å¿—é—´éš”")
    advanced_group.add_argument("--warmup_ratio", type=float, default=0.1,
                              help="å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    # å¤„ç†äº’æ–¥çš„generate_dataé€‰é¡¹
    if args.no_generate_data:
        args.generate_data = False
    
    # åº”ç”¨é¢„è®¾é…ç½®
    if args.preset:
        if args.preset == 'quick':
            # å¿«é€ŸéªŒè¯é…ç½®
            args.sft_batch_size = 4
            args.max_length = 1024
            args.max_new_tokens = 256
            args.sft_grad_accum = 4
            console.print("ğŸš€ ä½¿ç”¨å¿«é€ŸéªŒè¯é…ç½®")
        elif args.preset == 'standard':
            # æ ‡å‡†é…ç½®
            args.sft_batch_size = 2
            args.max_length = 2048
            args.max_new_tokens = 512
            console.print("â­ ä½¿ç”¨æ ‡å‡†æ¨èé…ç½®")
        elif args.preset == 'high_quality':
            # é«˜è´¨é‡é…ç½®
            args.sft_epochs = 2
            args.sft_batch_size = 1
            args.max_length = 4096
            args.max_new_tokens = 1024
            args.sft_lr = 1e-4
            console.print("ğŸ’ ä½¿ç”¨é«˜è´¨é‡é…ç½®")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not Path(args.train_data).exists():
        console.print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {args.train_data}")
        return 1
    
    if args.eval_data and not Path(args.eval_data).exists():
        console.print(f"âŒ è¯„ä¼°æ•°æ®ä¸å­˜åœ¨: {args.eval_data}")
        return 1
    
    # è¿è¡ŒSFTè®­ç»ƒ (ç§»é™¤presetå‚æ•°ï¼Œå› ä¸ºå·²ç»åº”ç”¨åˆ°å…·ä½“å‚æ•°ä¸­)
    success = run_sft_training(
        teacher_model=args.teacher_model,
        student_model=args.student_model,
        output_dir=args.output_dir,
        train_data=args.train_data,
        eval_data=args.eval_data,
        experiment_name=args.experiment_name,
        sft_epochs=args.sft_epochs,
        sft_batch_size=args.sft_batch_size,
        sft_lr=args.sft_lr,
        sft_grad_accum=args.sft_grad_accum,
        generate_data=args.generate_data,
        max_new_tokens=args.max_new_tokens,
        gen_temperature=args.gen_temperature,
        gen_top_p=args.gen_top_p,
        max_length=args.max_length,
        use_bf16=args.use_bf16,
        gradient_checkpointing=args.gradient_checkpointing,
        seed=args.seed,
        use_lora=args.use_lora,
        save_steps=args.save_steps,
        eval_steps=args.eval_steps,
        logging_steps=args.logging_steps,
        warmup_ratio=args.warmup_ratio
    )
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())