#!/usr/bin/env python3
"""
ä¸€é”®ä¸‹è½½å’Œé¢„å¤„ç†æ•°æ®é›†
æ”¯æŒGSM8K, HumanEval, ä»¥åŠç”ŸæˆAIOpsåˆæˆæ•°æ®
"""

import json
import os
import random
from pathlib import Path
from typing import List, Dict, Any
from datasets import load_dataset
from rich.console import Console
from rich.progress import track

console = Console()

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    dirs = ["data/raw", "data/processed"]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    console.print("[green]âœ“[/green] ç›®å½•ç»“æ„å·²åˆ›å»º")

def download_gsm8k(sample_size: int = 2000) -> List[Dict]:
    """ä¸‹è½½å¹¶å¤„ç†GSM8Kæ•°æ®é›†"""
    console.print(f"[blue]æ­£åœ¨ä¸‹è½½GSM8Kæ•°æ®é›† (æ ·æœ¬æ•°: {sample_size})...[/blue]")
    
    try:
        dataset = load_dataset("gsm8k", "main", cache_dir="data/raw/.cache")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        train_data = list(dataset['train'])
        test_data = list(dataset['test'])
        
        with open("data/raw/gsm8k_train.json", "w", encoding="utf-8") as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        with open("data/raw/gsm8k_test.json", "w", encoding="utf-8") as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        # é‡‡æ ·è®­ç»ƒæ•°æ®
        if sample_size < len(train_data):
            sampled_train = random.sample(train_data, sample_size)
        else:
            sampled_train = train_data
        
        processed_data = []
        for i, item in enumerate(track(sampled_train, description="å¤„ç†GSM8K")):
            processed_data.append({
                "id": f"gsm8k_{i:04d}",
                "domain": "math",
                "difficulty": "medium",
                "prompt": f"è¯·è§£å†³è¿™ä¸ªæ•°å­¦é—®é¢˜ï¼Œè¦æ±‚ç»™å‡ºè¯¦ç»†çš„è§£é¢˜æ­¥éª¤ï¼š\n\n{item['question']}",
                "expected_answer": item['answer'],
                "metadata": {
                    "source": "gsm8k",
                    "original_index": i
                }
            })
        
        console.print(f"[green]âœ“[/green] GSM8Kå¤„ç†å®Œæˆ: {len(processed_data)} æ¡æ ·æœ¬")
        return processed_data
        
    except Exception as e:
        console.print(f"[red]âœ—[/red] GSM8Kä¸‹è½½å¤±è´¥: {e}")
        return []

def download_humaneval(sample_size: int = 164) -> List[Dict]:
    """ä¸‹è½½å¹¶å¤„ç†HumanEvalæ•°æ®é›†"""
    console.print(f"[blue]æ­£åœ¨ä¸‹è½½HumanEvalæ•°æ®é›† (æ ·æœ¬æ•°: {sample_size})...[/blue]")
    
    try:
        dataset = load_dataset("openai_humaneval", cache_dir="data/raw/.cache")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        test_data = list(dataset['test'])
        with open("data/raw/humaneval.json", "w", encoding="utf-8") as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        processed_data = []
        for i, item in enumerate(track(test_data[:sample_size], description="å¤„ç†HumanEval")):
            processed_data.append({
                "id": f"humaneval_{item['task_id']}",
                "domain": "code", 
                "difficulty": "hard",
                "prompt": f"è¯·å®Œæˆä»¥ä¸‹Pythonå‡½æ•°ï¼Œè¦æ±‚ä»£ç ç®€æ´é«˜æ•ˆï¼š\n\n```python\n{item['prompt']}\n```\n\nè¯·ç›´æ¥ç»™å‡ºå®Œæ•´çš„å‡½æ•°å®ç°ã€‚",
                "expected_answer": item['canonical_solution'],
                "metadata": {
                    "source": "humaneval",
                    "task_id": item['task_id'],
                    "test_cases": item['test'],
                    "entry_point": item['entry_point']
                }
            })
        
        console.print(f"[green]âœ“[/green] HumanEvalå¤„ç†å®Œæˆ: {len(processed_data)} æ¡æ ·æœ¬")
        return processed_data
        
    except Exception as e:
        console.print(f"[red]âœ—[/red] HumanEvalä¸‹è½½å¤±è´¥: {e}")
        return []

def generate_aiops_synthetic(sample_size: int = 1000) -> List[Dict]:
    """ç”ŸæˆAIOpsåˆæˆæ•°æ®é›†"""
    console.print(f"[blue]æ­£åœ¨ç”ŸæˆAIOpsåˆæˆæ•°æ®é›† (æ ·æœ¬æ•°: {sample_size})...[/blue]")
    
    # æ—¥å¿—æ¨¡æ¿å’Œæ ¹å› æ¨¡æ¿
    templates = [
        {
            "scenario": "æ•°æ®åº“è¿æ¥æ± è€—å°½",
            "logs": [
                "2024-09-09 10:23:11 ERROR [ConnectionPool-1] HikariPool timeout after 30s, total=20, active=20, idle=0",
                "2024-09-09 10:23:12 WARN [PaymentService] Database connection failed: pool exhausted",
                "2024-09-09 10:23:13 ERROR [CircuitBreaker] Circuit opened for mysql-primary:3306"
            ],
            "config": {
                "hikari_pool_size": 20,
                "connection_timeout": 30000,
                "max_lifetime": 300000
            },
            "metrics": {
                "cpu_usage": "45%",
                "memory_usage": "78%", 
                "active_connections": 20,
                "qps": 850
            },
            "root_cause": "æ•°æ®åº“è¿æ¥æ± é…ç½®è¿‡å°ï¼Œåœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹æ— æ³•æ»¡è¶³éœ€æ±‚",
            "fix_plan": [
                "å¢åŠ è¿æ¥æ± å¤§å°åˆ°50-100",
                "ä¼˜åŒ–æ…¢æŸ¥è¯¢å‡å°‘è¿æ¥å ç”¨æ—¶é—´",
                "æ·»åŠ è¿æ¥æ± ç›‘æ§å‘Šè­¦"
            ]
        },
        {
            "scenario": "å†…å­˜æ³„æ¼å¯¼è‡´OOM",
            "logs": [
                "2024-09-09 11:15:23 WARN [GC] Full GC took 2.3s, heap: 7.8GB/8GB used", 
                "2024-09-09 11:15:45 ERROR [JVM] OutOfMemoryError: Java heap space",
                "2024-09-09 11:15:46 ERROR [K8s] Pod order-service-7d9f restarts: 3/5"
            ],
            "config": {
                "jvm_heap": "8GB",
                "gc_algorithm": "G1GC",
                "pod_memory_limit": "10GB"
            },
            "metrics": {
                "heap_usage": "95%",
                "gc_frequency": "æ¯30sä¸€æ¬¡FullGC",
                "restart_count": 3
            },
            "root_cause": "åº”ç”¨å­˜åœ¨å†…å­˜æ³„æ¼ï¼Œå¯¹è±¡æ— æ³•è¢«GCå›æ”¶å¯¼è‡´å †å†…å­˜è€—å°½",
            "fix_plan": [
                "ä½¿ç”¨Memory Profilerå®šä½å†…å­˜æ³„æ¼ç‚¹",
                "ä¸´æ—¶å¢åŠ å †å†…å­˜åˆ°12GB",
                "ä¼˜åŒ–å¯¹è±¡ç”Ÿå‘½å‘¨æœŸç®¡ç†"
            ]
        },
        {
            "scenario": "ç£ç›˜ç©ºé—´ä¸è¶³",
            "logs": [
                "2024-09-09 14:30:12 ERROR [FileSystem] Disk usage: 98% on /var/log",
                "2024-09-09 14:30:15 WARN [LogRotate] Cannot create new log file: No space left on device", 
                "2024-09-09 14:30:18 ERROR [ES] Elasticsearch cluster RED: disk threshold exceeded"
            ],
            "config": {
                "log_retention": "30å¤©",
                "disk_size": "500GB",
                "es_shard_size": "50GB"
            },
            "metrics": {
                "disk_usage": "98%",
                "log_growth_rate": "10GB/day",
                "es_index_size": "450GB"
            },
            "root_cause": "æ—¥å¿—æ–‡ä»¶å’ŒESç´¢å¼•å ç”¨è¿‡å¤šç£ç›˜ç©ºé—´ï¼Œä¸”ç¼ºä¹è‡ªåŠ¨æ¸…ç†æœºåˆ¶",
            "fix_plan": [
                "ç«‹å³æ¸…ç†7å¤©å‰çš„æ—¥å¿—æ–‡ä»¶",
                "é…ç½®æ—¥å¿—è‡ªåŠ¨è½®è½¬å’Œå‹ç¼©",
                "è®¾ç½®ESç´¢å¼•è‡ªåŠ¨åˆ é™¤ç­–ç•¥"
            ]
        },
        {
            "scenario": "ç½‘ç»œå»¶è¿Ÿå¼‚å¸¸",
            "logs": [
                "2024-09-09 16:45:30 WARN [HTTP] Upstream response time: 5.2s (normal: 200ms)",
                "2024-09-09 16:45:33 ERROR [LoadBalancer] Backend server timeout: api-server-3",
                "2024-09-09 16:45:35 INFO [K8s] Node network plugin showing errors"
            ],
            "config": {
                "upstream_timeout": "3s",
                "keepalive_requests": 100,
                "worker_connections": 1024
            },
            "metrics": {
                "avg_response_time": "5.2s", 
                "p99_latency": "8.5s",
                "network_errors": "12/min"
            },
            "root_cause": "ç½‘ç»œæ’ä»¶å¼‚å¸¸å¯¼è‡´è·¨èŠ‚ç‚¹é€šä¿¡å»¶è¿Ÿå¢åŠ ",
            "fix_plan": [
                "é‡å¯å¼‚å¸¸èŠ‚ç‚¹çš„ç½‘ç»œæ’ä»¶",
                "æ£€æŸ¥ç½‘ç»œè®¾å¤‡é…ç½®",
                "ä¸´æ—¶å°†æµé‡åˆ‡æ¢åˆ°æ­£å¸¸èŠ‚ç‚¹"
            ]
        }
    ]
    
    processed_data = []
    for i in track(range(sample_size), description="ç”ŸæˆAIOpsæ•°æ®"):
        template = random.choice(templates)
        
        # æ„å»ºæç¤ºè¯
        prompt_parts = [
            f"## æ•…éšœåœºæ™¯åˆ†æ\n\n**ç³»ç»Ÿæ—¥å¿—:**",
        ]
        
        for log in template["logs"]:
            prompt_parts.append(f"```\n{log}\n```")
        
        prompt_parts.extend([
            f"\n**é…ç½®ä¿¡æ¯:**",
            f"```json\n{json.dumps(template['config'], ensure_ascii=False, indent=2)}\n```",
            f"\n**ç›‘æ§æŒ‡æ ‡:**",  
            f"```json\n{json.dumps(template['metrics'], ensure_ascii=False, indent=2)}\n```",
            f"\nè¯·åˆ†æä¸Šè¿°æ—¥å¿—å’Œé…ç½®ï¼Œæ‰¾å‡ºæœ€å¯èƒ½çš„æ ¹æœ¬åŸå› ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„ä¿®å¤å»ºè®®ã€‚"
        ])
        
        processed_data.append({
            "id": f"aiops_{i:04d}",
            "domain": "aiops",
            "difficulty": "hard", 
            "prompt": "\n".join(prompt_parts),
            "expected_answer": template["root_cause"],
            "metadata": {
                "source": "synthetic",
                "scenario": template["scenario"],
                "fix_plan": template["fix_plan"]
            }
        })
    
    console.print(f"[green]âœ“[/green] AIOpsæ•°æ®ç”Ÿæˆå®Œæˆ: {len(processed_data)} æ¡æ ·æœ¬")
    return processed_data

def save_unified_dataset(data_list: List[List[Dict]], output_path: str):
    """ä¿å­˜ç»Ÿä¸€æ ¼å¼çš„æ•°æ®é›†"""
    all_data = []
    for data in data_list:
        all_data.extend(data)
    
    # æ‰“ä¹±æ•°æ®
    random.shuffle(all_data)
    
    # ä¿å­˜ä¸ºJSONLæ ¼å¼
    with open(output_path, "w", encoding="utf-8") as f:
        for item in all_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    console.print(f"[green]âœ“[/green] ç»Ÿä¸€æ•°æ®é›†å·²ä¿å­˜: {output_path}")
    console.print(f"[cyan]æ€»æ ·æœ¬æ•°: {len(all_data)}[/cyan]")
    
    # ç»Ÿè®¡ä¿¡æ¯
    domain_counts = {}
    for item in all_data:
        domain = item['domain']
        domain_counts[domain] = domain_counts.get(domain, 0) + 1
    
    console.print("[cyan]é¢†åŸŸåˆ†å¸ƒ:[/cyan]")
    for domain, count in domain_counts.items():
        console.print(f"  {domain}: {count} æ¡")

def main():
    """ä¸»å‡½æ•°"""
    console.print("[bold blue]ğŸš€ å¼€å§‹å‡†å¤‡å¤§æ¨¡å‹è’¸é¦æ•°æ®é›†[/bold blue]\n")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # åˆ›å»ºç›®å½•
    setup_directories()
    
    # ä¸‹è½½å’Œå¤„ç†æ•°æ®é›†
    gsm8k_data = download_gsm8k(sample_size=2000)
    humaneval_data = download_humaneval(sample_size=164) 
    aiops_data = generate_aiops_synthetic(sample_size=1000)
    
    # ä¿å­˜ç»Ÿä¸€æ•°æ®é›†
    if gsm8k_data or humaneval_data or aiops_data:
        save_unified_dataset(
            [gsm8k_data, humaneval_data, aiops_data],
            "data/processed/train_dataset.jsonl"
        )
        
        # åˆ›å»ºå°è§„æ¨¡è¯„æµ‹é›† (ä»æ¯ä¸ªé¢†åŸŸæŠ½å–éƒ¨åˆ†ä½œä¸ºè¯„æµ‹)
        eval_data = []
        if gsm8k_data:
            eval_data.extend(random.sample(gsm8k_data, min(200, len(gsm8k_data))))
        if humaneval_data:
            eval_data.extend(random.sample(humaneval_data, min(50, len(humaneval_data))))
        if aiops_data:
            eval_data.extend(random.sample(aiops_data, min(100, len(aiops_data))))
        
        random.shuffle(eval_data)
        with open("data/processed/eval_dataset.jsonl", "w", encoding="utf-8") as f:
            for item in eval_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        console.print(f"[green]âœ“[/green] è¯„æµ‹æ•°æ®é›†å·²ä¿å­˜: data/processed/eval_dataset.jsonl ({len(eval_data)} æ¡)")
    
    console.print("\n[bold green]ğŸ‰ æ•°æ®é›†å‡†å¤‡å®Œæˆï¼[/bold green]")
    console.print("[cyan]æ–‡ä»¶ä½ç½®:[/cyan]")
    console.print("  ğŸ“ è®­ç»ƒé›†: data/processed/train_dataset.jsonl")
    console.print("  ğŸ“ è¯„æµ‹é›†: data/processed/eval_dataset.jsonl") 
    console.print("  ğŸ“ åŸå§‹æ•°æ®: data/raw/")

if __name__ == "__main__":
    main()