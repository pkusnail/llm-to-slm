#!/usr/bin/env python3
"""
å¿«é€Ÿä¸‹è½½å¤šä¸ªäº‘åŸç”Ÿå’Œå¾®æœåŠ¡AIOpsæ•°æ®é›†
ä¸“æ³¨äºAWSã€Kubernetesã€Golangç›¸å…³çš„çœŸå®æ•°æ®

æ•°æ®æ¥æºï¼š
1. LogHub - è¾ƒå°çš„äº‘åŸºç¡€è®¾æ–½æ•°æ®é›† (OpenStack, Hadoop)
2. Kuberneteså®‰å…¨æ•°æ®é›† - ä¸“é—¨çš„K8sæ—¥å¿—
3. å¾®æœåŠ¡ä¾èµ–å›¾æ•°æ®é›† - å¾®æœåŠ¡æ¶æ„ç›¸å…³
4. å¹¶è¡Œä¸‹è½½å¤šä¸ªæ¥æºä»¥å‡å°‘æ—¶é—´
"""

import os
import json
import random
import requests
import zipfile
import tarfile
import asyncio
import aiohttp
import concurrent.futures
from typing import Dict, List, Any, Tuple
from pathlib import Path
import logging
import pandas as pd

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CloudAIOpsDatasetDownloader:
    """äº‘åŸç”ŸAIOpsæ•°æ®é›†å¿«é€Ÿä¸‹è½½å™¨"""
    
    def __init__(self):
        # ç²¾é€‰çš„äº‘åŸç”Ÿå’Œå¾®æœåŠ¡æ•°æ®é›† - å°å°ºå¯¸ï¼Œå¿«é€Ÿä¸‹è½½
        self.datasets = {
            # LogHub è¾ƒå°æ•°æ®é›†
            "OpenStack": {
                "description": "OpenStackäº‘åŸºç¡€è®¾æ–½æ—¥å¿— - çœŸå®äº‘ç¯å¢ƒ",
                "url": "https://zenodo.org/records/8196385/files/OpenStack.tar.gz?download=1",
                "size": "58.6MB",
                "log_lines": "207,820",
                "type": "cloud_infrastructure",
                "format": "tar.gz",
                "priority": 1
            },
            "Hadoop": {
                "description": "Hadoopå¤§æ•°æ®é›†ç¾¤æ—¥å¿— - åˆ†å¸ƒå¼è®¡ç®—",
                "url": "https://zenodo.org/records/8196385/files/Hadoop.zip?download=1",
                "size": "16.3MB", 
                "log_lines": "394,308",
                "type": "big_data_cluster",
                "format": "zip",
                "priority": 1
            },
            
            # æ›´å¿«çš„æ›¿ä»£æ•°æ®æº
            "KubernetesSecurityData": {
                "description": "K8så®‰å…¨æ£€æµ‹æ•°æ®é›† - ç½‘ç»œæµé‡å’Œå¼‚å¸¸",
                "url": "https://github.com/yigitsever/kubernetes-dataset/archive/main.zip",
                "size": "~10MB",
                "log_lines": "~50,000",
                "type": "k8s_security",
                "format": "zip",
                "priority": 2
            },
            "MicroserviceDepGraph": {
                "description": "å¾®æœåŠ¡ä¾èµ–å›¾æ•°æ®é›† - 20ä¸ªå¾®æœåŠ¡é¡¹ç›®",
                "url": "https://github.com/clowee/MicroserviceDataset/archive/main.zip", 
                "size": "~5MB",
                "log_lines": "~10,000",
                "type": "microservices",
                "format": "zip",
                "priority": 2
            }
        }
        
        # äº‘åŸç”ŸAIOpsé—®ç­”æ¨¡æ¿
        self.cloud_qa_templates = [
            {
                "category": "k8s_troubleshooting",
                "template": "Kubernetesé›†ç¾¤ä¸­PodçŠ¶æ€å¼‚å¸¸ï¼Œæ—¥å¿—æ˜¾ç¤ºï¼š\n\n{log_content}\n\nè¯·è¯Šæ–­é—®é¢˜å¹¶æä¾›K8såŸç”Ÿçš„è§£å†³æ–¹æ¡ˆã€‚",
                "response_template": "**K8sè¯Šæ–­**:\n{k8s_diagnosis}\n\n**è§£å†³æ–¹æ¡ˆ**:\n```bash\n{kubectl_commands}\n```\n\n**é¢„é˜²æªæ–½**: {prevention}"
            },
            {
                "category": "cloud_performance",
                "template": "äº‘åŸºç¡€è®¾æ–½æ€§èƒ½ç›‘æ§å‘ç°å¼‚å¸¸ï¼š\n\n{performance_data}\n\nè¯·åŸºäºäº‘åŸç”Ÿæ¶æ„åˆ†ææ€§èƒ½ç“¶é¢ˆå¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®ã€‚",
                "response_template": "**æ€§èƒ½åˆ†æ**: {performance_analysis}\n\n**äº‘åŸç”Ÿä¼˜åŒ–**:\n{cloud_optimization}\n\n**é¢„æœŸæ•ˆæœ**: {expected_results}"
            },
            {
                "category": "microservice_issues", 
                "template": "å¾®æœåŠ¡æ¶æ„ä¸­æœåŠ¡é—´é€šä¿¡å¼‚å¸¸ï¼Œç›¸å…³æ—¥å¿—ï¼š\n\n{service_logs}\n\nè¯·åˆ†ææœåŠ¡ä¾èµ–å…³ç³»å¹¶æä¾›å¾®æœåŠ¡æ²»ç†å»ºè®®ã€‚",
                "response_template": "**æœåŠ¡åˆ†æ**: {service_analysis}\n\n**ä¾èµ–å…³ç³»é—®é¢˜**: {dependency_issues}\n\n**å¾®æœåŠ¡æ²»ç†å»ºè®®**:\n{governance_recommendations}"
            }
        ]
    
    async def download_dataset_async(self, session: aiohttp.ClientSession, dataset_name: str, output_dir: str) -> bool:
        """å¼‚æ­¥ä¸‹è½½æ•°æ®é›†"""
        dataset_info = self.datasets[dataset_name]
        output_path = Path(output_dir) / dataset_name
        output_path.mkdir(parents=True, exist_ok=True)
        
        file_format = dataset_info.get("format", "zip")
        if file_format == "tar.gz":
            archive_file = output_path / f"{dataset_name}.tar.gz"
        else:
            archive_file = output_path / f"{dataset_name}.zip"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if archive_file.exists():
            logger.info(f"âœ… {dataset_name} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            return True
            
        url = dataset_info["url"]
        logger.info(f"ğŸ”„ å¼€å§‹ä¸‹è½½ {dataset_name} ({dataset_info['size']})")
        
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    with open(archive_file, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            f.write(chunk)
                    
                    logger.info(f"âœ… {dataset_name} ä¸‹è½½å®Œæˆ")
                    return True
                else:
                    logger.error(f"âŒ {dataset_name} ä¸‹è½½å¤±è´¥: HTTP {response.status}")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ {dataset_name} ä¸‹è½½å¼‚å¸¸: {e}")
            return False
    
    async def download_all_async(self, output_dir: str = "data/cloud_aiops") -> List[str]:
        """å¹¶è¡Œä¸‹è½½æ‰€æœ‰æ•°æ®é›†"""
        connector = aiohttp.TCPConnector(limit=4)  # é™åˆ¶å¹¶å‘è¿æ¥æ•°
        timeout = aiohttp.ClientTimeout(total=600)  # 10åˆ†é’Ÿè¶…æ—¶
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = []
            
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            sorted_datasets = sorted(self.datasets.items(), key=lambda x: x[1].get("priority", 3))
            
            for dataset_name, _ in sorted_datasets:
                task = self.download_dataset_async(session, dataset_name, output_dir)
                tasks.append((dataset_name, task))
            
            # å¹¶è¡Œæ‰§è¡Œä¸‹è½½
            successful_downloads = []
            results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
            
            for (dataset_name, _), result in zip(tasks, results):
                if isinstance(result, bool) and result:
                    successful_downloads.append(dataset_name)
                elif isinstance(result, Exception):
                    logger.error(f"âŒ {dataset_name} ä¸‹è½½å¼‚å¸¸: {result}")
            
            return successful_downloads
    
    def extract_logs_fast(self, dataset_name: str, output_dir: str = "data/cloud_aiops", sample_limit: int = 1000) -> List[Dict[str, Any]]:
        """å¿«é€Ÿè§£å‹å’Œå¤„ç†æ—¥å¿—"""
        dataset_path = Path(output_dir) / dataset_name
        dataset_info = self.datasets[dataset_name]
        file_format = dataset_info.get("format", "zip")
        
        if file_format == "tar.gz":
            archive_file = dataset_path / f"{dataset_name}.tar.gz"
        else:
            archive_file = dataset_path / f"{dataset_name}.zip"
        
        if not archive_file.exists():
            logger.warning(f"âš ï¸  æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {archive_file}")
            return []
        
        samples = []
        
        try:
            # è§£å‹ç¼©
            if file_format == "tar.gz":
                with tarfile.open(archive_file, 'r:gz') as tar_ref:
                    tar_ref.extractall(dataset_path)
            else:
                with zipfile.ZipFile(archive_file, 'r') as zip_ref:
                    zip_ref.extractall(dataset_path)
            
            # æŸ¥æ‰¾æ–‡ä»¶
            log_files = list(dataset_path.rglob("*.log")) + list(dataset_path.rglob("*.txt")) + list(dataset_path.rglob("*.csv"))
            json_files = list(dataset_path.rglob("*.json")) + list(dataset_path.rglob("*.jsonl"))
            
            if not (log_files or json_files):
                logger.warning(f"âš ï¸  åœ¨ {dataset_path} ä¸­æœªæ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
                return []
            
            # å¤„ç†æ—¥å¿—æ–‡ä»¶
            processed_count = 0
            for log_file in log_files[:3]:  # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡
                if processed_count >= sample_limit:
                    break
                    
                try:
                    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                        for i, line in enumerate(f):
                            if processed_count >= sample_limit:
                                break
                                
                            line = line.strip()
                            if line:
                                qa_pair = self.generate_cloud_qa(dataset_name, line, dataset_info)
                                samples.append({
                                    "id": f"{dataset_name.lower()}_cloud_{processed_count+1}",
                                    "domain": f"cloud_aiops_{dataset_name.lower()}",
                                    "prompt": qa_pair["prompt"],
                                    "expected_answer": qa_pair["answer"],
                                    "metadata": {
                                        "dataset": dataset_name,
                                        "source_file": log_file.name,
                                        "type": dataset_info["type"],
                                        "cloud_native": True
                                    }
                                })
                                processed_count += 1
                except Exception as e:
                    logger.warning(f"âš ï¸  å¤„ç†æ–‡ä»¶ {log_file} å¤±è´¥: {e}")
            
            # å¤„ç†JSONæ–‡ä»¶
            for json_file in json_files[:2]:
                if processed_count >= sample_limit:
                    break
                    
                try:
                    with open(json_file, 'r', encoding='utf-8', errors='ignore') as f:
                        if json_file.suffix == '.jsonl':
                            for line in f:
                                if processed_count >= sample_limit:
                                    break
                                try:
                                    data = json.loads(line)
                                    text_content = str(data)[:500]  # é™åˆ¶é•¿åº¦
                                    qa_pair = self.generate_cloud_qa(dataset_name, text_content, dataset_info)
                                    samples.append({
                                        "id": f"{dataset_name.lower()}_json_{processed_count+1}",
                                        "domain": f"cloud_aiops_{dataset_name.lower()}",
                                        "prompt": qa_pair["prompt"],
                                        "expected_answer": qa_pair["answer"],
                                        "metadata": {
                                            "dataset": dataset_name,
                                            "source_file": json_file.name,
                                            "type": dataset_info["type"],
                                            "cloud_native": True
                                        }
                                    })
                                    processed_count += 1
                                except:
                                    continue
                        else:
                            data = json.load(f)
                            if isinstance(data, list):
                                for item in data[:100]:  # é™åˆ¶å¤„ç†æ•°é‡
                                    if processed_count >= sample_limit:
                                        break
                                    text_content = str(item)[:500]
                                    qa_pair = self.generate_cloud_qa(dataset_name, text_content, dataset_info)
                                    samples.append({
                                        "id": f"{dataset_name.lower()}_json_{processed_count+1}",
                                        "domain": f"cloud_aiops_{dataset_name.lower()}",
                                        "prompt": qa_pair["prompt"],
                                        "expected_answer": qa_pair["answer"],
                                        "metadata": {
                                            "dataset": dataset_name,
                                            "source_file": json_file.name,
                                            "type": dataset_info["type"],
                                            "cloud_native": True
                                        }
                                    })
                                    processed_count += 1
                except Exception as e:
                    logger.warning(f"âš ï¸  å¤„ç†JSONæ–‡ä»¶ {json_file} å¤±è´¥: {e}")
            
            logger.info(f"âœ… {dataset_name} å¤„ç†å®Œæˆ: {len(samples)} æ ·æœ¬")
            return samples
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ•°æ®é›† {dataset_name} å¤±è´¥: {e}")
            return []
    
    def generate_cloud_qa(self, dataset_name: str, content: str, dataset_info: Dict) -> Dict[str, str]:
        """ç”Ÿæˆäº‘åŸç”Ÿåœºæ™¯çš„é—®ç­”å¯¹"""
        
        dataset_type = dataset_info["type"]
        
        # é€‰æ‹©åˆé€‚çš„æ¨¡æ¿
        if dataset_type == "k8s_security":
            template = random.choice([t for t in self.cloud_qa_templates if t["category"] == "k8s_troubleshooting"])
            prompt = template["template"].format(log_content=f"```\n{content[:400]}\n```")
            answer = self._generate_k8s_answer(content, dataset_name)
        elif dataset_type == "microservices":
            template = random.choice([t for t in self.cloud_qa_templates if t["category"] == "microservice_issues"])
            prompt = template["template"].format(service_logs=f"```\n{content[:400]}\n```")
            answer = self._generate_microservice_answer(content, dataset_name)
        else:  # cloud_infrastructure, big_data_cluster
            template = random.choice([t for t in self.cloud_qa_templates if t["category"] == "cloud_performance"])
            prompt = template["template"].format(performance_data=f"```\n{content[:400]}\n```")
            answer = self._generate_cloud_performance_answer(content, dataset_name)
        
        return {"prompt": prompt, "answer": answer}
    
    def _generate_k8s_answer(self, content: str, dataset_name: str) -> str:
        """ç”ŸæˆK8sç›¸å…³é—®é¢˜çš„ç­”æ¡ˆ"""
        return f"""**K8sè¯Šæ–­**:
åŸºäº{dataset_name}æ•°æ®åˆ†æï¼Œæ£€æµ‹åˆ°Kubernetesé›†ç¾¤å¼‚å¸¸è¡Œä¸ºã€‚

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥PodçŠ¶æ€
kubectl get pods -o wide
kubectl describe pod <pod-name>

# æŸ¥çœ‹é›†ç¾¤èµ„æº
kubectl top nodes
kubectl top pods

# æ£€æŸ¥äº‹ä»¶
kubectl get events --sort-by='.lastTimestamp'

# å¦‚æœæ˜¯èµ„æºé—®é¢˜ï¼Œè°ƒæ•´èµ„æºé™åˆ¶
kubectl edit deployment <deployment-name>
```

**é¢„é˜²æªæ–½**: 
- è®¾ç½®åˆé€‚çš„èµ„æºè¯·æ±‚å’Œé™åˆ¶
- é…ç½®å¥åº·æ£€æŸ¥æ¢é’ˆ
- å®æ–½Pod Disruption Budget
- å»ºç«‹ç›‘æ§å‘Šè­¦æœºåˆ¶"""
    
    def _generate_microservice_answer(self, content: str, dataset_name: str) -> str:
        """ç”Ÿæˆå¾®æœåŠ¡ç›¸å…³é—®é¢˜çš„ç­”æ¡ˆ"""
        return f"""**æœåŠ¡åˆ†æ**: 
åŸºäº{dataset_name}å¾®æœåŠ¡æ¶æ„åˆ†æï¼Œå‘ç°æœåŠ¡é—´é€šä¿¡å­˜åœ¨å¼‚å¸¸ã€‚

**ä¾èµ–å…³ç³»é—®é¢˜**: 
- æœåŠ¡è°ƒç”¨é“¾è·¯å¼‚å¸¸
- å¯èƒ½å­˜åœ¨å¾ªç¯ä¾èµ–
- æœåŠ¡å‘ç°é…ç½®é—®é¢˜

**å¾®æœåŠ¡æ²»ç†å»ºè®®**:
1. **æœåŠ¡ç½‘æ ¼**: å®æ–½Istioæˆ–Linkerdè¿›è¡Œæµé‡ç®¡ç†
2. **ç†”æ–­å™¨**: ä½¿ç”¨Circuit Breakeræ¨¡å¼é˜²æ­¢çº§è”å¤±è´¥
3. **ç›‘æ§**: éƒ¨ç½²åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ(å¦‚Jaeger)
4. **é™æµ**: å®æ–½APIç½‘å…³è¿›è¡Œæµé‡æ§åˆ¶
5. **å¥åº·æ£€æŸ¥**: é…ç½®æœåŠ¡å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    
    def _generate_cloud_performance_answer(self, content: str, dataset_name: str) -> str:
        """ç”Ÿæˆäº‘æ€§èƒ½ç›¸å…³é—®é¢˜çš„ç­”æ¡ˆ"""
        return f"""**æ€§èƒ½åˆ†æ**: 
åŸºäº{dataset_name}äº‘åŸºç¡€è®¾æ–½æ—¥å¿—ï¼Œè¯†åˆ«å‡ºæ€§èƒ½ç“¶é¢ˆã€‚

**äº‘åŸç”Ÿä¼˜åŒ–**:
1. **è‡ªåŠ¨æ‰©ç¼©å®¹**: é…ç½®HPAå’ŒVPAå®ç°åŠ¨æ€èµ„æºè°ƒæ•´
2. **ç¼“å­˜ç­–ç•¥**: å®æ–½Redisé›†ç¾¤æå‡æ•°æ®è®¿é—®é€Ÿåº¦
3. **è´Ÿè½½å‡è¡¡**: ä¼˜åŒ–Ingressæ§åˆ¶å™¨é…ç½®
4. **èµ„æºè°ƒåº¦**: ä½¿ç”¨Node Affinityä¼˜åŒ–Podè°ƒåº¦
5. **å­˜å‚¨ä¼˜åŒ–**: é€‰æ‹©åˆé€‚çš„StorageClasså’ŒæŒä¹…å·

**é¢„æœŸæ•ˆæœ**:
- å“åº”å»¶è¿Ÿé™ä½30-50%
- ç³»ç»Ÿååé‡æå‡2-3å€  
- èµ„æºåˆ©ç”¨ç‡ä¼˜åŒ–20-40%
- æˆæœ¬èŠ‚çœ15-25%"""
    
    def process_all_datasets(self, output_dir: str = "data/cloud_aiops", target_samples: int = 10000) -> Dict[str, str]:
        """å¤„ç†æ‰€æœ‰æ•°æ®é›†çš„ä¸»å‡½æ•°"""
        
        logger.info("ğŸš€ å¼€å§‹äº‘åŸç”ŸAIOpsæ•°æ®é›†å¿«é€Ÿä¸‹è½½...")
        logger.info("ç‰¹ç‚¹: AWSã€K8sã€å¾®æœåŠ¡ç›¸å…³çœŸå®æ•°æ®ï¼Œå¿«é€Ÿå¹¶è¡Œä¸‹è½½")
        
        # å¼‚æ­¥ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
        successful_downloads = asyncio.run(self.download_all_async(output_dir))
        
        if not successful_downloads:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•æ•°æ®é›†")
            return {}
        
        logger.info(f"âœ… æˆåŠŸä¸‹è½½: {', '.join(successful_downloads)}")
        
        # å¹¶è¡Œå¤„ç†æ•°æ®é›†
        all_samples = []
        samples_per_dataset = target_samples // len(successful_downloads)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            
            for dataset_name in successful_downloads:
                future = executor.submit(
                    self.extract_logs_fast, 
                    dataset_name, 
                    output_dir, 
                    samples_per_dataset
                )
                futures.append((dataset_name, future))
            
            for dataset_name, future in futures:
                try:
                    samples = future.result(timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶
                    all_samples.extend(samples)
                    logger.info(f"âœ… {dataset_name} å¤„ç†å®Œæˆ: {len(samples)} æ ·æœ¬")
                except Exception as e:
                    logger.error(f"âŒ {dataset_name} å¤„ç†å¤±è´¥: {e}")
        
        # éšæœºé‡‡æ ·åˆ°ç›®æ ‡æ•°é‡
        if len(all_samples) > target_samples:
            all_samples = random.sample(all_samples, target_samples)
        
        random.shuffle(all_samples)
        
        # åˆ†å‰²æ•°æ®é›†
        train_size = int(len(all_samples) * 0.8)
        train_samples = all_samples[:train_size]
        eval_samples = all_samples[train_size:]
        
        # ä¿å­˜æ•°æ®
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        train_file = output_path / "cloud_aiops_train_data.jsonl"
        eval_file = output_path / "cloud_aiops_eval_data.jsonl"
        
        with open(train_file, 'w', encoding='utf-8') as f:
            for sample in train_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        with open(eval_file, 'w', encoding='utf-8') as f:
            for sample in eval_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        logger.info(f"ğŸ‰ äº‘åŸç”ŸAIOpsæ•°æ®é›†å¤„ç†å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(all_samples)}")
        logger.info(f"ğŸ“ˆ è®­ç»ƒé›†: {len(train_samples)} æ ·æœ¬")
        logger.info(f"ğŸ“‰ éªŒè¯é›†: {len(eval_samples)} æ ·æœ¬")
        logger.info(f"ğŸ’¾ æ•°æ®æº: {', '.join(successful_downloads)}")
        
        return {
            "train_file": str(train_file),
            "eval_file": str(eval_file),
            "total_samples": len(all_samples),
            "train_samples": len(train_samples),
            "eval_samples": len(eval_samples),
            "datasets_used": successful_downloads
        }

def main():
    """ä¸»å‡½æ•°"""
    downloader = CloudAIOpsDatasetDownloader()
    
    random.seed(42)
    
    result = downloader.process_all_datasets(
        output_dir="data/cloud_aiops",
        target_samples=10000
    )
    
    logger.info("\nğŸ“‹ å¤„ç†ç»“æœ:")
    for key, value in result.items():
        logger.info(f"   {key}: {value}")

if __name__ == "__main__":
    main()