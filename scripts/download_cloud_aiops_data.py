#!/usr/bin/env python3
"""
Âø´ÈÄü‰∏ãËΩΩÂ§ö‰∏™‰∫ëÂéüÁîüÂíåÂæÆÊúçÂä°AIOpsÊï∞ÊçÆÈõÜ
‰∏ìÊ≥®‰∫éAWS„ÄÅKubernetes„ÄÅGolangÁõ∏ÂÖ≥ÁöÑÁúüÂÆûÊï∞ÊçÆ

Êï∞ÊçÆÊù•Ê∫êÔºö
1. LogHub - ËæÉÂ∞èÁöÑ‰∫ëÂü∫Á°ÄËÆæÊñΩÊï∞ÊçÆÈõÜ (OpenStack, Hadoop)
2. KubernetesÂÆâÂÖ®Êï∞ÊçÆÈõÜ - ‰∏ìÈó®ÁöÑK8sÊó•Âøó
3. ÂæÆÊúçÂä°‰æùËµñÂõæÊï∞ÊçÆÈõÜ - ÂæÆÊúçÂä°Êû∂ÊûÑÁõ∏ÂÖ≥
4. Âπ∂Ë°å‰∏ãËΩΩÂ§ö‰∏™Êù•Ê∫ê‰ª•ÂáèÂ∞ëÊó∂Èó¥
"""

import os
import json
import random
import requests
import zipfile
import tarfile
import asyncio
import aiohttp
import concurrent.futures
from typing import Dict, List, Any, Tuple
from pathlib import Path
import logging
import pandas as pd

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CloudAIOpsDatasetDownloader:
    """‰∫ëÂéüÁîüAIOpsÊï∞ÊçÆÈõÜÂø´ÈÄü‰∏ãËΩΩÂô®"""
    
    def __init__(self):
        # Á≤æÈÄâÁöÑ‰∫ëÂéüÁîüÂíåÂæÆÊúçÂä°Êï∞ÊçÆÈõÜ - Â∞èÂ∞∫ÂØ∏ÔºåÂø´ÈÄü‰∏ãËΩΩ
        self.datasets = {
            # LogHub ËæÉÂ∞èÊï∞ÊçÆÈõÜ
            "OpenStack": {
                "description": "OpenStack‰∫ëÂü∫Á°ÄËÆæÊñΩÊó•Âøó - ÁúüÂÆû‰∫ëÁéØÂ¢É",
                "url": "https://zenodo.org/records/8196385/files/OpenStack.tar.gz?download=1",
                "size": "58.6MB",
                "log_lines": "207,820",
                "type": "cloud_infrastructure",
                "format": "tar.gz",
                "priority": 1
            },
            "Hadoop": {
                "description": "HadoopÂ§ßÊï∞ÊçÆÈõÜÁæ§Êó•Âøó - ÂàÜÂ∏ÉÂºèËÆ°ÁÆó",
                "url": "https://zenodo.org/records/8196385/files/Hadoop.zip?download=1",
                "size": "16.3MB", 
                "log_lines": "394,308",
                "type": "big_data_cluster",
                "format": "zip",
                "priority": 1
            },
            
            # Êõ¥Âø´ÁöÑÊõø‰ª£Êï∞ÊçÆÊ∫ê
            "KubernetesSecurityData": {
                "description": "K8sÂÆâÂÖ®Ê£ÄÊµãÊï∞ÊçÆÈõÜ - ÁΩëÁªúÊµÅÈáèÂíåÂºÇÂ∏∏",
                "url": "https://github.com/yigitsever/kubernetes-dataset/archive/main.zip",
                "size": "~10MB",
                "log_lines": "~50,000",
                "type": "k8s_security",
                "format": "zip",
                "priority": 2
            },
            "MicroserviceDepGraph": {
                "description": "ÂæÆÊúçÂä°‰æùËµñÂõæÊï∞ÊçÆÈõÜ - 20‰∏™ÂæÆÊúçÂä°È°πÁõÆ",
                "url": "https://github.com/clowee/MicroserviceDataset/archive/main.zip", 
                "size": "~5MB",
                "log_lines": "~10,000",
                "type": "microservices",
                "format": "zip",
                "priority": 2
            }
        }
        
        # ‰∫ëÂéüÁîüAIOpsÈóÆÁ≠îÊ®°Êùø
        self.cloud_qa_templates = [
            {
                "category": "k8s_troubleshooting",
                "template": "KubernetesÈõÜÁæ§‰∏≠PodÁä∂ÊÄÅÂºÇÂ∏∏ÔºåÊó•ÂøóÊòæÁ§∫Ôºö\n\n{log_content}\n\nËØ∑ËØäÊñ≠ÈóÆÈ¢òÂπ∂Êèê‰æõK8sÂéüÁîüÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ",
                "response_template": "**K8sËØäÊñ≠**:\n{k8s_diagnosis}\n\n**Ëß£ÂÜ≥ÊñπÊ°à**:\n```bash\n{kubectl_commands}\n```\n\n**È¢ÑÈò≤Êé™ÊñΩ**: {prevention}"
            },
            {
                "category": "cloud_performance",
                "template": "‰∫ëÂü∫Á°ÄËÆæÊñΩÊÄßËÉΩÁõëÊéßÂèëÁé∞ÂºÇÂ∏∏Ôºö\n\n{performance_data}\n\nËØ∑Âü∫‰∫é‰∫ëÂéüÁîüÊû∂ÊûÑÂàÜÊûêÊÄßËÉΩÁì∂È¢àÂπ∂ÁªôÂá∫‰ºòÂåñÂª∫ËÆÆ„ÄÇ",
                "response_template": "**ÊÄßËÉΩÂàÜÊûê**: {performance_analysis}\n\n**‰∫ëÂéüÁîü‰ºòÂåñ**:\n{cloud_optimization}\n\n**È¢ÑÊúüÊïàÊûú**: {expected_results}"
            },
            {
                "category": "microservice_issues", 
                "template": "ÂæÆÊúçÂä°Êû∂ÊûÑ‰∏≠ÊúçÂä°Èó¥ÈÄö‰ø°ÂºÇÂ∏∏ÔºåÁõ∏ÂÖ≥Êó•ÂøóÔºö\n\n{service_logs}\n\nËØ∑ÂàÜÊûêÊúçÂä°‰æùËµñÂÖ≥Á≥ªÂπ∂Êèê‰æõÂæÆÊúçÂä°Ê≤ªÁêÜÂª∫ËÆÆ„ÄÇ",
                "response_template": "**ÊúçÂä°ÂàÜÊûê**: {service_analysis}\n\n**‰æùËµñÂÖ≥Á≥ªÈóÆÈ¢ò**: {dependency_issues}\n\n**ÂæÆÊúçÂä°Ê≤ªÁêÜÂª∫ËÆÆ**:\n{governance_recommendations}"
            }
        ]
    
    async def download_dataset_async(self, session: aiohttp.ClientSession, dataset_name: str, output_dir: str) -> bool:
        """ÂºÇÊ≠•‰∏ãËΩΩÊï∞ÊçÆÈõÜ"""
        dataset_info = self.datasets[dataset_name]
        output_path = Path(output_dir) / dataset_name
        output_path.mkdir(parents=True, exist_ok=True)
        
        file_format = dataset_info.get("format", "zip")
        if file_format == "tar.gz":
            archive_file = output_path / f"{dataset_name}.tar.gz"
        else:
            archive_file = output_path / f"{dataset_name}.zip"
        
        # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®
        if archive_file.exists():
            logger.info(f"‚úÖ {dataset_name} Â∑≤Â≠òÂú®ÔºåË∑≥Ëøá‰∏ãËΩΩ")
            return True
            
        url = dataset_info["url"]
        logger.info(f"üîÑ ÂºÄÂßã‰∏ãËΩΩ {dataset_name} ({dataset_info['size']})")
        
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    with open(archive_file, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            f.write(chunk)
                    
                    logger.info(f"‚úÖ {dataset_name} ‰∏ãËΩΩÂÆåÊàê")
                    return True
                else:
                    logger.error(f"‚ùå {dataset_name} ‰∏ãËΩΩÂ§±Ë¥•: HTTP {response.status}")
                    return False
                    
        except Exception as e:
            logger.error(f"‚ùå {dataset_name} ‰∏ãËΩΩÂºÇÂ∏∏: {e}")
            return False
    
    async def download_all_async(self, output_dir: str = "data/cloud_aiops") -> List[str]:
        """Âπ∂Ë°å‰∏ãËΩΩÊâÄÊúâÊï∞ÊçÆÈõÜ"""
        connector = aiohttp.TCPConnector(limit=4)  # ÈôêÂà∂Âπ∂ÂèëËøûÊé•Êï∞
        timeout = aiohttp.ClientTimeout(total=600)  # 10ÂàÜÈíüË∂ÖÊó∂
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = []
            
            # Êåâ‰ºòÂÖàÁ∫ßÊéíÂ∫è
            sorted_datasets = sorted(self.datasets.items(), key=lambda x: x[1].get("priority", 3))
            
            for dataset_name, _ in sorted_datasets:
                task = self.download_dataset_async(session, dataset_name, output_dir)
                tasks.append((dataset_name, task))
            
            # Âπ∂Ë°åÊâßË°å‰∏ãËΩΩ
            successful_downloads = []
            results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
            
            for (dataset_name, _), result in zip(tasks, results):
                if isinstance(result, bool) and result:
                    successful_downloads.append(dataset_name)
                elif isinstance(result, Exception):
                    logger.error(f"‚ùå {dataset_name} ‰∏ãËΩΩÂºÇÂ∏∏: {result}")
            
            return successful_downloads
    
    def extract_logs_fast(self, dataset_name: str, output_dir: str = "data/cloud_aiops", sample_limit: int = 1000) -> List[Dict[str, Any]]:
        """Âø´ÈÄüËß£ÂéãÂíåÂ§ÑÁêÜÊó•Âøó"""
        dataset_path = Path(output_dir) / dataset_name
        dataset_info = self.datasets[dataset_name]
        file_format = dataset_info.get("format", "zip")
        
        if file_format == "tar.gz":
            archive_file = dataset_path / f"{dataset_name}.tar.gz"
        else:
            archive_file = dataset_path / f"{dataset_name}.zip"
        
        if not archive_file.exists():
            logger.warning(f"‚ö†Ô∏è  Êï∞ÊçÆÈõÜÊñá‰ª∂‰∏çÂ≠òÂú®: {archive_file}")
            return []
        
        samples = []
        
        try:
            # Ëß£ÂéãÁº©
            if file_format == "tar.gz":
                with tarfile.open(archive_file, 'r:gz') as tar_ref:
                    tar_ref.extractall(dataset_path)
            else:
                with zipfile.ZipFile(archive_file, 'r') as zip_ref:
                    zip_ref.extractall(dataset_path)
            
            # Êü•ÊâæÊñá‰ª∂
            log_files = list(dataset_path.rglob("*.log")) + list(dataset_path.rglob("*.txt")) + list(dataset_path.rglob("*.csv"))
            json_files = list(dataset_path.rglob("*.json")) + list(dataset_path.rglob("*.jsonl"))
            
            if not (log_files or json_files):
                logger.warning(f"‚ö†Ô∏è  Âú® {dataset_path} ‰∏≠Êú™ÊâæÂà∞ÂèØÂ§ÑÁêÜÁöÑÊñá‰ª∂")
                return []
            
            # Â§ÑÁêÜÊó•ÂøóÊñá‰ª∂
            processed_count = 0
            for log_file in log_files[:3]:  # ÈôêÂà∂Â§ÑÁêÜÊñá‰ª∂Êï∞Èáè
                if processed_count >= sample_limit:
                    break
                    
                try:
                    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                        for i, line in enumerate(f):
                            if processed_count >= sample_limit:
                                break
                                
                            line = line.strip()
                            if line:
                                qa_pair = self.generate_cloud_qa(dataset_name, line, dataset_info)
                                samples.append({
                                    "id": f"{dataset_name.lower()}_cloud_{processed_count+1}",
                                    "domain": f"cloud_aiops_{dataset_name.lower()}",
                                    "prompt": qa_pair["prompt"],
                                    "expected_answer": qa_pair["answer"],
                                    "metadata": {
                                        "dataset": dataset_name,
                                        "source_file": log_file.name,
                                        "type": dataset_info["type"],
                                        "cloud_native": True
                                    }
                                })
                                processed_count += 1
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Â§ÑÁêÜÊñá‰ª∂ {log_file} Â§±Ë¥•: {e}")
            
            # Â§ÑÁêÜJSONÊñá‰ª∂
            for json_file in json_files[:2]:
                if processed_count >= sample_limit:
                    break
                    
                try:
                    with open(json_file, 'r', encoding='utf-8', errors='ignore') as f:
                        if json_file.suffix == '.jsonl':
                            for line in f:
                                if processed_count >= sample_limit:
                                    break
                                try:
                                    data = json.loads(line)
                                    text_content = str(data)[:500]  # ÈôêÂà∂ÈïøÂ∫¶
                                    qa_pair = self.generate_cloud_qa(dataset_name, text_content, dataset_info)
                                    samples.append({
                                        "id": f"{dataset_name.lower()}_json_{processed_count+1}",
                                        "domain": f"cloud_aiops_{dataset_name.lower()}",
                                        "prompt": qa_pair["prompt"],
                                        "expected_answer": qa_pair["answer"],
                                        "metadata": {
                                            "dataset": dataset_name,
                                            "source_file": json_file.name,
                                            "type": dataset_info["type"],
                                            "cloud_native": True
                                        }
                                    })
                                    processed_count += 1
                                except:
                                    continue
                        else:
                            data = json.load(f)
                            if isinstance(data, list):
                                for item in data[:100]:  # ÈôêÂà∂Â§ÑÁêÜÊï∞Èáè
                                    if processed_count >= sample_limit:
                                        break
                                    text_content = str(item)[:500]
                                    qa_pair = self.generate_cloud_qa(dataset_name, text_content, dataset_info)
                                    samples.append({
                                        "id": f"{dataset_name.lower()}_json_{processed_count+1}",
                                        "domain": f"cloud_aiops_{dataset_name.lower()}",
                                        "prompt": qa_pair["prompt"],
                                        "expected_answer": qa_pair["answer"],
                                        "metadata": {
                                            "dataset": dataset_name,
                                            "source_file": json_file.name,
                                            "type": dataset_info["type"],
                                            "cloud_native": True
                                        }
                                    })
                                    processed_count += 1
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Â§ÑÁêÜJSONÊñá‰ª∂ {json_file} Â§±Ë¥•: {e}")
            
            logger.info(f"‚úÖ {dataset_name} Â§ÑÁêÜÂÆåÊàê: {len(samples)} Ê†∑Êú¨")
            return samples
            
        except Exception as e:
            logger.error(f"‚ùå Â§ÑÁêÜÊï∞ÊçÆÈõÜ {dataset_name} Â§±Ë¥•: {e}")
            return []
    
    def generate_cloud_qa(self, dataset_name: str, content: str, dataset_info: Dict) -> Dict[str, str]:
        """ÁîüÊàê‰∫ëÂéüÁîüÂú∫ÊôØÁöÑÈóÆÁ≠îÂØπ"""
        
        dataset_type = dataset_info["type"]
        
        # ÈÄâÊã©ÂêàÈÄÇÁöÑÊ®°Êùø
        if dataset_type == "k8s_security":
            template = random.choice([t for t in self.cloud_qa_templates if t["category"] == "k8s_troubleshooting"])
            prompt = template["template"].format(log_content=f"```\n{content[:400]}\n```")
            answer = self._generate_k8s_answer(content, dataset_name)
        elif dataset_type == "microservices":
            template = random.choice([t for t in self.cloud_qa_templates if t["category"] == "microservice_issues"])
            prompt = template["template"].format(service_logs=f"```\n{content[:400]}\n```")
            answer = self._generate_microservice_answer(content, dataset_name)
        else:  # cloud_infrastructure, big_data_cluster
            template = random.choice([t for t in self.cloud_qa_templates if t["category"] == "cloud_performance"])
            prompt = template["template"].format(performance_data=f"```\n{content[:400]}\n```")
            answer = self._generate_cloud_performance_answer(content, dataset_name)
        
        return {"prompt": prompt, "answer": answer}
    
    def _generate_k8s_answer(self, content: str, dataset_name: str) -> str:
        """ÁîüÊàêK8sÁõ∏ÂÖ≥ÈóÆÈ¢òÁöÑÁ≠îÊ°à"""
        return f"""**K8sËØäÊñ≠**:
Âü∫‰∫é{dataset_name}Êï∞ÊçÆÂàÜÊûêÔºåÊ£ÄÊµãÂà∞KubernetesÈõÜÁæ§ÂºÇÂ∏∏Ë°å‰∏∫„ÄÇ

**Ëß£ÂÜ≥ÊñπÊ°à**:
```bash
# Ê£ÄÊü•PodÁä∂ÊÄÅ
kubectl get pods -o wide
kubectl describe pod <pod-name>

# Êü•ÁúãÈõÜÁæ§ËµÑÊ∫ê
kubectl top nodes
kubectl top pods

# Ê£ÄÊü•‰∫ã‰ª∂
kubectl get events --sort-by='.lastTimestamp'

# Â¶ÇÊûúÊòØËµÑÊ∫êÈóÆÈ¢òÔºåË∞ÉÊï¥ËµÑÊ∫êÈôêÂà∂
kubectl edit deployment <deployment-name>
```

**È¢ÑÈò≤Êé™ÊñΩ**: 
- ËÆæÁΩÆÂêàÈÄÇÁöÑËµÑÊ∫êËØ∑Ê±ÇÂíåÈôêÂà∂
- ÈÖçÁΩÆÂÅ•Â∫∑Ê£ÄÊü•Êé¢Èíà
- ÂÆûÊñΩPod Disruption Budget
- Âª∫Á´ãÁõëÊéßÂëäË≠¶Êú∫Âà∂"""
    
    def _generate_microservice_answer(self, content: str, dataset_name: str) -> str:
        """ÁîüÊàêÂæÆÊúçÂä°Áõ∏ÂÖ≥ÈóÆÈ¢òÁöÑÁ≠îÊ°à"""
        return f"""**ÊúçÂä°ÂàÜÊûê**: 
Âü∫‰∫é{dataset_name}ÂæÆÊúçÂä°Êû∂ÊûÑÂàÜÊûêÔºåÂèëÁé∞ÊúçÂä°Èó¥ÈÄö‰ø°Â≠òÂú®ÂºÇÂ∏∏„ÄÇ

**‰æùËµñÂÖ≥Á≥ªÈóÆÈ¢ò**: 
- ÊúçÂä°Ë∞ÉÁî®ÈìæË∑ØÂºÇÂ∏∏
- ÂèØËÉΩÂ≠òÂú®Âæ™ÁéØ‰æùËµñ
- ÊúçÂä°ÂèëÁé∞ÈÖçÁΩÆÈóÆÈ¢ò

**ÂæÆÊúçÂä°Ê≤ªÁêÜÂª∫ËÆÆ**:
1. **ÊúçÂä°ÁΩëÊ†º**: ÂÆûÊñΩIstioÊàñLinkerdËøõË°åÊµÅÈáèÁÆ°ÁêÜ
2. **ÁÜîÊñ≠Âô®**: ‰ΩøÁî®Circuit BreakerÊ®°ÂºèÈò≤Ê≠¢Á∫ßËÅîÂ§±Ë¥•
3. **ÁõëÊéß**: ÈÉ®ÁΩ≤ÂàÜÂ∏ÉÂºèËøΩË∏™Á≥ªÁªü(Â¶ÇJaeger)
4. **ÈôêÊµÅ**: ÂÆûÊñΩAPIÁΩëÂÖ≥ËøõË°åÊµÅÈáèÊéßÂà∂
5. **ÂÅ•Â∫∑Ê£ÄÊü•**: ÈÖçÁΩÆÊúçÂä°ÂÅ•Â∫∑Ê£ÄÊü•Á´ØÁÇπ"""
    
    def _generate_cloud_performance_answer(self, content: str, dataset_name: str) -> str:
        """ÁîüÊàê‰∫ëÊÄßËÉΩÁõ∏ÂÖ≥ÈóÆÈ¢òÁöÑÁ≠îÊ°à"""
        return f"""**ÊÄßËÉΩÂàÜÊûê**: 
Âü∫‰∫é{dataset_name}‰∫ëÂü∫Á°ÄËÆæÊñΩÊó•ÂøóÔºåËØÜÂà´Âá∫ÊÄßËÉΩÁì∂È¢à„ÄÇ

**‰∫ëÂéüÁîü‰ºòÂåñ**:
1. **Ëá™Âä®Êâ©Áº©ÂÆπ**: ÈÖçÁΩÆHPAÂíåVPAÂÆûÁé∞Âä®ÊÄÅËµÑÊ∫êË∞ÉÊï¥
2. **ÁºìÂ≠òÁ≠ñÁï•**: ÂÆûÊñΩRedisÈõÜÁæ§ÊèêÂçáÊï∞ÊçÆËÆøÈóÆÈÄüÂ∫¶
3. **Ë¥üËΩΩÂùáË°°**: ‰ºòÂåñIngressÊéßÂà∂Âô®ÈÖçÁΩÆ
4. **ËµÑÊ∫êË∞ÉÂ∫¶**: ‰ΩøÁî®Node Affinity‰ºòÂåñPodË∞ÉÂ∫¶
5. **Â≠òÂÇ®‰ºòÂåñ**: ÈÄâÊã©ÂêàÈÄÇÁöÑStorageClassÂíåÊåÅ‰πÖÂç∑

**È¢ÑÊúüÊïàÊûú**:
- ÂìçÂ∫îÂª∂ËøüÈôç‰Ωé30-50%
- Á≥ªÁªüÂêûÂêêÈáèÊèêÂçá2-3ÂÄç  
- ËµÑÊ∫êÂà©Áî®Áéá‰ºòÂåñ20-40%
- ÊàêÊú¨ËäÇÁúÅ15-25%"""
    
    def process_all_datasets(self, output_dir: str = "data/cloud_aiops", target_samples: int = 10000) -> Dict[str, str]:
        """Â§ÑÁêÜÊâÄÊúâÊï∞ÊçÆÈõÜÁöÑ‰∏ªÂáΩÊï∞"""
        
        logger.info("üöÄ ÂºÄÂßã‰∫ëÂéüÁîüAIOpsÊï∞ÊçÆÈõÜÂø´ÈÄü‰∏ãËΩΩ...")
        logger.info("ÁâπÁÇπ: AWS„ÄÅK8s„ÄÅÂæÆÊúçÂä°Áõ∏ÂÖ≥ÁúüÂÆûÊï∞ÊçÆÔºåÂø´ÈÄüÂπ∂Ë°å‰∏ãËΩΩ")
        
        # ÂºÇÊ≠•‰∏ãËΩΩÊâÄÊúâÊï∞ÊçÆÈõÜ
        successful_downloads = asyncio.run(self.download_all_async(output_dir))
        
        if not successful_downloads:
            logger.error("‚ùå Ê≤°ÊúâÊàêÂäü‰∏ãËΩΩ‰ªª‰ΩïÊï∞ÊçÆÈõÜ")
            return {}
        
        logger.info(f"‚úÖ ÊàêÂäü‰∏ãËΩΩ: {', '.join(successful_downloads)}")
        
        # Âπ∂Ë°åÂ§ÑÁêÜÊï∞ÊçÆÈõÜ
        all_samples = []
        samples_per_dataset = target_samples // len(successful_downloads)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            
            for dataset_name in successful_downloads:
                future = executor.submit(
                    self.extract_logs_fast, 
                    dataset_name, 
                    output_dir, 
                    samples_per_dataset
                )
                futures.append((dataset_name, future))
            
            for dataset_name, future in futures:
                try:
                    samples = future.result(timeout=120)  # 2ÂàÜÈíüË∂ÖÊó∂
                    all_samples.extend(samples)
                    logger.info(f"‚úÖ {dataset_name} Â§ÑÁêÜÂÆåÊàê: {len(samples)} Ê†∑Êú¨")
                except Exception as e:
                    logger.error(f"‚ùå {dataset_name} Â§ÑÁêÜÂ§±Ë¥•: {e}")
        
        # ÈöèÊú∫ÈááÊ†∑Âà∞ÁõÆÊ†áÊï∞Èáè
        if len(all_samples) > target_samples:
            all_samples = random.sample(all_samples, target_samples)
        
        random.shuffle(all_samples)
        
        # ÂàÜÂâ≤Êï∞ÊçÆÈõÜ
        train_size = int(len(all_samples) * 0.8)
        train_samples = all_samples[:train_size]
        eval_samples = all_samples[train_size:]
        
        # ‰øùÂ≠òÊï∞ÊçÆ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        train_file = output_path / "cloud_aiops_train_data.jsonl"
        eval_file = output_path / "cloud_aiops_eval_data.jsonl"
        
        with open(train_file, 'w', encoding='utf-8') as f:
            for sample in train_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        with open(eval_file, 'w', encoding='utf-8') as f:
            for sample in eval_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        logger.info(f"üéâ ‰∫ëÂéüÁîüAIOpsÊï∞ÊçÆÈõÜÂ§ÑÁêÜÂÆåÊàêÔºÅ")
        logger.info(f"üìä ÊÄªÊ†∑Êú¨Êï∞: {len(all_samples)}")
        logger.info(f"üìà ËÆ≠ÁªÉÈõÜ: {len(train_samples)} Ê†∑Êú¨")
        logger.info(f"üìâ È™åËØÅÈõÜ: {len(eval_samples)} Ê†∑Êú¨")
        logger.info(f"üíæ Êï∞ÊçÆÊ∫ê: {', '.join(successful_downloads)}")
        
        return {
            "train_file": str(train_file),
            "eval_file": str(eval_file),
            "total_samples": len(all_samples),
            "train_samples": len(train_samples),
            "eval_samples": len(eval_samples),
            "datasets_used": successful_downloads
        }

def main():
    """‰∏ªÂáΩÊï∞"""
    downloader = CloudAIOpsDatasetDownloader()
    
    random.seed(42)
    
    result = downloader.process_all_datasets(
        output_dir="data/cloud_aiops",
        target_samples=10000
    )
    
    logger.info("\nüìã Â§ÑÁêÜÁªìÊûú:")
    for key, value in result.items():
        logger.info(f"   {key}: {value}")

if __name__ == "__main__":
    main()