#!/usr/bin/env python3
"""
çŸ¥è¯†è’¸é¦æ¨¡å‹æ¨ç†æµ‹è¯•è„šæœ¬ - å¤šæ–¹æ¡ˆæ”¯æŒç‰ˆæœ¬
å¯¹æ¯”åŸå§‹8Bæ¨¡å‹å’ŒKDè®­ç»ƒåæ¨¡å‹çš„æ¨ç†èƒ½åŠ›

ä½¿ç”¨æ–¹æ³•:
python test_kd_inference_v2.py --mode quick    # å¿«é€ŸéªŒè¯ (10æ ·æœ¬)
python test_kd_inference_v2.py --mode medium   # ä¸­ç­‰è§„æ¨¡ (50æ ·æœ¬)  
python test_kd_inference_v2.py --mode full     # å®Œæ•´è¯„ä¼° (350æ ·æœ¬)
"""
import torch
import json
import time
import random
import argparse
from typing import List, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import logging
from pathlib import Path
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GPUOptimizedTester:
    def __init__(self, mode="quick", temperature=0.7, student_model="Qwen/Qwen3-8B", 
                 kd_model_path=None, original_gpu_ids=None, kd_gpu_ids=None,
                 eval_data_path=None, max_length=512, batch_sizes=None):
        self.mode = mode
        self.temperature = temperature
        self.student_model = student_model
        self.kd_model_path = kd_model_path
        self.eval_data_path = eval_data_path
        self.max_length = max_length
        self.tokenizer = None
        self.test_samples = []
        
        # GPUä¼˜åŒ–é…ç½®
        self.gpu_count = torch.cuda.device_count()
        if batch_sizes:
            self.batch_sizes = batch_sizes
        else:
            self.batch_sizes = {
                "quick": min(8, self.gpu_count * 2),
                "medium": min(16, self.gpu_count * 2),
                "full": min(24, self.gpu_count * 3)
            }
        
        # GPUåˆ†é…ç­–ç•¥
        if original_gpu_ids is None:
            self.original_gpu_ids = list(range(min(4, self.gpu_count)))
        else:
            self.original_gpu_ids = original_gpu_ids
            
        if kd_gpu_ids is None:
            remaining_gpus = list(range(4, self.gpu_count))
            self.kd_gpu_ids = remaining_gpus if remaining_gpus else self.original_gpu_ids
        else:
            self.kd_gpu_ids = kd_gpu_ids
        
        logger.info(f"æ£€æµ‹åˆ° {self.gpu_count} ä¸ªGPU")
        logger.info(f"æ¨¡å¼: {mode}, æ‰¹æ¬¡å¤§å°: {self.batch_sizes[mode]}, æ¸©åº¦: {temperature}")
        
    def load_eval_data(self, data_path):
        """åŠ è½½è¯„ä¼°æ•°æ®"""
        logger.info(f"åŠ è½½è¯„ä¼°æ•°æ®: {data_path}")
        
        all_samples = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    all_samples.append(json.loads(line))
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æ ·æœ¬
        if self.mode == "quick":
            # å¿«é€Ÿæ¨¡å¼ï¼šç²¾é€‰10ä¸ªä»£è¡¨æ€§æ ·æœ¬
            selected_samples = self._select_representative_samples(all_samples, 10)
            logger.info("å¿«é€Ÿæ¨¡å¼ï¼šé€‰æ‹©10ä¸ªä»£è¡¨æ€§æ ·æœ¬")
        elif self.mode == "medium":
            # ä¸­ç­‰æ¨¡å¼ï¼šéšæœºé‡‡æ ·50ä¸ª
            selected_samples = random.sample(all_samples, min(50, len(all_samples)))
            logger.info("ä¸­ç­‰æ¨¡å¼ï¼šéšæœºé‡‡æ ·50ä¸ªæ ·æœ¬")
        else:
            # å®Œæ•´æ¨¡å¼ï¼šå…¨éƒ¨æ ·æœ¬
            selected_samples = all_samples
            logger.info(f"å®Œæ•´æ¨¡å¼ï¼šä½¿ç”¨å…¨éƒ¨{len(all_samples)}ä¸ªæ ·æœ¬")
            
        self.test_samples = selected_samples
        return len(selected_samples)
    
    def _select_representative_samples(self, all_samples, count=10):
        """é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬"""
        # æŒ‰é¢†åŸŸåˆ†ç±»
        by_domain = {}
        for sample in all_samples:
            domain = sample.get('domain', 'unknown')
            if domain not in by_domain:
                by_domain[domain] = []
            by_domain[domain].append(sample)
        
        # å‡åŒ€é€‰æ‹©
        selected = []
        domains = list(by_domain.keys())
        per_domain = count // len(domains)
        remainder = count % len(domains)
        
        for i, domain in enumerate(domains):
            n = per_domain + (1 if i < remainder else 0)
            selected.extend(random.sample(by_domain[domain], min(n, len(by_domain[domain]))))
        
        return selected[:count]
    
    def load_model_optimized(self, model_path, adapter_path=None, gpu_ids=None):
        """åŠ è½½æ¨¡å‹å¹¶ä¼˜åŒ–GPUåˆ†é…"""
        logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
        
        if self.tokenizer is None:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # æ™ºèƒ½GPUåˆ†é… - ä½¿ç”¨æŒ‡å®šçš„GPU
        device_map = {}
        if gpu_ids:
            # æ‰‹åŠ¨æŒ‡å®šGPUåˆ†é…
            layers_per_gpu = 5  # 8Bæ¨¡å‹å¤§çº¦36å±‚ï¼Œåˆ†é…åˆ°æŒ‡å®šGPU
            gpu_idx = 0
            device_map["model.embed_tokens"] = gpu_ids[gpu_idx]
            
            total_layers = 36  # Qwen3-8Bå±‚æ•°
            for i in range(total_layers):
                if i > 0 and i % layers_per_gpu == 0:
                    gpu_idx = (gpu_idx + 1) % len(gpu_ids)
                device_map[f"model.layers.{i}"] = gpu_ids[gpu_idx]
                
            device_map["model.norm"] = gpu_ids[-1]
            device_map["lm_head"] = gpu_ids[-1]
        else:
            device_map = "auto"
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=device_map,
            trust_remote_code=True
        )
        
        # å¦‚æœæœ‰adapterï¼ŒåŠ è½½LoRA
        if adapter_path:
            logger.info(f"åŠ è½½LoRAé€‚é…å™¨: {adapter_path}")
            model = PeftModel.from_pretrained(model, adapter_path)
        
        return model
    
    def batch_generate(self, model, prompts_batch, max_length=None, temperature=None):
        if max_length is None:
            max_length = self.max_length
        if temperature is None:
            temperature = self.temperature
        """æ‰¹é‡ç”Ÿæˆå›å¤"""
        # æ‰¹é‡ç¼–ç 
        inputs = self.tokenizer(
            prompts_batch, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=1024
        ).to(next(model.parameters()).device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                num_return_sequences=1,
                use_cache=True
            )
        
        # è§£ç æ‰¹é‡ç»“æœ
        responses = []
        input_length = inputs.input_ids.shape[1]
        
        for i, output in enumerate(outputs):
            generated = output[input_length:]
            response = self.tokenizer.decode(generated, skip_special_tokens=True).strip()
            responses.append(response)
        
        return responses
    
    def evaluate_model_batch(self, model, model_name):
        """æ‰¹é‡è¯„ä¼°æ¨¡å‹"""
        logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼°: {model_name}")
        
        batch_size = self.batch_sizes[self.mode]
        all_results = []
        total_time = 0
        
        # å‡†å¤‡æ‰¹æ¬¡
        batches = []
        for i in range(0, len(self.test_samples), batch_size):
            batch = self.test_samples[i:i+batch_size]
            batches.append(batch)
        
        logger.info(f"æ€»è®¡ {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡ {batch_size} ä¸ªæ ·æœ¬")
        
        for batch_idx, batch_samples in enumerate(batches):
            start_time = time.time()
            
            # æå–prompts
            prompts = [sample["prompt"] for sample in batch_samples]
            
            # æ‰¹é‡æ¨ç†
            responses = self.batch_generate(model, prompts, temperature=self.temperature)
            
            batch_time = time.time() - start_time
            total_time += batch_time
            
            # ä¿å­˜ç»“æœ
            for sample, response in zip(batch_samples, responses):
                result = {
                    "id": sample["id"],
                    "domain": sample.get("domain", "unknown"),
                    "difficulty": sample.get("difficulty", "unknown"),
                    "prompt": sample["prompt"],
                    f"{model_name}_response": response,
                    f"{model_name}_time": batch_time / len(batch_samples),  # å¹³å‡æ—¶é—´
                    "expected_answer": sample.get("original_answer", ""),
                    "teacher_response": sample.get("teacher_response", "")
                }
                all_results.append(result)
            
            logger.info(f"æ‰¹æ¬¡ {batch_idx+1}/{len(batches)} å®Œæˆï¼Œç”¨æ—¶ {batch_time:.2f}s")
        
        logger.info(f"{model_name} è¯„ä¼°å®Œæˆï¼Œæ€»ç”¨æ—¶ {total_time:.2f}s")
        return all_results, total_time
    
    def run_comparison_test(self, eval_data_path=None):
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        logger.info(f"=== å¼€å§‹ {self.mode} æ¨¡å¼å¯¹æ¯”æµ‹è¯• ===")
        
        # åŠ è½½æ•°æ®
        data_path = eval_data_path or self.eval_data_path
        if not data_path:
            raise ValueError("å¿…é¡»æŒ‡å®šè¯„ä¼°æ•°æ®è·¯å¾„")
        sample_count = self.load_eval_data(data_path)
        logger.info(f"åŠ è½½äº† {sample_count} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        all_results = []
        timing_info = {}
        
        try:
            # æµ‹è¯•åŸå§‹æ¨¡å‹
            logger.info("=== åŠ è½½åŸå§‹8Bæ¨¡å‹ ===")
            original_model = self.load_model_optimized(
                self.student_model, 
                gpu_ids=self.original_gpu_ids
            )
            
            original_results, original_time = self.evaluate_model_batch(
                original_model, "original"
            )
            timing_info["original_time"] = original_time
            
            # æ¸…ç†å†…å­˜
            del original_model
            torch.cuda.empty_cache()
            
            # æµ‹è¯•KDæ¨¡å‹
            logger.info("=== åŠ è½½KDè®­ç»ƒåæ¨¡å‹ ===")
            kd_model = self.load_model_optimized(
                self.student_model,
                adapter_path=self.kd_model_path,
                gpu_ids=self.kd_gpu_ids
            )
            
            kd_results, kd_time = self.evaluate_model_batch(
                kd_model, "kd"
            )
            timing_info["kd_time"] = kd_time
            
            # åˆå¹¶ç»“æœ
            for orig, kd in zip(original_results, kd_results):
                combined = {**orig}
                combined.update({
                    k: v for k, v in kd.items() 
                    if k.startswith("kd_")
                })
                all_results.append(combined)
                
        except Exception as e:
            logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"kd_comparison_{self.mode}_{timestamp}.json"
        
        final_results = {
            "config": {
                "mode": self.mode,
                "sample_count": sample_count,
                "batch_size": self.batch_sizes[self.mode],
                "gpu_count": self.gpu_count,
                "temperature": self.temperature
            },
            "timing": timing_info,
            "results": all_results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ€»ç»“
        self.print_summary(final_results, filename)
        
        return final_results
    
    def print_summary(self, results, filename):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        config = results["config"]
        timing = results["timing"]
        
        print("\n" + "="*60)
        print(f"ğŸ¯ KDæ¨¡å‹å¯¹æ¯”æµ‹è¯•å®Œæˆ - {config['mode'].upper()}æ¨¡å¼")
        print("="*60)
        print(f"ğŸ“Š æµ‹è¯•è§„æ¨¡: {config['sample_count']} ä¸ªæ ·æœ¬")
        print(f"âš¡ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨GPU: {config['gpu_count']} ä¸ª")
        print()
        print("â±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"   åŸå§‹8Bæ¨¡å‹: {timing.get('original_time', 0):.2f}s")
        print(f"   KDè®­ç»ƒåæ¨¡å‹: {timing.get('kd_time', 0):.2f}s")
        print(f"   æ€»ç”¨æ—¶: {sum(timing.values()):.2f}s")
        print()
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        print("="*60)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="KDæ¨¡å‹æ¨ç†å¯¹æ¯”æµ‹è¯•")
    parser.add_argument(
        "--mode", 
        choices=["quick", "medium", "full"],
        default="quick",
        help="æµ‹è¯•æ¨¡å¼: quick(10æ ·æœ¬), medium(50æ ·æœ¬), full(å…¨éƒ¨æ ·æœ¬)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=1.2,
        help="æ¨ç†æ¸©åº¦ (0.1=ä¿å®ˆ, 0.7=å¹³è¡¡, 1.2=åˆ›æ„,æœ€èƒ½å±•ç¤ºKDæ•ˆæœ). é»˜è®¤1.2"
    )
    parser.add_argument(
        "--student_model",
        default="Qwen/Qwen3-8B",
        help="å­¦ç”Ÿæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--kd_model_path",
        required=True,
        help="KDè®­ç»ƒåLoRAé€‚é…å™¨è·¯å¾„"
    )
    parser.add_argument(
        "--eval_data",
        required=True,
        help="è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--max_length",
        type=int,
        default=1536,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦"
    )
    parser.add_argument(
        "--original_gpu_ids",
        type=int,
        nargs="+",
        default=[0, 1, 2, 3],
        help="åŸå§‹æ¨¡å‹ä½¿ç”¨çš„GPU IDåˆ—è¡¨"
    )
    parser.add_argument(
        "--kd_gpu_ids",
        type=int,
        nargs="+",
        default=[4, 5, 6, 7],
        help="KDæ¨¡å‹ä½¿ç”¨çš„GPU IDåˆ—è¡¨"
    )
    parser.add_argument(
        "--quick_batch_size",
        type=int,
        default=8,
        help="quickæ¨¡å¼æ‰¹æ¬¡å¤§å°"
    )
    parser.add_argument(
        "--medium_batch_size",
        type=int,
        default=16,
        help="mediumæ¨¡å¼æ‰¹æ¬¡å¤§å°"
    )
    parser.add_argument(
        "--full_batch_size",
        type=int,
        default=24,
        help="fullæ¨¡å¼æ‰¹æ¬¡å¤§å°"
    )
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print(f"\nğŸš€ å¯åŠ¨KDæ¨¡å‹å¯¹æ¯”æµ‹è¯• - {args.mode.upper()}æ¨¡å¼")
    print(f"ğŸŒ¡ï¸ æ¨ç†æ¸©åº¦: {args.temperature} (1.2æœ€èƒ½å±•ç¤ºKDæ•ˆæœ)")
    print(f"ğŸ’¾ KDæ¨¡å‹è·¯å¾„: {args.kd_model_path}")
    print(f"ğŸ“ è¯„ä¼°æ•°æ®: {args.eval_data}")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    torch.manual_seed(42)
    
    # æ„å»ºæ‰¹æ¬¡å¤§å°é…ç½®
    batch_sizes = {
        "quick": args.quick_batch_size,
        "medium": args.medium_batch_size,
        "full": args.full_batch_size
    }
    
    try:
        tester = GPUOptimizedTester(
            mode=args.mode, 
            temperature=args.temperature,
            student_model=args.student_model,
            kd_model_path=args.kd_model_path,
            original_gpu_ids=args.original_gpu_ids,
            kd_gpu_ids=args.kd_gpu_ids,
            eval_data_path=args.eval_data,
            max_length=args.max_length,
            batch_sizes=batch_sizes
        )
        results = tester.run_comparison_test()
        
        print(f"\nâœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()