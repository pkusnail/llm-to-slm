#!/usr/bin/env python3
"""
çŸ¥è¯†è’¸é¦æ¨¡å‹æ¨ç†æµ‹è¯•è„šæœ¬ - å¤šæ–¹æ¡ˆæ”¯æŒç‰ˆæœ¬
å¯¹æ¯”åŸå§‹8Bæ¨¡å‹å’ŒKDè®­ç»ƒåæ¨¡å‹çš„æ¨ç†èƒ½åŠ›

ä½¿ç”¨æ–¹æ³•:
python test_kd_inference_v2.py --mode quick    # å¿«é€ŸéªŒè¯ (10æ ·æœ¬)
python test_kd_inference_v2.py --mode medium   # ä¸­ç­‰è§„æ¨¡ (50æ ·æœ¬)  
python test_kd_inference_v2.py --mode full     # å®Œæ•´è¯„ä¼° (350æ ·æœ¬)
"""
import torch
import json
import time
import random
import argparse
from typing import List, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import logging
from pathlib import Path
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GPUOptimizedTester:
    def __init__(self, mode="quick", temperature=0.7):
        self.mode = mode
        self.temperature = temperature
        self.tokenizer = None
        self.test_samples = []
        
        # GPUä¼˜åŒ–é…ç½®
        self.gpu_count = torch.cuda.device_count()
        self.batch_sizes = {
            "quick": min(8, self.gpu_count * 2),     # æ¿€è¿›batchï¼šæ¯GPU 2ä¸ªæ ·æœ¬
            "medium": min(16, self.gpu_count * 2),   # ä¸­ç­‰batchï¼šæ¯GPU 2ä¸ªæ ·æœ¬
            "full": min(24, self.gpu_count * 3)      # æœ€å¤§batchï¼šæ¯GPU 3ä¸ªæ ·æœ¬
        }
        
        logger.info(f"æ£€æµ‹åˆ° {self.gpu_count} ä¸ªGPU")
        logger.info(f"æ¨¡å¼: {mode}, æ‰¹æ¬¡å¤§å°: {self.batch_sizes[mode]}, æ¸©åº¦: {temperature}")
        
    def load_eval_data(self, data_path="outputs/experiment/qwen3_30b_to_8b_ultrabatch_512/sft/sft_eval_data_clean.jsonl"):
        """åŠ è½½è¯„ä¼°æ•°æ®"""
        logger.info(f"åŠ è½½è¯„ä¼°æ•°æ®: {data_path}")
        
        all_samples = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    all_samples.append(json.loads(line))
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æ ·æœ¬
        if self.mode == "quick":
            # å¿«é€Ÿæ¨¡å¼ï¼šç²¾é€‰10ä¸ªä»£è¡¨æ€§æ ·æœ¬
            selected_samples = self._select_representative_samples(all_samples, 10)
            logger.info("å¿«é€Ÿæ¨¡å¼ï¼šé€‰æ‹©10ä¸ªä»£è¡¨æ€§æ ·æœ¬")
        elif self.mode == "medium":
            # ä¸­ç­‰æ¨¡å¼ï¼šéšæœºé‡‡æ ·50ä¸ª
            selected_samples = random.sample(all_samples, min(50, len(all_samples)))
            logger.info("ä¸­ç­‰æ¨¡å¼ï¼šéšæœºé‡‡æ ·50ä¸ªæ ·æœ¬")
        else:
            # å®Œæ•´æ¨¡å¼ï¼šå…¨éƒ¨æ ·æœ¬
            selected_samples = all_samples
            logger.info(f"å®Œæ•´æ¨¡å¼ï¼šä½¿ç”¨å…¨éƒ¨{len(all_samples)}ä¸ªæ ·æœ¬")
            
        self.test_samples = selected_samples
        return len(selected_samples)
    
    def _select_representative_samples(self, all_samples, count=10):
        """é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬"""
        # æŒ‰é¢†åŸŸåˆ†ç±»
        by_domain = {}
        for sample in all_samples:
            domain = sample.get('domain', 'unknown')
            if domain not in by_domain:
                by_domain[domain] = []
            by_domain[domain].append(sample)
        
        # å‡åŒ€é€‰æ‹©
        selected = []
        domains = list(by_domain.keys())
        per_domain = count // len(domains)
        remainder = count % len(domains)
        
        for i, domain in enumerate(domains):
            n = per_domain + (1 if i < remainder else 0)
            selected.extend(random.sample(by_domain[domain], min(n, len(by_domain[domain]))))
        
        return selected[:count]
    
    def load_model_optimized(self, model_path, adapter_path=None, gpu_ids=None):
        """åŠ è½½æ¨¡å‹å¹¶ä¼˜åŒ–GPUåˆ†é…"""
        logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
        
        if self.tokenizer is None:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # æ™ºèƒ½GPUåˆ†é… - ä½¿ç”¨æŒ‡å®šçš„GPU
        device_map = {}
        if gpu_ids:
            # æ‰‹åŠ¨æŒ‡å®šGPUåˆ†é…
            layers_per_gpu = 5  # 8Bæ¨¡å‹å¤§çº¦36å±‚ï¼Œåˆ†é…åˆ°æŒ‡å®šGPU
            gpu_idx = 0
            device_map["model.embed_tokens"] = gpu_ids[gpu_idx]
            
            total_layers = 36  # Qwen3-8Bå±‚æ•°
            for i in range(total_layers):
                if i > 0 and i % layers_per_gpu == 0:
                    gpu_idx = (gpu_idx + 1) % len(gpu_ids)
                device_map[f"model.layers.{i}"] = gpu_ids[gpu_idx]
                
            device_map["model.norm"] = gpu_ids[-1]
            device_map["lm_head"] = gpu_ids[-1]
        else:
            device_map = "auto"
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=device_map,
            trust_remote_code=True
        )
        
        # å¦‚æœæœ‰adapterï¼ŒåŠ è½½LoRA
        if adapter_path:
            logger.info(f"åŠ è½½LoRAé€‚é…å™¨: {adapter_path}")
            model = PeftModel.from_pretrained(model, adapter_path)
        
        return model
    
    def batch_generate(self, model, prompts_batch, max_length=512, temperature=0.7):
        """æ‰¹é‡ç”Ÿæˆå›å¤"""
        # æ‰¹é‡ç¼–ç 
        inputs = self.tokenizer(
            prompts_batch, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=1024
        ).to(next(model.parameters()).device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                num_return_sequences=1,
                use_cache=True
            )
        
        # è§£ç æ‰¹é‡ç»“æœ
        responses = []
        input_length = inputs.input_ids.shape[1]
        
        for i, output in enumerate(outputs):
            generated = output[input_length:]
            response = self.tokenizer.decode(generated, skip_special_tokens=True).strip()
            responses.append(response)
        
        return responses
    
    def evaluate_model_batch(self, model, model_name):
        """æ‰¹é‡è¯„ä¼°æ¨¡å‹"""
        logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼°: {model_name}")
        
        batch_size = self.batch_sizes[self.mode]
        all_results = []
        total_time = 0
        
        # å‡†å¤‡æ‰¹æ¬¡
        batches = []
        for i in range(0, len(self.test_samples), batch_size):
            batch = self.test_samples[i:i+batch_size]
            batches.append(batch)
        
        logger.info(f"æ€»è®¡ {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡ {batch_size} ä¸ªæ ·æœ¬")
        
        for batch_idx, batch_samples in enumerate(batches):
            start_time = time.time()
            
            # æå–prompts
            prompts = [sample["prompt"] for sample in batch_samples]
            
            # æ‰¹é‡æ¨ç†
            responses = self.batch_generate(model, prompts, temperature=self.temperature)
            
            batch_time = time.time() - start_time
            total_time += batch_time
            
            # ä¿å­˜ç»“æœ
            for sample, response in zip(batch_samples, responses):
                result = {
                    "id": sample["id"],
                    "domain": sample.get("domain", "unknown"),
                    "difficulty": sample.get("difficulty", "unknown"),
                    "prompt": sample["prompt"],
                    f"{model_name}_response": response,
                    f"{model_name}_time": batch_time / len(batch_samples),  # å¹³å‡æ—¶é—´
                    "expected_answer": sample.get("original_answer", ""),
                    "teacher_response": sample.get("teacher_response", "")
                }
                all_results.append(result)
            
            logger.info(f"æ‰¹æ¬¡ {batch_idx+1}/{len(batches)} å®Œæˆï¼Œç”¨æ—¶ {batch_time:.2f}s")
        
        logger.info(f"{model_name} è¯„ä¼°å®Œæˆï¼Œæ€»ç”¨æ—¶ {total_time:.2f}s")
        return all_results, total_time
    
    def run_comparison_test(self):
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        logger.info(f"=== å¼€å§‹ {self.mode} æ¨¡å¼å¯¹æ¯”æµ‹è¯• ===")
        
        # åŠ è½½æ•°æ®
        sample_count = self.load_eval_data()
        logger.info(f"åŠ è½½äº† {sample_count} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        all_results = []
        timing_info = {}
        
        try:
            # æµ‹è¯•åŸå§‹æ¨¡å‹
            logger.info("=== åŠ è½½åŸå§‹8Bæ¨¡å‹ ===")
            original_model = self.load_model_optimized(
                "Qwen/Qwen3-8B", 
                gpu_ids=[0, 1, 2, 3]  # ä½¿ç”¨å‰4ä¸ªGPU
            )
            
            original_results, original_time = self.evaluate_model_batch(
                original_model, "original"
            )
            timing_info["original_time"] = original_time
            
            # æ¸…ç†å†…å­˜
            del original_model
            torch.cuda.empty_cache()
            
            # æµ‹è¯•KDæ¨¡å‹
            logger.info("=== åŠ è½½KDè®­ç»ƒåæ¨¡å‹ ===")
            kd_model = self.load_model_optimized(
                "Qwen/Qwen3-8B",
                adapter_path="outputs/experiment/gpu_optimized_kd_fixed/gpu_optimized_kd_20250910_170252/final_model",
                gpu_ids=[4, 5, 6, 7]  # ä½¿ç”¨å4ä¸ªGPU
            )
            
            kd_results, kd_time = self.evaluate_model_batch(
                kd_model, "kd"
            )
            timing_info["kd_time"] = kd_time
            
            # åˆå¹¶ç»“æœ
            for orig, kd in zip(original_results, kd_results):
                combined = {**orig}
                combined.update({
                    k: v for k, v in kd.items() 
                    if k.startswith("kd_")
                })
                all_results.append(combined)
                
        except Exception as e:
            logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"kd_comparison_{self.mode}_{timestamp}.json"
        
        final_results = {
            "config": {
                "mode": self.mode,
                "sample_count": sample_count,
                "batch_size": self.batch_sizes[self.mode],
                "gpu_count": self.gpu_count,
                "temperature": self.temperature
            },
            "timing": timing_info,
            "results": all_results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ€»ç»“
        self.print_summary(final_results, filename)
        
        return final_results
    
    def print_summary(self, results, filename):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        config = results["config"]
        timing = results["timing"]
        
        print("\n" + "="*60)
        print(f"ğŸ¯ KDæ¨¡å‹å¯¹æ¯”æµ‹è¯•å®Œæˆ - {config['mode'].upper()}æ¨¡å¼")
        print("="*60)
        print(f"ğŸ“Š æµ‹è¯•è§„æ¨¡: {config['sample_count']} ä¸ªæ ·æœ¬")
        print(f"âš¡ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨GPU: {config['gpu_count']} ä¸ª")
        print()
        print("â±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"   åŸå§‹8Bæ¨¡å‹: {timing.get('original_time', 0):.2f}s")
        print(f"   KDè®­ç»ƒåæ¨¡å‹: {timing.get('kd_time', 0):.2f}s")
        print(f"   æ€»ç”¨æ—¶: {sum(timing.values()):.2f}s")
        print()
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        print("="*60)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="KDæ¨¡å‹æ¨ç†å¯¹æ¯”æµ‹è¯•")
    parser.add_argument(
        "--mode", 
        choices=["quick", "medium", "full"],
        default="quick",
        help="æµ‹è¯•æ¨¡å¼: quick(10æ ·æœ¬,~3åˆ†é’Ÿ), medium(50æ ·æœ¬,~15åˆ†é’Ÿ), full(350æ ·æœ¬,~2å°æ—¶)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="æ¨ç†æ¸©åº¦ (0.1=ä¿å®ˆ, 0.7=å¹³è¡¡, 1.2=åˆ›æ„). é»˜è®¤0.7"
    )
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print(f"\nğŸš€ å¯åŠ¨KDæ¨¡å‹å¯¹æ¯”æµ‹è¯• - {args.mode.upper()}æ¨¡å¼")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    torch.manual_seed(42)
    
    try:
        tester = GPUOptimizedTester(mode=args.mode, temperature=args.temperature)
        results = tester.run_comparison_test()
        
        print(f"\nâœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()